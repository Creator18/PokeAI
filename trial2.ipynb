{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "677ae718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: State Management & Utilities\n",
    "# ============================================================================\n",
    "# CHANGES FROM PREVIOUS VERSION:\n",
    "# 1. Added TAUGHT_TRANSITIONS_FILE path\n",
    "# 2. Added Markov similarity weights as constants\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "BASE_PATH = Path(\"C:/Users/HP/Documents/cogai/\")\n",
    "ACTION_FILE = BASE_PATH / \"action.json\"\n",
    "STATE_FILE = BASE_PATH / \"game_state.json\"\n",
    "TAUGHT_TRANSITIONS_FILE = BASE_PATH / \"taught_transitions.json\"  # NEW: Markov data\n",
    "\n",
    "# === MARKOV SIMILARITY WEIGHTS (NEW) ===\n",
    "MARKOV_IMMEDIATE_WEIGHT = 0.5\n",
    "MARKOV_SEQUENTIAL_WEIGHT = 0.3\n",
    "MARKOV_PARTIAL_WEIGHT = 0.2\n",
    "MARKOV_FAMILIARITY_THRESHOLD = 0.6\n",
    "\n",
    "# Sequential match weights\n",
    "MARKOV_SEQ_FULL_WEIGHT = 1.0    # 8 actions match\n",
    "MARKOV_SEQ_MEDIUM_WEIGHT = 0.6  # 5 actions match\n",
    "MARKOV_SEQ_SHORT_WEIGHT = 0.3   # 3 actions match\n",
    "\n",
    "# Position proximity thresholds\n",
    "MARKOV_POS_EXACT_BONUS = 0.35\n",
    "MARKOV_POS_NEAR_BONUS = 0.25    # Within 2 tiles\n",
    "MARKOV_POS_FAR_BONUS = 0.1      # Within 5 tiles\n",
    "MARKOV_POS_MAX_DIST = 5         # Skip if farther\n",
    "\n",
    "EXPECTED_STATE_DIM = 6\n",
    "PALETTE_DIM = 768\n",
    "TILE_DIM = 600\n",
    "\n",
    "# State vector from Lua: [x, y, map_id, in_battle, menu_flag, direction]\n",
    "# x, y: 0-255 (raw tile coordinates)\n",
    "# map_id: 0-255\n",
    "# in_battle: 0 or 1\n",
    "# menu_flag: 0 or 1 (from game_state == 1)\n",
    "# direction: 0-3 (DOWN=0, UP=1, LEFT=2, RIGHT=3) - already normalized by Lua\n",
    "\n",
    "def normalize_game_state(raw_state):\n",
    "    \"\"\"Normalize context state for learning.\n",
    "    \n",
    "    Lua sends: [x, y, map_id, in_battle, menu_flag, direction]\n",
    "    - x, y: 0-255 tile coordinates\n",
    "    - map_id: 0-255\n",
    "    - in_battle: 0 or 1\n",
    "    - menu_flag: 0 or 1\n",
    "    - direction: 0-3 (already normalized by Lua)\n",
    "    \"\"\"\n",
    "    if len(raw_state) < 6:\n",
    "        return raw_state\n",
    "    \n",
    "    normalized = raw_state.copy()\n",
    "    \n",
    "    # Normalize x, y to 0-1 range for learning\n",
    "    normalized[0] = raw_state[0] / 255.0\n",
    "    normalized[1] = raw_state[1] / 255.0\n",
    "    \n",
    "    # Map ID: keep as-is but clamp\n",
    "    normalized[2] = np.clip(raw_state[2], 0, 255)\n",
    "    \n",
    "    # Battle flag: ensure binary\n",
    "    normalized[3] = 1.0 if raw_state[3] > 0 else 0.0\n",
    "    \n",
    "    # Menu flag: ensure 0-1\n",
    "    normalized[4] = 1.0 if raw_state[4] > 0 else 0.0\n",
    "    \n",
    "    # Direction: already 0-3 from Lua, keep as-is\n",
    "    normalized[5] = int(raw_state[5]) % 4\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def compute_derived_features(current, prev):\n",
    "    \"\"\"Extract temporal features (8D)\"\"\"\n",
    "    if prev is None:\n",
    "        return np.zeros(8)\n",
    "    \n",
    "    # Velocity uses normalized coordinates\n",
    "    vel_x = current[0] - prev[0]\n",
    "    vel_y = current[1] - prev[1]\n",
    "    map_changed = 1.0 if abs(current[2] - prev[2]) > 0.5 else 0.0\n",
    "    battle_started = 1.0 if current[3] > prev[3] else 0.0\n",
    "    battle_ended = 1.0 if current[3] < prev[3] else 0.0\n",
    "    menu_opened = 1.0 if current[4] > prev[4] else 0.0\n",
    "    menu_closed = 1.0 if current[4] < prev[4] else 0.0\n",
    "    direction_changed = 1.0 if current[5] != prev[5] else 0.0\n",
    "    \n",
    "    return np.array([vel_x, vel_y, map_changed, battle_started, battle_ended,\n",
    "                     menu_opened, menu_closed, direction_changed])\n",
    "\n",
    "def build_learning_state(derived, palette, tiles, in_battle):\n",
    "    \"\"\"\n",
    "    HYBRID PERCEPTION:\n",
    "    - Overworld: tiles (spatial) + palette (context)\n",
    "    - Battle: palette only (tiles are just UI)\n",
    "    \"\"\"\n",
    "    if in_battle > 0.5:\n",
    "        state = np.concatenate([derived, palette])\n",
    "    else:\n",
    "        state = np.concatenate([derived, tiles, palette])\n",
    "    \n",
    "    noise = np.random.randn(len(state)) * 0.0001\n",
    "    return state + noise\n",
    "\n",
    "def read_game_state(max_retries=3):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        context_state: normalized state for learning/error calculation\n",
    "        palette_state: visual palette data\n",
    "        tile_state: visual tile data\n",
    "        dead: death flag\n",
    "        raw_position: (raw_x, raw_y) for tile tracking - NOT normalized\n",
    "    \"\"\"\n",
    "    if not STATE_FILE.exists():\n",
    "        return np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), np.zeros(TILE_DIM), False, (0, 0)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with open(STATE_FILE, \"r\") as f:\n",
    "                data = json.loads(f.read())\n",
    "            \n",
    "            raw = data.get(\"state\", [])\n",
    "            palette_raw = data.get(\"palette\", [])\n",
    "            tiles_raw = data.get(\"tiles\", [])\n",
    "            dead = bool(data.get(\"dead\", False))\n",
    "            \n",
    "            # Store raw position BEFORE normalization (these are tile coordinates 0-255)\n",
    "            raw_x = int(raw[0]) if len(raw) > 0 else 0\n",
    "            raw_y = int(raw[1]) if len(raw) > 1 else 0\n",
    "            raw_position = (raw_x, raw_y)\n",
    "\n",
    "            context_state = normalize_game_state(np.array(raw, dtype=float))\n",
    "            palette_state = np.array(palette_raw, dtype=float) if palette_raw else np.zeros(PALETTE_DIM)\n",
    "            tile_state = np.array(tiles_raw, dtype=float) if tiles_raw else np.zeros(TILE_DIM)\n",
    "            \n",
    "            break\n",
    "\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(0.001)\n",
    "                continue\n",
    "            return np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), np.zeros(TILE_DIM), False, (0, 0)\n",
    "        except Exception:\n",
    "            return np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), np.zeros(TILE_DIM), False, (0, 0)\n",
    "\n",
    "    if context_state.shape[0] < EXPECTED_STATE_DIM:\n",
    "        context_state = np.pad(context_state, (0, EXPECTED_STATE_DIM - context_state.shape[0]))\n",
    "    elif context_state.shape[0] > EXPECTED_STATE_DIM:\n",
    "        context_state = context_state[:EXPECTED_STATE_DIM]\n",
    "    \n",
    "    if palette_state.shape[0] < PALETTE_DIM:\n",
    "        palette_state = np.pad(palette_state, (0, PALETTE_DIM - palette_state.shape[0]))\n",
    "    elif palette_state.shape[0] > PALETTE_DIM:\n",
    "        palette_state = palette_state[:PALETTE_DIM]\n",
    "    \n",
    "    if tile_state.shape[0] < TILE_DIM:\n",
    "        tile_state = np.pad(tile_state, (0, TILE_DIM - tile_state.shape[0]))\n",
    "    elif tile_state.shape[0] > TILE_DIM:\n",
    "        tile_state = tile_state[:TILE_DIM]\n",
    "\n",
    "    return context_state, palette_state, tile_state, dead, raw_position\n",
    "\n",
    "def write_action(action_name):\n",
    "    if action_name:\n",
    "        action_name = action_name.upper()\n",
    "    \n",
    "    try:\n",
    "        with open(ACTION_FILE, \"w\") as f:\n",
    "            json.dump({\"action\": action_name}, f)\n",
    "            f.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write action: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9dd8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Perceptron Classes\n",
    "# ============================================================================\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, kind, action=None, group=None, entity_type=None):\n",
    "        self.kind = kind\n",
    "        self.action = action\n",
    "        self.group = group\n",
    "        self.entity_type = entity_type\n",
    "        \n",
    "        self.utility = 1.0\n",
    "        self.weights = None\n",
    "        \n",
    "        self.eligibility_fast = 0.0\n",
    "        self.eligibility_slow = 0.0\n",
    "        \n",
    "        self.familiarity = 0.0\n",
    "        self.activation_history = deque(maxlen=10)\n",
    "        \n",
    "        self.learning_rate = 0.01\n",
    "        self.prediction_errors = deque(maxlen=50)\n",
    "\n",
    "    def ensure_weights(self, dim):\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(dim) * 0.001\n",
    "\n",
    "    def predict(self, state):\n",
    "        self.ensure_weights(len(state))\n",
    "        \n",
    "        # Handle dimension mismatch\n",
    "        if len(self.weights) != len(state):\n",
    "            min_dim = min(len(self.weights), len(state))\n",
    "            raw_activation = np.dot(self.weights[:min_dim], state[:min_dim])\n",
    "        else:\n",
    "            raw_activation = np.dot(self.weights, state)\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            novelty_factor = 1.0 / (1.0 + np.sqrt(self.familiarity * 0.5))\n",
    "            decayed_activation = raw_activation * novelty_factor\n",
    "            self.activation_history.append(abs(raw_activation))\n",
    "            return decayed_activation\n",
    "        else:\n",
    "            return raw_activation\n",
    "\n",
    "    def adapt_learning_rate(self):\n",
    "        if len(self.prediction_errors) >= 50:\n",
    "            avg_error = np.mean(self.prediction_errors)\n",
    "            \n",
    "            if avg_error < 0.1:\n",
    "                self.learning_rate = max(0.001, self.learning_rate * 0.99)\n",
    "            elif avg_error > 0.5:\n",
    "                self.learning_rate = min(0.05, self.learning_rate * 1.01)\n",
    "\n",
    "    def update(self, state, error, gamma_fast=0.5, gamma_slow=0.95, stagnation=0.0):\n",
    "        self.ensure_weights(len(state))\n",
    "        \n",
    "        # Handle dimension mismatch\n",
    "        if len(self.weights) != len(state):\n",
    "            min_dim = min(len(self.weights), len(state))\n",
    "            state = state[:min_dim]\n",
    "            self.weights = self.weights[:min_dim]\n",
    "        \n",
    "        self.eligibility_fast = gamma_fast * self.eligibility_fast + 1.0\n",
    "        self.eligibility_slow = gamma_slow * self.eligibility_slow + 1.0\n",
    "        \n",
    "        self.adapt_learning_rate()\n",
    "        \n",
    "        fast_update = 0.7 * self.learning_rate * error * state * self.eligibility_fast\n",
    "        slow_update = 0.3 * self.learning_rate * error * state * self.eligibility_slow\n",
    "        self.weights += fast_update + slow_update\n",
    "\n",
    "        if self.kind == \"action\":\n",
    "            if error > 0.01:\n",
    "                if stagnation > 0.5:\n",
    "                    self.utility *= 0.97\n",
    "                elif error > 0.2:\n",
    "                    self.utility = min(self.utility * 1.02, 2.0)\n",
    "                else:\n",
    "                    self.utility *= 0.995\n",
    "            \n",
    "            if self.group == \"move\":\n",
    "                self.utility = np.clip(self.utility, 0.1, 2.0)\n",
    "            else:\n",
    "                self.utility = np.clip(self.utility, 0.01, 2.0)\n",
    "        \n",
    "        if self.kind == \"entity\" and len(self.activation_history) > 0:\n",
    "            recent_avg = np.mean(self.activation_history)\n",
    "            if recent_avg > 0.1:\n",
    "                self.familiarity += 0.03\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            prediction = self.predict(state)\n",
    "            self.prediction_errors.append(abs(prediction - error))\n",
    "\n",
    "\n",
    "class ControlSwapPerceptron(Perceptron):\n",
    "    def __init__(self):\n",
    "        super().__init__(kind=\"control_swap\")\n",
    "        self.swap_history = deque(maxlen=100)\n",
    "        self.confidence = 0.0\n",
    "        \n",
    "    def should_swap(self, state, movement_stagnation):\n",
    "        if self.weights is None:\n",
    "            return False, 0.0\n",
    "        \n",
    "        self.ensure_weights(len(state))\n",
    "        swap_score = np.dot(self.weights, state)\n",
    "        stagnation_factor = np.tanh(movement_stagnation / 5.0)\n",
    "        combined_score = swap_score * 0.7 + stagnation_factor * 0.3\n",
    "        \n",
    "        return combined_score > 0.5, abs(combined_score)\n",
    "    \n",
    "    def record_swap_outcome(self, state, swapped, novelty_gained):\n",
    "        self.swap_history.append((swapped, novelty_gained))\n",
    "        \n",
    "        if len(self.swap_history) >= 20:\n",
    "            recent = list(self.swap_history)[-20:]\n",
    "            successful = sum(1 for swap, nov in recent if swap and nov > 0.2)\n",
    "            self.confidence = successful / 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "980759f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3 PART 1: Brain Class - Initialization & Markov System\n",
    "# ============================================================================\n",
    "# CHANGES FROM PREVIOUS VERSION:\n",
    "# 1. Added taught_transitions storage and loading\n",
    "# 2. Added compute_markov_similarity() method\n",
    "# 3. Added get_markov_action() method\n",
    "# 4. Added extract_partial_context() for partial similarity\n",
    "# ============================================================================\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.perceptrons = []\n",
    "        \n",
    "        self.prev_learning_states = deque(maxlen=50)\n",
    "        self.prev_context_states = deque(maxlen=10)\n",
    "        self.last_positions = deque(maxlen=30)\n",
    "        self.action_history = deque(maxlen=100)\n",
    "        \n",
    "        self.control_mode = \"move\"\n",
    "        self.timestep = 0\n",
    "        self.last_action = None\n",
    "        self.last_direction = 0\n",
    "        \n",
    "        self.MOVE_UTILITY_FLOOR = 0.05\n",
    "        self.INTERACT_UTILITY_FLOOR = 0.15\n",
    "        \n",
    "        # === PERSISTENT EXPLORATION MEMORY ===\n",
    "        self.EXPLORATION_MEMORY_FILE = BASE_PATH / \"exploration_memory.json\"\n",
    "        self.exploration_memory = {}\n",
    "        self.current_map_id = None\n",
    "        self.SAVE_INTERVAL = 100\n",
    "        \n",
    "        # Direction mapping\n",
    "        self.DIRECTION_NAMES = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        self.DIRECTION_TO_INT = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "        self.INT_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        \n",
    "        self.DIRECTION_DELTAS_INT = {0: (0, 1), 1: (0, -1), 2: (-1, 0), 3: (1, 0)}\n",
    "        self.ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "        self.DELTA_TO_DIRECTION = {(0, 1): 0, (0, -1): 1, (-1, 0): 2, (1, 0): 3}\n",
    "        \n",
    "        self.load_exploration_memory()\n",
    "        \n",
    "        # === MARKOV TRANSITION SYSTEM (NEW) ===\n",
    "        self.taught_transitions = []  # Flattened list of frames from batches\n",
    "        self.taught_batches = []      # Original batch structure\n",
    "        self.taught_metadata = {}     # Metadata from teaching session\n",
    "        self.markov_enabled = True\n",
    "        self.markov_action_count = 0\n",
    "        self.curiosity_action_count = 0\n",
    "        self.last_markov_score = 0.0\n",
    "        self.last_markov_action = None\n",
    "        \n",
    "        # === ACTION EXECUTION CONFIRMATION ===\n",
    "        self.pending_action = None\n",
    "        self.pending_action_frames = 0\n",
    "        self.ACTION_CONFIRM_FRAMES = 3\n",
    "        self.last_confirmed_action = None\n",
    "        \n",
    "        # === TILE INTERACTION PROBING ===\n",
    "        self.INTERACTION_VERIFY_FRAMES = 8\n",
    "        self.MIN_SUCCESS_RATE_THRESHOLD = 0.1\n",
    "        self.pending_interaction_verify = None\n",
    "        self.interaction_verify_countdown = 0\n",
    "        \n",
    "        # === MENU ESCAPE B-BOOST ===\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.B_BOOST_INCREMENT = 0.15\n",
    "        self.B_BOOST_MAX = 3.0\n",
    "        self.MENU_TRAP_THRESHOLD = 5\n",
    "        self.original_b_utility = None\n",
    "        \n",
    "        # === ADAPTIVE MODE SWAPPING ===\n",
    "        self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD = 15\n",
    "        self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD = 25\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.THRESHOLD_INCREMENT = 15\n",
    "        self.MAX_THRESHOLD = 150\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.swap_chain_count = 0\n",
    "        self.position_at_mode_swap = None\n",
    "        self.last_map_id = None\n",
    "        self.last_battle_state = None\n",
    "        \n",
    "        # === UNPRODUCTIVE MODE SWAP TRACKING ===\n",
    "        self.UNPRODUCTIVE_SWAP_THRESHOLD = 3\n",
    "        self.unproductive_swap_count = 0\n",
    "        self.utilities_before_swapping = {}\n",
    "        self.swap_chain_active = False\n",
    "        \n",
    "        # === STATE STAGNATION DETECTION ===\n",
    "        self.STATE_STAGNATION_THRESHOLD = 20\n",
    "        self.state_stagnation_count = 0\n",
    "        self.last_context_state_hash = None\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.STAGNATION_INITIATOR_PENALTY = 0.7\n",
    "        \n",
    "        # === \"BOTH\" MODE THRESHOLDS ===\n",
    "        self.BOTH_MODE_STAGNATION_THRESHOLD = 35\n",
    "        self.BOTH_MODE_SWAP_THRESHOLD = 5\n",
    "        \n",
    "        # === TURN AS PROGRESS TRACKING ===\n",
    "        self.last_direction_for_progress = None\n",
    "        self.direction_change_counts_as_progress = True\n",
    "        \n",
    "        # === NOVELTY WEIGHTS ===\n",
    "        self.UNVISITED_TILE_BONUS = 1.5\n",
    "        self.OBSTRUCTION_PENALTY = 0.25\n",
    "        \n",
    "        # === TRANSITION SYSTEM ===\n",
    "        self.TRANSITION_ATTRACTION_WEIGHT = 0.6\n",
    "        self.TEMP_DEBT_ACCUMULATION = 0.5\n",
    "        self.TEMP_DEBT_DECAY = 0.02\n",
    "        self.TEMP_DEBT_MAX = 15.0\n",
    "        \n",
    "        # === DEBT CAPS AND DECAY ===\n",
    "        self.MAX_MAP_DEBT = 10.0\n",
    "        self.MAX_LOCATION_DEBT = 5.0\n",
    "        self.DEBT_DECAY_RATE = 0.005\n",
    "        \n",
    "        # === TRANSITION BAN SYSTEM ===\n",
    "        self.transition_bans = {}\n",
    "        self.BAN_VICINITY_RADIUS = 3\n",
    "        self.BAN_COVERAGE_LIFT_THRESHOLD = 0.6\n",
    "        self.BAN_TIMEOUT_STEPS = 300\n",
    "        \n",
    "        # Multi-scale memory\n",
    "        self.visited_maps = {}\n",
    "        self.map_novelty_debt = {}\n",
    "        self.location_memory = {}\n",
    "        self.location_novelty = {}\n",
    "        self.action_execution_count = {}\n",
    "        \n",
    "        self.swap_perceptron = ControlSwapPerceptron()\n",
    "        self.error_history = deque(maxlen=1000)\n",
    "        self.numeric_error_history = deque(maxlen=1000)\n",
    "        self.visual_error_history = deque(maxlen=1000)\n",
    "        self._entity_norms_cache = {}\n",
    "        self._cache_valid = False\n",
    "        self.innate_entities_spawned = False\n",
    "        \n",
    "        # === REPETITION CORRECTION ===\n",
    "        self.consecutive_action_count = 0\n",
    "        self.current_repeated_action = None\n",
    "        self.LEARNING_SLOWDOWN_START = 3\n",
    "        self.LEARNING_SLOWDOWN_MAX = 10\n",
    "        self.PENALTY_THRESHOLD = 12\n",
    "        self.HARD_RESET_THRESHOLD = 18\n",
    "        \n",
    "        # === PATTERN DETECTION ===\n",
    "        self.PATTERN_CHECK_WINDOW = 50\n",
    "        self.PATTERN_MIN_REPEATS = 3\n",
    "        self.PATTERN_MAX_LENGTH = 10\n",
    "        self.detected_pattern = None\n",
    "        self.pattern_repeat_count = 0\n",
    "\n",
    "        # === PROBE ACTION CACHE ===\n",
    "        self._cached_probe_action = None\n",
    "        self._cached_probe_dir = None\n",
    "        self._probe_cache_position = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # MARKOV TRANSITION SYSTEM (NEW) - Synchronized with Lua batched format\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_taught_transitions(self, filepath=None):\n",
    "        \"\"\"\n",
    "        Load state-action pairs from human demonstration.\n",
    "        \n",
    "        Lua format:\n",
    "        {\n",
    "            \"batches\": [\n",
    "                {\n",
    "                    \"batch_type\": \"action_change\" | \"steady\",\n",
    "                    \"trigger_action\": \"UP\",  // only for action_change\n",
    "                    \"frames\": [\n",
    "                        {\n",
    "                            \"state\": {\"map_id\", \"x\", \"y\", \"direction\", \"in_battle\", \"in_menu\"},\n",
    "                            \"action\": \"UP\",\n",
    "                            \"recent_actions\": [\"DOWN\", \"LEFT\", ...],\n",
    "                            \"frame_offset\": 0\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            \"metadata\": {\"total_frames\", \"action_changes\", \"maps_visited\"}\n",
    "        }\n",
    "        \"\"\"\n",
    "        filepath = filepath or TAUGHT_TRANSITIONS_FILE\n",
    "        try:\n",
    "            if Path(filepath).exists():\n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Flatten batches into transitions list\n",
    "                self.taught_transitions = []\n",
    "                self.taught_batches = data.get('batches', [])\n",
    "                \n",
    "                for batch in self.taught_batches:\n",
    "                    batch_type = batch.get('batch_type', 'steady')\n",
    "                    trigger_action = batch.get('trigger_action')\n",
    "                    \n",
    "                    for frame in batch.get('frames', []):\n",
    "                        # Convert to unified transition format\n",
    "                        transition = {\n",
    "                            'state': frame.get('state', {}),\n",
    "                            'action': frame.get('action'),\n",
    "                            'recent_actions': frame.get('recent_actions', []),\n",
    "                            'frame_offset': frame.get('frame_offset', 0),\n",
    "                            'batch_type': batch_type,\n",
    "                            'trigger_action': trigger_action\n",
    "                        }\n",
    "                        self.taught_transitions.append(transition)\n",
    "                \n",
    "                # Store metadata\n",
    "                self.taught_metadata = data.get('metadata', {})\n",
    "                \n",
    "                print(f\"  ðŸ“š Loaded taught transitions:\")\n",
    "                print(f\"     Batches: {len(self.taught_batches)}\")\n",
    "                print(f\"     Frames: {len(self.taught_transitions)}\")\n",
    "                print(f\"     Action changes: {self.taught_metadata.get('action_changes', 0)}\")\n",
    "                print(f\"     Maps visited: {self.taught_metadata.get('maps_visited', [])}\")\n",
    "            else:\n",
    "                self.taught_transitions = []\n",
    "                self.taught_batches = []\n",
    "                self.taught_metadata = {}\n",
    "                print(f\"  No taught transitions file found at {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading taught transitions: {e}\")\n",
    "            self.taught_transitions = []\n",
    "            self.taught_batches = []\n",
    "            self.taught_metadata = {}\n",
    "    \n",
    "    def extract_partial_context(self, context_state, raw_position=None):\n",
    "        \"\"\"Extract partial context flags for similarity matching.\"\"\"\n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        \n",
    "        # Check if movement is blocked (recent position stagnation)\n",
    "        movement_blocked = self.get_position_stagnation() > 3\n",
    "        \n",
    "        # Check if near a known transition\n",
    "        near_transition = False\n",
    "        memory = self.get_current_map_memory(current_map)\n",
    "        for t in memory.get('transitions', []):\n",
    "            t_pos = tuple(t['position']) if isinstance(t['position'], list) else t['position']\n",
    "            if abs(raw_x - t_pos[0]) + abs(raw_y - t_pos[1]) <= 2:\n",
    "                near_transition = True\n",
    "                break\n",
    "        \n",
    "        # Check if tile has been probed\n",
    "        tile_probed = not self.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "        \n",
    "        return {\n",
    "            'in_battle': context_state[3] > 0.5,\n",
    "            'in_menu': context_state[4] > 0.5,\n",
    "            'movement_blocked': movement_blocked,\n",
    "            'near_transition': near_transition,\n",
    "            'tile_probed': tile_probed\n",
    "        }\n",
    "    \n",
    "    def compute_markov_similarity(self, context_state, raw_position=None):\n",
    "        \"\"\"\n",
    "        Compute similarity between current state and all taught transitions.\n",
    "        Returns (best_score, best_action, best_transition_idx)\n",
    "        \n",
    "        Similarity = 0.5 * immediate + 0.3 * sequential + 0.2 * partial\n",
    "        \n",
    "        Prioritizes action_change batches over steady batches.\n",
    "        \"\"\"\n",
    "        if not self.taught_transitions:\n",
    "            return 0.0, None, -1\n",
    "        \n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        current_dir = int(context_state[5])\n",
    "        in_battle = context_state[3] > 0.5\n",
    "        in_menu = context_state[4] > 0.5\n",
    "        \n",
    "        # Get current action history for sequential matching\n",
    "        current_actions = list(self.action_history)\n",
    "        \n",
    "        # Get current partial context\n",
    "        current_partial = self.extract_partial_context(context_state, raw_position)\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_action = None\n",
    "        best_idx = -1\n",
    "        \n",
    "        for idx, transition in enumerate(self.taught_transitions):\n",
    "            t_state = transition.get('state', {})\n",
    "            t_action = transition.get('action')\n",
    "            t_recent = transition.get('recent_actions', [])\n",
    "            batch_type = transition.get('batch_type', 'steady')\n",
    "            \n",
    "            # Skip if no action\n",
    "            if not t_action or t_action == \"NONE\":\n",
    "                continue\n",
    "            \n",
    "            # === IMMEDIATE SIMILARITY (50%) ===\n",
    "            immediate_score = 0.0\n",
    "            \n",
    "            # Map match (required - if different map, skip)\n",
    "            if t_state.get('map_id') != current_map:\n",
    "                continue\n",
    "            immediate_score += 0.25\n",
    "            \n",
    "            # Position proximity\n",
    "            t_x = t_state.get('x', 0)\n",
    "            t_y = t_state.get('y', 0)\n",
    "            pos_dist = abs(raw_x - t_x) + abs(raw_y - t_y)\n",
    "            \n",
    "            if pos_dist == 0:\n",
    "                immediate_score += MARKOV_POS_EXACT_BONUS\n",
    "            elif pos_dist <= 2:\n",
    "                immediate_score += MARKOV_POS_NEAR_BONUS\n",
    "            elif pos_dist <= MARKOV_POS_MAX_DIST:\n",
    "                immediate_score += MARKOV_POS_FAR_BONUS\n",
    "            else:\n",
    "                continue  # Too far, skip\n",
    "            \n",
    "            # Direction match\n",
    "            if t_state.get('direction') == current_dir:\n",
    "                immediate_score += 0.2\n",
    "            \n",
    "            # Battle/menu state match (Lua sends 0/1 integers)\n",
    "            t_in_battle = t_state.get('in_battle', 0) == 1\n",
    "            t_in_menu = t_state.get('in_menu', 0) == 1\n",
    "            \n",
    "            if t_in_battle == in_battle:\n",
    "                immediate_score += 0.1\n",
    "            if t_in_menu == in_menu:\n",
    "                immediate_score += 0.1\n",
    "            \n",
    "            # === SEQUENTIAL SIMILARITY (30%) ===\n",
    "            sequential_score = 0.0\n",
    "            \n",
    "            if t_recent and current_actions:\n",
    "                # Check 8-action match\n",
    "                if len(current_actions) >= 8 and len(t_recent) >= 8:\n",
    "                    if list(current_actions)[-8:] == t_recent[-8:]:\n",
    "                        sequential_score = MARKOV_SEQ_FULL_WEIGHT\n",
    "                \n",
    "                # Check 5-action match\n",
    "                if sequential_score < MARKOV_SEQ_MEDIUM_WEIGHT:\n",
    "                    if len(current_actions) >= 5 and len(t_recent) >= 5:\n",
    "                        if list(current_actions)[-5:] == t_recent[-5:]:\n",
    "                            sequential_score = MARKOV_SEQ_MEDIUM_WEIGHT\n",
    "                \n",
    "                # Check 3-action match\n",
    "                if sequential_score < MARKOV_SEQ_SHORT_WEIGHT:\n",
    "                    if len(current_actions) >= 3 and len(t_recent) >= 3:\n",
    "                        if list(current_actions)[-3:] == t_recent[-3:]:\n",
    "                            sequential_score = MARKOV_SEQ_SHORT_WEIGHT\n",
    "            \n",
    "            # === PARTIAL SIMILARITY (20%) ===\n",
    "            partial_score = 0.0\n",
    "            partial_matches = 0\n",
    "            partial_total = 2  # Only use in_battle and in_menu from Lua data\n",
    "            \n",
    "            if t_in_battle == current_partial['in_battle']:\n",
    "                partial_matches += 1\n",
    "            if t_in_menu == current_partial['in_menu']:\n",
    "                partial_matches += 1\n",
    "            \n",
    "            partial_score = partial_matches / partial_total\n",
    "            \n",
    "            # === COMBINED SCORE ===\n",
    "            total_score = (\n",
    "                MARKOV_IMMEDIATE_WEIGHT * immediate_score +\n",
    "                MARKOV_SEQUENTIAL_WEIGHT * sequential_score +\n",
    "                MARKOV_PARTIAL_WEIGHT * partial_score\n",
    "            )\n",
    "            \n",
    "            # Boost action_change batches (more intentional actions)\n",
    "            if batch_type == \"action_change\":\n",
    "                total_score *= 1.2\n",
    "            \n",
    "            # Prefer frame_offset 0 (the actual decision point)\n",
    "            if transition.get('frame_offset', 0) == 0:\n",
    "                total_score *= 1.1\n",
    "            \n",
    "            if total_score > best_score:\n",
    "                best_score = total_score\n",
    "                best_action = t_action\n",
    "                best_idx = idx\n",
    "        \n",
    "        return best_score, best_action, best_idx\n",
    "    \n",
    "    def get_markov_action(self, context_state, raw_position=None):\n",
    "        \"\"\"\n",
    "        Check if we should use Markov imitation.\n",
    "        Returns (should_use_markov, action, confidence)\n",
    "        \"\"\"\n",
    "        if not self.markov_enabled or not self.taught_transitions:\n",
    "            return False, None, 0.0\n",
    "        \n",
    "        score, action, idx = self.compute_markov_similarity(context_state, raw_position)\n",
    "        \n",
    "        self.last_markov_score = score\n",
    "        \n",
    "        if score >= MARKOV_FAMILIARITY_THRESHOLD:\n",
    "            self.last_markov_action = action\n",
    "            return True, action, score\n",
    "        \n",
    "        return False, None, score\n",
    "\n",
    "    # =========================================================================\n",
    "    # ACTION EXECUTION CONFIRMATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def set_pending_action(self, action_name):\n",
    "        self.pending_action = action_name\n",
    "        self.pending_action_frames = 0\n",
    "    \n",
    "    def confirm_action_executed(self, context_state, prev_context_state):\n",
    "        if self.pending_action is None:\n",
    "            return True\n",
    "        self.pending_action_frames += 1\n",
    "        action_executed = False\n",
    "        if prev_context_state is not None:\n",
    "            if self.pending_action in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "                pos_changed = (context_state[0] != prev_context_state[0] or \n",
    "                              context_state[1] != prev_context_state[1])\n",
    "                dir_changed = context_state[5] != prev_context_state[5]\n",
    "                action_executed = pos_changed or dir_changed\n",
    "            elif self.pending_action in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "                menu_changed = abs(context_state[4] - prev_context_state[4]) > 0.1\n",
    "                battle_changed = context_state[3] != prev_context_state[3]\n",
    "                map_changed = context_state[2] != prev_context_state[2]\n",
    "                action_executed = menu_changed or battle_changed or map_changed\n",
    "        if action_executed or self.pending_action_frames >= self.ACTION_CONFIRM_FRAMES:\n",
    "            self.last_confirmed_action = self.pending_action\n",
    "            self.pending_action = None\n",
    "            self.pending_action_frames = 0\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def should_send_new_action(self):\n",
    "        return self.pending_action is None or self.pending_action_frames >= self.ACTION_CONFIRM_FRAMES\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION MEMORY PERSISTENCE\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_exploration_memory(self):\n",
    "        try:\n",
    "            if self.EXPLORATION_MEMORY_FILE.exists():\n",
    "                with open(self.EXPLORATION_MEMORY_FILE, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.exploration_memory = {}\n",
    "                    for map_key, map_data in data.items():\n",
    "                        map_id = int(map_key.replace('map_', ''))\n",
    "                        self.exploration_memory[map_id] = self._deserialize_map_memory(map_data)\n",
    "                print(f\"  Loaded exploration memory: {len(self.exploration_memory)} maps\")\n",
    "            else:\n",
    "                self.exploration_memory = {}\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading exploration memory: {e}\")\n",
    "            self.exploration_memory = {}\n",
    "\n",
    "    def _deserialize_map_memory(self, map_data):\n",
    "        memory = {\n",
    "            'visited_tiles': set(tuple(t) for t in map_data.get('visited_tiles', [])),\n",
    "            'obstructions': set(tuple(t) for t in map_data.get('obstructions', [])),\n",
    "            'interactable_objects': map_data.get('interactable_objects', []),\n",
    "            'last_visited_timestep': map_data.get('last_visited_timestep', 0),\n",
    "            'transitions': map_data.get('transitions', []),\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': {}\n",
    "        }\n",
    "        for tile_key, tile_data in map_data.get('tile_interactions', {}).items():\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(tile_data.get('directions_tried', [])),\n",
    "                'direction_attempts': {int(k): v for k, v in tile_data.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {int(k): v for k, v in tile_data.get('direction_successes', {}).items()},\n",
    "                'exhausted': tile_data.get('exhausted', False)\n",
    "            }\n",
    "        return memory\n",
    "\n",
    "    def save_exploration_memory(self):\n",
    "        try:\n",
    "            data = {f'map_{mid}': self._serialize_map_memory(md) for mid, md in self.exploration_memory.items()}\n",
    "            with open(self.EXPLORATION_MEMORY_FILE, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving exploration memory: {e}\")\n",
    "\n",
    "    def _serialize_map_memory(self, map_data):\n",
    "        serialized_ti = {}\n",
    "        for tile_key, td in map_data.get('tile_interactions', {}).items():\n",
    "            serialized_ti[tile_key] = {\n",
    "                'directions_tried': list(td.get('directions_tried', set())),\n",
    "                'direction_attempts': {str(k): v for k, v in td.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {str(k): v for k, v in td.get('direction_successes', {}).items()},\n",
    "                'exhausted': td.get('exhausted', False)\n",
    "            }\n",
    "        return {\n",
    "            'visited_tiles': list(map_data['visited_tiles']),\n",
    "            'obstructions': list(map_data['obstructions']),\n",
    "            'interactable_objects': map_data['interactable_objects'],\n",
    "            'last_visited_timestep': map_data['last_visited_timestep'],\n",
    "            'transitions': map_data.get('transitions', []),\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': serialized_ti\n",
    "        }\n",
    "\n",
    "    def get_current_map_memory(self, map_id):\n",
    "        if map_id not in self.exploration_memory:\n",
    "            self.exploration_memory[map_id] = {\n",
    "                'visited_tiles': set(), 'obstructions': set(), 'interactable_objects': [],\n",
    "                'last_visited_timestep': self.timestep, 'transitions': [], 'temp_debt': 0.0,\n",
    "                'tile_interactions': {}\n",
    "            }\n",
    "        return self.exploration_memory[map_id]\n",
    "\n",
    "    def record_visited_tile(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['visited_tiles'].add((int(x), int(y)))\n",
    "        memory['last_visited_timestep'] = self.timestep\n",
    "\n",
    "    def record_obstruction(self, x, y, map_id, direction):\n",
    "        dx, dy = self.DIRECTION_DELTAS_INT.get(direction, (0, 0))\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['obstructions'].add((int(x + dx), int(y + dy)))\n",
    "\n",
    "    # =========================================================================\n",
    "    # TILE-BASED INTERACTION PROBING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_tile_interaction_key(self, x, y):\n",
    "        return f\"{int(x)}_{int(y)}\"\n",
    "    \n",
    "    def get_tile_interaction_state(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_key = self.get_tile_interaction_key(x, y)\n",
    "        if tile_key not in memory['tile_interactions']:\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(),\n",
    "                'direction_attempts': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'direction_successes': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'exhausted': False\n",
    "            }\n",
    "        return memory['tile_interactions'][tile_key]\n",
    "    \n",
    "    def should_interact_at_tile(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if tile_state['exhausted']:\n",
    "            return False\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return True\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            successes = tile_state['direction_successes'].get(d, 0)\n",
    "            if attempts > 0 and successes / attempts >= self.MIN_SUCCESS_RATE_THRESHOLD:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_untried_directions(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        return [d for d in range(4) if d not in tile_state['directions_tried']]\n",
    "    \n",
    "    def get_best_interaction_direction(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        untried = self.get_untried_directions(x, y, map_id)\n",
    "        if untried:\n",
    "            return untried[0]\n",
    "        best_dir, best_rate = None, 0.0\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            if attempts > 0:\n",
    "                rate = tile_state['direction_successes'].get(d, 0) / attempts\n",
    "                if rate > best_rate:\n",
    "                    best_rate, best_dir = rate, d\n",
    "        return best_dir\n",
    "    \n",
    "    def get_best_probe_action(self, raw_x, raw_y, current_map, current_dir):\n",
    "        \"\"\"Cached version - returns (action, target_direction) for tile probing.\"\"\"\n",
    "        cache_key = (raw_x, raw_y, current_map, current_dir)\n",
    "        \n",
    "        if self._probe_cache_position == cache_key:\n",
    "            return self._cached_probe_action, self._cached_probe_dir\n",
    "        \n",
    "        if not self.should_interact_at_tile(raw_x, raw_y, current_map):\n",
    "            result = (None, None)\n",
    "        else:\n",
    "            untried = self.get_untried_directions(raw_x, raw_y, current_map)\n",
    "            if not untried:\n",
    "                best_dir = self.get_best_interaction_direction(raw_x, raw_y, current_map)\n",
    "                if best_dir is not None:\n",
    "                    result = ('A', current_dir) if current_dir == best_dir else (self.INT_TO_ACTION[best_dir], best_dir)\n",
    "                else:\n",
    "                    result = (None, None)\n",
    "            elif current_dir in untried:\n",
    "                result = ('A', current_dir)\n",
    "            else:\n",
    "                target_dir = untried[0]\n",
    "                result = (self.INT_TO_ACTION[target_dir], target_dir)\n",
    "        \n",
    "        self._probe_cache_position = cache_key\n",
    "        self._cached_probe_action, self._cached_probe_dir = result\n",
    "        return result\n",
    "    \n",
    "    def record_tile_interaction_attempt(self, x, y, map_id, direction, success):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        tile_state['directions_tried'].add(direction)\n",
    "        tile_state['direction_attempts'][direction] = tile_state['direction_attempts'].get(direction, 0) + 1\n",
    "        if success:\n",
    "            tile_state['direction_successes'][direction] = tile_state['direction_successes'].get(direction, 0) + 1\n",
    "            memory = self.get_current_map_memory(map_id)\n",
    "            dir_name = self.DIRECTION_NAMES.get(direction, str(direction))\n",
    "            interactable = [int(x), int(y), dir_name]\n",
    "            if interactable not in memory['interactable_objects']:\n",
    "                memory['interactable_objects'].append(interactable)\n",
    "                print(f\"  ðŸŽ¯ INTERACTABLE FOUND: ({x}, {y}) facing {dir_name}\")\n",
    "        self._check_tile_exhaustion(x, y, map_id)\n",
    "    \n",
    "    def _check_tile_exhaustion(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return\n",
    "        if not any(tile_state['direction_successes'].get(d, 0) > 0 for d in range(4)):\n",
    "            tile_state['exhausted'] = True\n",
    "            print(f\"  âœ“ Tile ({x}, {y}) exhausted - no interactions found\")\n",
    "    \n",
    "    def get_direction_success_rate(self, x, y, map_id, direction):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        attempts = tile_state['direction_attempts'].get(direction, 0)\n",
    "        if attempts == 0:\n",
    "            return None\n",
    "        return tile_state['direction_successes'].get(direction, 0) / attempts\n",
    "    \n",
    "    def start_interaction_verification(self, x, y, map_id, direction):\n",
    "        self.pending_interaction_verify = {'x': x, 'y': y, 'map_id': map_id, 'direction': direction}\n",
    "        self.interaction_verify_countdown = self.INTERACTION_VERIFY_FRAMES\n",
    "    \n",
    "    def check_interaction_verification(self, context_state, prev_context_state):\n",
    "        if self.pending_interaction_verify is None:\n",
    "            return\n",
    "        self.interaction_verify_countdown -= 1\n",
    "        success = False\n",
    "        if prev_context_state is not None:\n",
    "            menu_changed = abs(context_state[4] - prev_context_state[4]) > 0.1\n",
    "            battle_started = context_state[3] > 0.5 and prev_context_state[3] <= 0.5\n",
    "            map_changed = int(context_state[2]) != int(prev_context_state[2])\n",
    "            success = menu_changed or battle_started or map_changed\n",
    "        if success or self.interaction_verify_countdown <= 0:\n",
    "            info = self.pending_interaction_verify\n",
    "            self.record_tile_interaction_attempt(info['x'], info['y'], info['map_id'], info['direction'], success)\n",
    "            self.pending_interaction_verify = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITION SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def record_transition(self, from_pos, from_map, to_map, direction, action_type):\n",
    "        memory = self.get_current_map_memory(from_map)\n",
    "        for t in memory['transitions']:\n",
    "            if t['position'] == from_pos and t['direction'] == direction:\n",
    "                t['use_count'] += 1\n",
    "                t['last_used'] = self.timestep\n",
    "                return\n",
    "        memory['transitions'].append({\n",
    "            'position': from_pos, 'direction': direction, 'action': action_type,\n",
    "            'destination_map': to_map, 'use_count': 1, 'last_used': self.timestep\n",
    "        })\n",
    "        print(f\"  ðŸšª TRANSITION FOUND: Map {from_map} ({from_pos}) â†’ Map {to_map}\")\n",
    "\n",
    "    def get_transition_attraction(self, current_map):\n",
    "        memory = self.get_current_map_memory(current_map)\n",
    "        transitions = memory.get('transitions', [])\n",
    "        if not transitions:\n",
    "            return 0.0, None\n",
    "        current_debt = self.map_novelty_debt.get(current_map, 0.0)\n",
    "        current_temp_debt = self.get_temp_debt(current_map)\n",
    "        current_coverage = self.get_exploration_coverage(current_map)\n",
    "        best_attraction, best_transition = 0.0, None\n",
    "        for t in transitions:\n",
    "            if self.is_transition_banned(current_map, t['position'], t['direction']):\n",
    "                continue\n",
    "            dest_map = t['destination_map']\n",
    "            dest_debt = self.map_novelty_debt.get(dest_map, 0.0)\n",
    "            dest_temp_debt = self.get_temp_debt(dest_map)\n",
    "            dest_coverage = self.get_exploration_coverage(dest_map)\n",
    "            debt_diff = (current_debt + current_temp_debt * 2.0) - (dest_debt + dest_temp_debt * 2.0)\n",
    "            coverage_diff = current_coverage - dest_coverage\n",
    "            attraction = debt_diff * 0.5 + coverage_diff * 0.5\n",
    "            if t['use_count'] < 3:\n",
    "                attraction *= 1.5\n",
    "            if attraction > best_attraction:\n",
    "                best_attraction, best_transition = attraction, t\n",
    "        return best_attraction * self.TRANSITION_ATTRACTION_WEIGHT, best_transition\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITION BAN SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def create_transition_ban(self, map_id, tile_pos, direction_back):\n",
    "        self.transition_bans[map_id] = {\n",
    "            'banned_tile': tile_pos, 'banned_direction': direction_back,\n",
    "            'vicinity_radius': self.BAN_VICINITY_RADIUS, 'vicinity_active': False,\n",
    "            'created_at': self.timestep\n",
    "        }\n",
    "        print(f\"  ðŸš« TRANSITION BAN: Map {map_id} at {tile_pos} facing {self.DIRECTION_NAMES.get(direction_back, '?')}\")\n",
    "    \n",
    "    def is_transition_banned(self, map_id, position, direction):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return False\n",
    "        ban = self.transition_bans[map_id]\n",
    "        banned_tile = tuple(ban['banned_tile']) if isinstance(ban['banned_tile'], list) else ban['banned_tile']\n",
    "        position = tuple(position) if isinstance(position, list) else position\n",
    "        if position == banned_tile and direction == ban['banned_direction']:\n",
    "            return True\n",
    "        if ban['vicinity_active']:\n",
    "            dist = abs(position[0] - banned_tile[0]) + abs(position[1] - banned_tile[1])\n",
    "            if dist <= ban['vicinity_radius'] and direction == ban['banned_direction']:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_position_banned(self, map_id, x, y, direction):\n",
    "        return self.is_transition_banned(map_id, (x, y), direction)\n",
    "    \n",
    "    def update_transition_ban(self, map_id, current_pos):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return\n",
    "        ban = self.transition_bans[map_id]\n",
    "        banned_tile = tuple(ban['banned_tile']) if isinstance(ban['banned_tile'], list) else ban['banned_tile']\n",
    "        if not ban['vicinity_active'] and abs(current_pos[0] - banned_tile[0]) + abs(current_pos[1] - banned_tile[1]) >= 3:\n",
    "            ban['vicinity_active'] = True\n",
    "            print(f\"  ðŸš« VICINITY BAN ACTIVE: Map {map_id}\")\n",
    "    \n",
    "    def check_ban_lift_conditions(self, map_id):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return\n",
    "        ban = self.transition_bans[map_id]\n",
    "        should_lift, reason = False, \"\"\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        non_banned = [t for t in memory.get('transitions', []) if not self.is_transition_banned(map_id, t['position'], t['direction'])]\n",
    "        if non_banned:\n",
    "            should_lift, reason = True, \"alternative transition found\"\n",
    "        elif self.get_exploration_coverage(map_id) >= self.BAN_COVERAGE_LIFT_THRESHOLD:\n",
    "            should_lift, reason = True, f\"coverage reached\"\n",
    "        elif self.timestep - ban['created_at'] >= self.BAN_TIMEOUT_STEPS:\n",
    "            should_lift, reason = True, \"timeout\"\n",
    "        if should_lift:\n",
    "            del self.transition_bans[map_id]\n",
    "            print(f\"  âœ… BAN LIFTED: Map {map_id} - {reason}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # DEBT SYSTEMS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        raw_debt = memory.get('temp_debt', 0.0)\n",
    "        if map_id != self.current_map_id:\n",
    "            steps_away = self.timestep - memory.get('last_visited_timestep', 0)\n",
    "            return max(0.0, raw_debt - steps_away * self.TEMP_DEBT_DECAY)\n",
    "        return raw_debt\n",
    "\n",
    "    def accumulate_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['temp_debt'] = min(self.TEMP_DEBT_MAX, memory.get('temp_debt', 0.0) + self.TEMP_DEBT_ACCUMULATION)\n",
    "\n",
    "    def decay_all_debts(self):\n",
    "        \"\"\"Decay debts for non-current locations.\"\"\"\n",
    "        for map_id in list(self.map_novelty_debt.keys()):\n",
    "            if map_id != self.current_map_id:\n",
    "                self.map_novelty_debt[map_id] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.map_novelty_debt[map_id] < 0.1:\n",
    "                    del self.map_novelty_debt[map_id]\n",
    "        \n",
    "        current_loc = None\n",
    "        if self.current_map_id is not None and len(self.last_positions) > 0:\n",
    "            pos = self.last_positions[-1]\n",
    "            current_loc = self.get_location_key(pos[0], pos[1], self.current_map_id)\n",
    "        \n",
    "        for loc in list(self.location_novelty.keys()):\n",
    "            if loc != current_loc:\n",
    "                self.location_novelty[loc] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.location_novelty[loc] < 0.1:\n",
    "                    del self.location_novelty[loc]\n",
    "\n",
    "    def get_exploration_coverage(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        visited = len(memory['visited_tiles'])\n",
    "        obstructions = len(memory['obstructions'])\n",
    "        if visited == 0 or visited + obstructions < 10:\n",
    "            return 0.0\n",
    "        return visited / (visited + obstructions)\n",
    "\n",
    "    def detect_obstruction(self, prev_context, context_state, raw_position, prev_raw_position):\n",
    "        if prev_context is None or prev_raw_position is None:\n",
    "            return False\n",
    "        if self.last_action not in ['UP', 'DOWN', 'LEFT', 'RIGHT']:\n",
    "            return False\n",
    "        if raw_position == prev_raw_position:\n",
    "            self.record_obstruction(raw_position[0], raw_position[1], int(context_state[2]), int(context_state[5]))\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # =========================================================================\n",
    "    # MENU TRAP B-BOOST\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_menu_trap_tracking(self, context_state, action_taken, raw_position=None):\n",
    "        current_pos = raw_position if raw_position else (round(context_state[0] * 255), round(context_state[1] * 255))\n",
    "        if self.menu_trap_position is not None and current_pos != self.menu_trap_position:\n",
    "            self.reset_menu_trap_boost()\n",
    "            return\n",
    "        if self.get_context_state_hash(context_state) == self.last_context_state_hash:\n",
    "            if action_taken in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "                self.menu_trap_frames += 1\n",
    "                self.menu_trap_position = current_pos\n",
    "                if self.menu_trap_frames > self.MENU_TRAP_THRESHOLD:\n",
    "                    if self.original_b_utility is None:\n",
    "                        for a in self.actions():\n",
    "                            if a.action == 'B':\n",
    "                                self.original_b_utility = a.utility\n",
    "                                break\n",
    "                    self.menu_trap_b_boost = min(self.B_BOOST_MAX, self.menu_trap_b_boost + self.B_BOOST_INCREMENT)\n",
    "        elif current_pos != self.menu_trap_position:\n",
    "            self.reset_menu_trap_boost()\n",
    "\n",
    "    def reset_menu_trap_boost(self):\n",
    "        if self.menu_trap_b_boost > 1.0 and self.original_b_utility is not None:\n",
    "            for a in self.actions():\n",
    "                if a.action == 'B':\n",
    "                    a.utility = self.original_b_utility\n",
    "                    break\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.original_b_utility = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # STANDARD METHODS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def add(self, p):\n",
    "        self.perceptrons.append(p)\n",
    "        self._cache_valid = False\n",
    "\n",
    "    def actions(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"action\"]\n",
    "\n",
    "    def entities(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"entity\"]\n",
    "\n",
    "    def get_location_key(self, x, y, map_id, bin_size=5):\n",
    "        return (int(map_id), int(x // bin_size) * bin_size, int(y // bin_size) * bin_size)\n",
    "\n",
    "    def is_near_map_edge(self, x, y):\n",
    "        return x < 10 or x > 245 or y < 10 or y > 245\n",
    "\n",
    "    def record_action_execution(self, action_name):\n",
    "        if action_name:\n",
    "            self.action_execution_count[action_name] = self.action_execution_count.get(action_name, 0) + 1\n",
    "\n",
    "    def get_position_stagnation(self):\n",
    "        if len(self.last_positions) < 2:\n",
    "            return 0\n",
    "        current_pos = self.last_positions[-1]\n",
    "        return sum(1 for pos in reversed(list(self.last_positions)[:-1]) if pos == current_pos)\n",
    "\n",
    "    def get_group_weight(self, group):\n",
    "        return sum(a.utility for a in self.actions() if a.group == group)\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODE SWAP & STAGNATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_context_state_hash(self, context_state):\n",
    "        return (round(context_state[0], 2), round(context_state[1], 2), int(context_state[2]),\n",
    "                int(context_state[3]), round(context_state[4], 2), int(context_state[5]))\n",
    "\n",
    "    def check_state_stagnation(self, context_state):\n",
    "        current_hash = self.get_context_state_hash(context_state)\n",
    "        if current_hash == self.last_context_state_hash:\n",
    "            self.state_stagnation_count += 1\n",
    "            if self.state_stagnation_count == 1 and self.last_action:\n",
    "                self.stagnation_initiator_action = self.last_action\n",
    "        else:\n",
    "            self.state_stagnation_count = 0\n",
    "            self.stagnation_initiator_action = None\n",
    "        self.last_context_state_hash = current_hash\n",
    "        return self.state_stagnation_count >= self.STATE_STAGNATION_THRESHOLD\n",
    "\n",
    "    def check_direction_change_progress(self, context_state):\n",
    "        current_dir = int(context_state[5])\n",
    "        if self.last_direction_for_progress is None:\n",
    "            self.last_direction_for_progress = current_dir\n",
    "            return False\n",
    "        changed = current_dir != self.last_direction_for_progress\n",
    "        self.last_direction_for_progress = current_dir\n",
    "        return changed\n",
    "\n",
    "    def apply_stagnation_initiator_penalty(self):\n",
    "        if self.stagnation_initiator_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.stagnation_initiator_action:\n",
    "                old_util = a.utility\n",
    "                a.utility *= self.STAGNATION_INITIATOR_PENALTY\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                a.utility = max(a.utility, floor)\n",
    "                print(f\"  ðŸ“ STAGNATION PENALTY: {self.stagnation_initiator_action} {old_util:.3f} â†’ {a.utility:.3f}\")\n",
    "                break\n",
    "        self.stagnation_initiator_action = None\n",
    "\n",
    "    def check_productive_change(self, context_state):\n",
    "        current_map = int(context_state[2])\n",
    "        current_battle = context_state[3] > 0.5\n",
    "        current_pos = (context_state[0], context_state[1])\n",
    "        productive, reason = False, \"\"\n",
    "        \n",
    "        if self.last_map_id is not None and current_map != self.last_map_id:\n",
    "            productive, reason = True, \"map change\"\n",
    "        if self.last_battle_state is not None and current_battle != self.last_battle_state:\n",
    "            productive, reason = True, \"battle change\"\n",
    "        if self.position_at_mode_swap is not None:\n",
    "            dist = np.sqrt((current_pos[0] - self.position_at_mode_swap[0])**2 + \n",
    "                          (current_pos[1] - self.position_at_mode_swap[1])**2)\n",
    "            if dist > 0.03:\n",
    "                productive, reason = True, f\"moved {dist*255:.1f} tiles\"\n",
    "        \n",
    "        if self.direction_change_counts_as_progress and self.check_direction_change_progress(context_state):\n",
    "            self.state_stagnation_count = max(0, self.state_stagnation_count - 5)\n",
    "        \n",
    "        self.last_map_id = current_map\n",
    "        self.last_battle_state = current_battle\n",
    "        return productive, reason\n",
    "\n",
    "    def on_productive_change(self, reason):\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.swap_chain_count = 0\n",
    "        self.state_stagnation_count = 0\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.unproductive_swap_count = 0\n",
    "\n",
    "    def on_mode_swap(self, from_mode, to_mode):\n",
    "        self.swap_chain_count += 1\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.unproductive_swap_count += 1\n",
    "        if self.unproductive_swap_count >= self.UNPRODUCTIVE_SWAP_THRESHOLD:\n",
    "            self._reset_highest_to_third(to_mode)\n",
    "            self.unproductive_swap_count = 0\n",
    "        if to_mode == \"interact\":\n",
    "            self.interact_to_move_threshold = min(self.MAX_THRESHOLD, self.interact_to_move_threshold + self.THRESHOLD_INCREMENT)\n",
    "        else:\n",
    "            self.move_to_interact_threshold = min(self.MAX_THRESHOLD, self.move_to_interact_threshold + self.THRESHOLD_INCREMENT)\n",
    "\n",
    "    def _reset_highest_to_third(self, mode):\n",
    "        if mode in [\"battle\", \"both\"]:\n",
    "            return\n",
    "        group = \"move\" if mode == \"move\" else \"interact\"\n",
    "        group_actions = [a for a in self.actions() if a.group == group]\n",
    "        if len(group_actions) < 3:\n",
    "            return\n",
    "        sorted_actions = sorted(group_actions, key=lambda a: a.utility, reverse=True)\n",
    "        floor = self.INTERACT_UTILITY_FLOOR if group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "        sorted_actions[0].utility = max(sorted_actions[2].utility * 0.9, floor)\n",
    "\n",
    "    def should_use_both_mode(self):\n",
    "        return (self.state_stagnation_count > self.BOTH_MODE_STAGNATION_THRESHOLD or \n",
    "                self.unproductive_swap_count > self.BOTH_MODE_SWAP_THRESHOLD)\n",
    "\n",
    "    def determine_control_mode(self, context_state, raw_position=None):\n",
    "        if context_state[3] > 0.5:\n",
    "            return \"battle\"\n",
    "        \n",
    "        self.frames_in_current_mode += 1\n",
    "        position_stagnation = self.get_position_stagnation()\n",
    "        \n",
    "        productive, reason = self.check_productive_change(context_state)\n",
    "        if productive:\n",
    "            self.on_productive_change(reason)\n",
    "        \n",
    "        if self.should_use_both_mode():\n",
    "            return \"both\"\n",
    "        \n",
    "        if self.check_state_stagnation(context_state):\n",
    "            self.apply_stagnation_initiator_penalty()\n",
    "            new_mode = \"interact\" if self.control_mode == \"move\" else \"move\"\n",
    "            self.control_mode = new_mode\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.on_mode_swap(self.control_mode, new_mode)\n",
    "            self.state_stagnation_count = 0\n",
    "            return self.control_mode\n",
    "        \n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        \n",
    "        tile_needs_probing = self.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "        untried_directions = self.get_untried_directions(raw_x, raw_y, current_map)\n",
    "        \n",
    "        if tile_needs_probing and untried_directions and self.control_mode == \"move\" and self.frames_in_current_mode >= 3:\n",
    "            self.control_mode = \"interact\"\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.frames_in_current_mode = 0\n",
    "            return self.control_mode\n",
    "        \n",
    "        if self.control_mode == \"move\" and position_stagnation >= self.move_to_interact_threshold:\n",
    "            self.control_mode = \"interact\"\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.on_mode_swap(\"move\", \"interact\")\n",
    "        elif self.control_mode == \"interact\":\n",
    "            if (not tile_needs_probing or not untried_directions) and self.frames_in_current_mode >= 5:\n",
    "                self.control_mode = \"move\"\n",
    "                self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "                self.frames_in_current_mode = 0\n",
    "            elif self.frames_in_current_mode >= self.interact_to_move_threshold:\n",
    "                self.control_mode = \"move\"\n",
    "                self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "                self.on_mode_swap(\"interact\", \"move\")\n",
    "        \n",
    "        return self.control_mode\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION TRACKING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_exploration_tracking(self, context_state, prev_context_state, raw_position=None, prev_raw_position=None):\n",
    "        current_map = int(context_state[2])\n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_pos = (raw_x, raw_y)\n",
    "        \n",
    "        if self.current_map_id is not None and current_map != self.current_map_id:\n",
    "            prev_map = self.current_map_id\n",
    "            if prev_context_state is not None and prev_raw_position is not None:\n",
    "                self.record_transition(prev_raw_position, prev_map, current_map,\n",
    "                    int(prev_context_state[5]), 'interact' if self.last_action == 'A' else 'walk')\n",
    "            if prev_raw_position is not None:\n",
    "                entry_dir = int(context_state[5]) if prev_context_state is not None else 0\n",
    "                self.create_transition_ban(current_map, current_pos, (entry_dir + 2) % 4)\n",
    "            self.on_map_change(current_map)\n",
    "        \n",
    "        self.current_map_id = current_map\n",
    "        self.record_visited_tile(raw_x, raw_y, current_map)\n",
    "        self.accumulate_temp_debt(current_map)\n",
    "        self.update_transition_ban(current_map, current_pos)\n",
    "        self.check_ban_lift_conditions(current_map)\n",
    "        \n",
    "        if prev_context_state is not None and prev_raw_position is not None:\n",
    "            self.detect_obstruction(prev_context_state, context_state, raw_position, prev_raw_position)\n",
    "        \n",
    "        self.check_interaction_verification(context_state, prev_context_state)\n",
    "        self.last_direction = int(context_state[5])\n",
    "        \n",
    "        if self.timestep % 300 == 0:\n",
    "            self.decay_all_debts()\n",
    "\n",
    "    def on_map_change(self, new_map):\n",
    "        self.save_exploration_memory()\n",
    "        self.control_mode = \"move\"\n",
    "        self.frames_in_current_mode = 0\n",
    "        memory = self.get_current_map_memory(new_map)\n",
    "        tile_interactions = memory.get('tile_interactions', {})\n",
    "        print(f\"  ðŸ—ºï¸ MAP CHANGE â†’ {new_map}: {len(memory['visited_tiles'])} visited, {len(memory['obstructions'])} obs\")\n",
    "        print(f\"     Tiles probed: {len(tile_interactions)}, exhausted: {sum(1 for t in tile_interactions.values() if t.get('exhausted', False))}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # REPETITION & PATTERN HANDLING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def track_consecutive_action(self, action_name):\n",
    "        if action_name == self.current_repeated_action:\n",
    "            self.consecutive_action_count += 1\n",
    "        else:\n",
    "            self.current_repeated_action = action_name\n",
    "            self.consecutive_action_count = 1\n",
    "\n",
    "    def get_learning_multiplier(self, action_name):\n",
    "        if action_name != self.current_repeated_action or self.consecutive_action_count < self.LEARNING_SLOWDOWN_START:\n",
    "            return 1.0\n",
    "        progress = min(1.0, (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) / \n",
    "                       (self.LEARNING_SLOWDOWN_MAX - self.LEARNING_SLOWDOWN_START))\n",
    "        return max(0.05, 1.0 - 0.95 * progress)\n",
    "\n",
    "    def get_nth_highest_utility(self, group, n=3):\n",
    "        utilities = sorted([a.utility for a in self.actions() if a.group == group], reverse=True)\n",
    "        if len(utilities) < n:\n",
    "            return self.INTERACT_UTILITY_FLOOR if group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "        return utilities[n-1]\n",
    "\n",
    "    def detect_pattern(self):\n",
    "        if len(self.action_history) < 6:\n",
    "            return None, 0\n",
    "        recent = list(self.action_history)[-self.PATTERN_CHECK_WINDOW:]\n",
    "        for pattern_len in range(1, self.PATTERN_MAX_LENGTH + 1):\n",
    "            if len(recent) < pattern_len * self.PATTERN_MIN_REPEATS:\n",
    "                continue\n",
    "            candidate = tuple(recent[-pattern_len:])\n",
    "            repeat_count, idx = 0, len(recent) - pattern_len\n",
    "            while idx >= 0 and tuple(recent[idx:idx + pattern_len]) == candidate:\n",
    "                repeat_count += 1\n",
    "                idx -= pattern_len\n",
    "            if repeat_count >= self.PATTERN_MIN_REPEATS:\n",
    "                return candidate, repeat_count\n",
    "        return None, 0\n",
    "\n",
    "    def apply_pattern_penalty(self):\n",
    "        pattern, repeat_count = self.detect_pattern()\n",
    "        if pattern is None:\n",
    "            self.detected_pattern, self.pattern_repeat_count = None, 0\n",
    "            return\n",
    "        self.detected_pattern, self.pattern_repeat_count = pattern, repeat_count\n",
    "        for action_name in set(pattern):\n",
    "            group = \"interact\" if action_name in [\"A\", \"B\", \"Start\", \"Select\"] else \"move\"\n",
    "            third_util = self.get_nth_highest_utility(group, n=3)\n",
    "            for a in self.actions():\n",
    "                if a.action == action_name:\n",
    "                    floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                    a.utility = max(third_util * 0.9, floor)\n",
    "                    break\n",
    "\n",
    "    def apply_repetition_penalty(self):\n",
    "        if self.current_repeated_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.current_repeated_action:\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                if self.consecutive_action_count >= self.HARD_RESET_THRESHOLD:\n",
    "                    a.utility = max(self.get_nth_highest_utility(a.group, n=3) * 0.9, floor)\n",
    "                    self.consecutive_action_count = 0\n",
    "                elif self.consecutive_action_count >= self.PENALTY_THRESHOLD:\n",
    "                    a.utility = max(a.utility * 0.7, floor)\n",
    "                break\n",
    "\n",
    "    # =========================================================================\n",
    "    # ENTITY & LEARNING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def spawn_innate_entities(self, learning_state):\n",
    "        if self.innate_entities_spawned:\n",
    "            return\n",
    "        for etype, indices in [(\"sense_menu\", [5, 6]), (\"sense_battle\", [3, 4]), \n",
    "                                (\"sense_movement\", [0, 1]), (\"sense_map_transition\", [2])]:\n",
    "            entity = Perceptron(\"entity\", entity_type=etype)\n",
    "            entity.ensure_weights(len(learning_state))\n",
    "            entity.weights = np.zeros(len(learning_state))\n",
    "            for idx in indices:\n",
    "                entity.weights[idx] = 0.5 if len(indices) > 1 else 1.0\n",
    "            self.add(entity)\n",
    "        self.innate_entities_spawned = True\n",
    "\n",
    "    def enforce_utility_floors(self):\n",
    "        for a in self.actions():\n",
    "            floor = self.MOVE_UTILITY_FLOOR if a.group == \"move\" else self.INTERACT_UTILITY_FLOOR\n",
    "            a.utility = max(a.utility, floor)\n",
    "\n",
    "    def get_spawn_threshold_adaptive(self, error_type='combined', percentile=50):\n",
    "        history = {'numeric': self.numeric_error_history, 'visual': self.visual_error_history}.get(error_type, self.error_history)\n",
    "        return max(0.001, np.percentile(history, percentile)) if len(history) >= 100 else 0.0005\n",
    "\n",
    "    def stagnation_level(self, window=10):\n",
    "        if len(self.prev_learning_states) < window:\n",
    "            return 0.0\n",
    "        recent = list(self.prev_learning_states)[-window:]\n",
    "        return 1.0 - np.tanh(np.mean([np.linalg.norm(recent[i] - recent[i-1]) for i in range(1, len(recent))]) * 2.0)\n",
    "\n",
    "    def predict_future_error(self, state, action, context_state, raw_position=None):\n",
    "        entity_novelty = np.mean([e.predict(state) * e.utility for e in self.entities()]) if self.entities() else 0.5\n",
    "        combined = entity_novelty * 0.7 + action.utility * 0.3\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        \n",
    "        map_debt = min(self.map_novelty_debt.get(current_map, 0.0), self.MAX_MAP_DEBT)\n",
    "        loc_debt = min(self.location_novelty.get(loc, 0.0), self.MAX_LOCATION_DEBT)\n",
    "        total_debt = map_debt + self.get_temp_debt(current_map) + loc_debt * 0.5\n",
    "        combined *= 1.0 / (1.0 + total_debt * 5.0)\n",
    "        \n",
    "        if action.action == self.current_repeated_action and self.consecutive_action_count > self.LEARNING_SLOWDOWN_START:\n",
    "            combined *= 1.0 / (1.0 + (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) * 0.15)\n",
    "        if self.detected_pattern and action.action in self.detected_pattern:\n",
    "            combined *= 1.0 / (1.0 + self.pattern_repeat_count * 0.2)\n",
    "        \n",
    "        return combined + np.random.randn() * 0.05\n",
    "\n",
    "    def compute_multi_modal_error(self, state, next_state):\n",
    "        diffs = [abs(next_state[i] - state[i]) for i in range(min(8, len(state), len(next_state)))]\n",
    "        weights = [0.5, 0.5, 10.0, 5.0, 3.0, 2.0, 1.5, 0.3]\n",
    "        weighted = sum(d * w for d, w in zip(diffs, weights)) + np.linalg.norm(next_state[8:] - state[8:]) * 2.0\n",
    "        numeric = sum(diffs)\n",
    "        visual = np.linalg.norm(next_state[8:] - state[8:])\n",
    "        return weighted, numeric, visual\n",
    "\n",
    "    def learn(self, learning_state, next_learning_state, context_state, next_context_state, dead=False,\n",
    "              raw_position=None, next_raw_position=None):\n",
    "        if learning_state.shape != next_learning_state.shape:\n",
    "            max_dim = max(len(learning_state), len(next_learning_state))\n",
    "            learning_state = np.pad(learning_state, (0, max(0, max_dim - len(learning_state))))\n",
    "            next_learning_state = np.pad(next_learning_state, (0, max(0, max_dim - len(next_learning_state))))\n",
    "        \n",
    "        if not self.innate_entities_spawned:\n",
    "            self.spawn_innate_entities(learning_state)\n",
    "        \n",
    "        prev_context = self.prev_context_states[-1] if self.prev_context_states else None\n",
    "        prev_raw = getattr(self, '_last_raw_position', None)\n",
    "        self.update_exploration_tracking(context_state, prev_context, raw_position, prev_raw)\n",
    "        self._last_raw_position = raw_position\n",
    "        \n",
    "        weighted_error, numeric_error, visual_error = self.compute_multi_modal_error(learning_state, next_learning_state)\n",
    "        self.error_history.append(weighted_error)\n",
    "        self.numeric_error_history.append(numeric_error)\n",
    "        self.visual_error_history.append(visual_error)\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        \n",
    "        self.visited_maps[current_map] = self.visited_maps.get(current_map, 0) + 1\n",
    "        self.location_memory[loc] = self.location_memory.get(loc, 0) + 1\n",
    "        \n",
    "        if self.visited_maps[current_map] > 10:\n",
    "            self.map_novelty_debt[current_map] = min(self.MAX_MAP_DEBT, \n",
    "                self.map_novelty_debt.get(current_map, 0.0) + 0.05 * (self.visited_maps[current_map] - 10))\n",
    "        if self.location_memory[loc] > 15:\n",
    "            self.location_novelty[loc] = min(self.MAX_LOCATION_DEBT,\n",
    "                self.location_novelty.get(loc, 0.0) + 0.1 * (self.location_memory[loc] - 15))\n",
    "        \n",
    "        if self.visited_maps[current_map] > 30:\n",
    "            weighted_error *= 0.5\n",
    "        if self.location_memory[loc] > 25:\n",
    "            weighted_error *= 0.7\n",
    "        \n",
    "        stagnation = self.stagnation_level()\n",
    "        learning_mult = self.get_learning_multiplier(self.last_action) if self.last_action else 1.0\n",
    "        if self.detected_pattern and self.last_action in self.detected_pattern:\n",
    "            learning_mult *= 0.5\n",
    "        \n",
    "        for p in self.perceptrons:\n",
    "            mult = learning_mult if (p.kind == \"action\" and p.action == self.last_action) else 1.0\n",
    "            if p.kind == \"action\" and self.detected_pattern and p.action in self.detected_pattern:\n",
    "                mult *= 0.5\n",
    "            p.update(learning_state, weighted_error * mult, stagnation=stagnation)\n",
    "        \n",
    "        for a in self.actions():\n",
    "            if a.action in ['Start', 'Select'] and a.weights is not None:\n",
    "                a.weights *= 0.999\n",
    "        \n",
    "        self.apply_repetition_penalty()\n",
    "        self.apply_pattern_penalty()\n",
    "        self.enforce_utility_floors()\n",
    "        \n",
    "        if prev_context is not None and np.linalg.norm(context_state[:2] - prev_context[:2]) > 0.001:\n",
    "            if self.last_action:\n",
    "                for a in self.actions():\n",
    "                    if a.action == self.last_action:\n",
    "                        boost = 1.15 if raw_position and self.is_near_map_edge(*raw_position) else 1.08\n",
    "                        a.utility = min(a.utility * boost, 2.0)\n",
    "                        break\n",
    "        \n",
    "        if self.timestep % self.SAVE_INTERVAL == 0:\n",
    "            self.save_exploration_memory()\n",
    "        \n",
    "        self.action_history.append(self.last_action)\n",
    "\n",
    "    def log_state(self, learning_state, context_state):\n",
    "        self.prev_learning_states.append(learning_state)\n",
    "        self.prev_context_states.append(context_state)\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        self.last_positions.append((int(x), int(y)))\n",
    "    \n",
    "    def get_tile_interaction_stats(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_interactions = memory.get('tile_interactions', {})\n",
    "        return {\n",
    "            'probed': len(tile_interactions),\n",
    "            'exhausted': sum(1 for t in tile_interactions.values() if t.get('exhausted', False)),\n",
    "            'with_success': sum(1 for t in tile_interactions.values() if any(t.get('direction_successes', {}).get(d, 0) > 0 for d in range(4)))\n",
    "        }\n",
    "\n",
    "    def load_taught_model(self, filepath):\n",
    "        \"\"\"Load weights, utilities, and debt from taught model.\"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                model = json.load(f)\n",
    "            \n",
    "            # Check if model has required structure\n",
    "            if \"perceptrons\" not in model:\n",
    "                print(f\"  âš ï¸ Model file empty or invalid, starting fresh\")\n",
    "                return 0\n",
    "            \n",
    "            for saved_action in model[\"perceptrons\"][\"actions\"]:\n",
    "                for a in self.actions():\n",
    "                    if a.action == saved_action[\"action\"]:\n",
    "                        a.utility = saved_action[\"utility\"]\n",
    "                        a.learning_rate = saved_action.get(\"learning_rate\", 0.01)\n",
    "                        a.familiarity = saved_action.get(\"familiarity\", 0.0)\n",
    "                        if saved_action.get(\"weights_nonzero\"):\n",
    "                            dim = saved_action.get(\"weights_shape\", 1376)\n",
    "                            a.weights = np.zeros(dim)\n",
    "                            for idx, val in saved_action[\"weights_nonzero\"]:\n",
    "                                if idx < dim:\n",
    "                                    a.weights[idx] = val\n",
    "                        break\n",
    "                    if a.action in ['Start', 'Select'] and a.weights is not None:\n",
    "                        a.weights = np.zeros(len(a.weights))\n",
    "                        a.utility = 0.05\n",
    "            \n",
    "            for saved_entity in model[\"perceptrons\"].get(\"entities\", []):\n",
    "                for e in self.entities():\n",
    "                    if e.entity_type == saved_entity[\"entity_type\"]:\n",
    "                        e.utility = saved_entity.get(\"utility\", 1.0)\n",
    "                        e.familiarity = saved_entity.get(\"familiarity\", 0.0)\n",
    "                        if saved_entity.get(\"weights_nonzero\"):\n",
    "                            dim = saved_entity.get(\"weights_shape\", 1376)\n",
    "                            e.weights = np.zeros(dim)\n",
    "                            for idx, val in saved_entity[\"weights_nonzero\"]:\n",
    "                                if idx < dim:\n",
    "                                    e.weights[idx] = val\n",
    "                        break\n",
    "            \n",
    "            if \"debt_tracking\" in model:\n",
    "                debt = model[\"debt_tracking\"]\n",
    "                self.map_novelty_debt = {int(k): v for k, v in debt.get(\"map_novelty_debt\", {}).items()}\n",
    "                self.visited_maps = {int(k): v for k, v in debt.get(\"visited_maps\", {}).items()}\n",
    "                for k, v in debt.get(\"location_novelty\", {}).items():\n",
    "                    self.location_novelty[eval(k)] = v\n",
    "            \n",
    "            loaded_timestep = model.get(\"timestep\", 0)\n",
    "            self.timestep = loaded_timestep\n",
    "            return loaded_timestep\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  âš ï¸ Error loading model: {e}, starting fresh\")\n",
    "            return 0\n",
    "\n",
    "    def merge_taught_exploration(self, taught_filepath):\n",
    "        \"\"\"Merge transitions and interactables from taught exploration into AI's memory.\"\"\"\n",
    "        if not Path(taught_filepath).exists():\n",
    "            print(f\"  No taught exploration memory found at {taught_filepath}\")\n",
    "            return\n",
    "        \n",
    "        with open(taught_filepath, 'r') as f:\n",
    "            taught_data = json.load(f)\n",
    "        \n",
    "        transitions_added = 0\n",
    "        interactables_added = 0\n",
    "        \n",
    "        for map_key, taught_map in taught_data.items():\n",
    "            map_id = int(map_key.replace('map_', ''))\n",
    "            ai_map = self.get_current_map_memory(map_id)\n",
    "            \n",
    "            for t_trans in taught_map.get('transitions', []):\n",
    "                t_pos = tuple(t_trans['position'])\n",
    "                t_dir = t_trans['direction']\n",
    "                exists = any(\n",
    "                    tuple(existing['position']) == t_pos and existing['direction'] == t_dir\n",
    "                    for existing in ai_map['transitions']\n",
    "                )\n",
    "                if not exists:\n",
    "                    ai_map['transitions'].append(t_trans)\n",
    "                    transitions_added += 1\n",
    "            \n",
    "            for t_inter in taught_map.get('interactable_objects', []):\n",
    "                if t_inter not in ai_map['interactable_objects']:\n",
    "                    ai_map['interactable_objects'].append(t_inter)\n",
    "                    interactables_added += 1\n",
    "        \n",
    "        print(f\"  Merged: {transitions_added} transitions, {interactables_added} interactables\")\n",
    "    \n",
    "    def save_model_checkpoint(self, filepath):\n",
    "        \"\"\"Save current model state.\"\"\"\n",
    "        model = {\n",
    "            \"timestep\": self.timestep,\n",
    "            \"perceptrons\": {\"actions\": [], \"entities\": []},\n",
    "            \"debt_tracking\": {\n",
    "                \"map_novelty_debt\": {str(k): v for k, v in self.map_novelty_debt.items()},\n",
    "                \"location_novelty\": {str(k): v for k, v in self.location_novelty.items()},\n",
    "                \"visited_maps\": {str(k): v for k, v in self.visited_maps.items()}\n",
    "            },\n",
    "            \"control_mode\": self.control_mode,\n",
    "            \"markov_stats\": {  # NEW: Save Markov usage stats\n",
    "                \"markov_action_count\": self.markov_action_count,\n",
    "                \"curiosity_action_count\": self.curiosity_action_count\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for a in self.actions():\n",
    "            action_data = {\n",
    "                \"action\": a.action,\n",
    "                \"group\": a.group,\n",
    "                \"utility\": float(a.utility),\n",
    "                \"weights_shape\": len(a.weights) if a.weights is not None else 0,\n",
    "                \"weights_nonzero\": [[i, float(v)] for i, v in enumerate(a.weights) if abs(v) > 1e-10] if a.weights is not None else [],\n",
    "                \"learning_rate\": float(a.learning_rate),\n",
    "                \"familiarity\": float(a.familiarity)\n",
    "            }\n",
    "            model[\"perceptrons\"][\"actions\"].append(action_data)\n",
    "        \n",
    "        for e in self.entities():\n",
    "            entity_data = {\n",
    "                \"entity_type\": e.entity_type,\n",
    "                \"utility\": float(e.utility),\n",
    "                \"weights_shape\": len(e.weights) if e.weights is not None else 0,\n",
    "                \"weights_nonzero\": [[i, float(v)] for i, v in enumerate(e.weights) if abs(v) > 1e-10] if e.weights is not None else [],\n",
    "                \"familiarity\": float(e.familiarity)\n",
    "            }\n",
    "            model[\"perceptrons\"][\"entities\"].append(entity_data)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c1df11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Action Selection - With Markov Integration\n",
    "# ============================================================================\n",
    "# CHANGES FROM PREVIOUS VERSION:\n",
    "# 1. Added Markov lookup at the start of anticipatory_action()\n",
    "# 2. If Markov confidence >= threshold, use taught action directly\n",
    "# 3. Track markov_action_count vs curiosity_action_count\n",
    "# 4. Markov actions bypass mode restrictions\n",
    "# ============================================================================\n",
    "\n",
    "import random\n",
    "\n",
    "GBA_ACTIONS = [\"Up\", \"Down\", \"Left\", \"Right\", \"A\", \"B\", \"Start\", \"Select\"]\n",
    "ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "DIRECTION_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "ACTION_TO_DIRECTION = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "\n",
    "def manhattan_distance(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "\n",
    "def anticipatory_action(brain, learning_state, context_state, \n",
    "                       exploration_weight=1.3, min_interact_prob=0.15,\n",
    "                       raw_position=None,\n",
    "                       forced_explore_prob=0.18,\n",
    "                       override_threshold=1.5):\n",
    "    \"\"\"\n",
    "    HYBRID ACTION SELECTION:\n",
    "    1. Check Markov (imitation) first - if familiar situation, copy taught action\n",
    "    2. Otherwise use Curiosity (exploration) with confidence-based override\n",
    "    \"\"\"\n",
    "    actions_list = brain.actions()\n",
    "    if not actions_list:\n",
    "        return Perceptron(\"action\", action=\"UP\", group=\"move\")\n",
    "\n",
    "    # === MARKOV CHECK (NEW) ===\n",
    "    use_markov, markov_action, markov_confidence = brain.get_markov_action(\n",
    "        context_state, raw_position\n",
    "    )\n",
    "    \n",
    "    if use_markov and markov_action:\n",
    "        # Find the action perceptron for the Markov-suggested action\n",
    "        for a in actions_list:\n",
    "            if a.action == markov_action:\n",
    "                brain.markov_action_count += 1\n",
    "                brain.record_action_execution(a.action)\n",
    "                brain.track_consecutive_action(a.action)\n",
    "                \n",
    "                # Still start interaction verification if it's an A press\n",
    "                if a.action == 'A':\n",
    "                    raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "                    raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "                    current_map = int(context_state[2])\n",
    "                    if brain.should_interact_at_tile(raw_x, raw_y, current_map):\n",
    "                        brain.start_interaction_verification(\n",
    "                            raw_x, raw_y, current_map, int(context_state[5])\n",
    "                        )\n",
    "                \n",
    "                return a\n",
    "        # If action not found in perceptrons, fall through to curiosity\n",
    "\n",
    "    # === CURIOSITY-DRIVEN SELECTION (existing logic) ===\n",
    "    brain.curiosity_action_count += 1\n",
    "    \n",
    "    mode = brain.determine_control_mode(context_state, raw_position=raw_position)\n",
    "    current_map = int(context_state[2])\n",
    "    current_dir = int(context_state[5])\n",
    "    \n",
    "    raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "    raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "    current_pos = (raw_x, raw_y)\n",
    "    \n",
    "    memory = brain.get_current_map_memory(current_map)\n",
    "    visited_tiles = memory['visited_tiles']\n",
    "    obstructions = memory['obstructions']\n",
    "    \n",
    "    tile_needs_probing = brain.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "    probe_action, probe_dir = brain.get_best_probe_action(raw_x, raw_y, current_map, current_dir)\n",
    "    \n",
    "    transition_attraction, best_transition = brain.get_transition_attraction(current_map)\n",
    "    coverage = brain.get_exploration_coverage(current_map)\n",
    "\n",
    "    # Forced random exploration (exclude Start/Select)\n",
    "    if random.random() < forced_explore_prob:\n",
    "        valid = [a for a in actions_list if a.action not in ['Start', 'Select']]\n",
    "        chosen = random.choice(valid)\n",
    "        brain.record_action_execution(chosen.action)\n",
    "        brain.track_consecutive_action(chosen.action)\n",
    "        if chosen.action == 'A' and tile_needs_probing:\n",
    "            brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "        return chosen\n",
    "\n",
    "    # Score ALL actions\n",
    "    action_scores = {}\n",
    "    \n",
    "    for a in actions_list:\n",
    "        # Skip Start/Select entirely\n",
    "        if a.action in ['Start', 'Select']:\n",
    "            action_scores[a.action] = (a, 0.0)\n",
    "            continue\n",
    "            \n",
    "        predicted = brain.predict_future_error(learning_state, a, context_state, raw_position=raw_position)\n",
    "        \n",
    "        if a.group == \"move\":\n",
    "            predicted *= exploration_weight\n",
    "            \n",
    "            dx, dy = ACTION_DELTAS.get(a.action, (0, 0))\n",
    "            target_tile = (raw_x + dx, raw_y + dy)\n",
    "            action_direction = ACTION_TO_DIRECTION.get(a.action, -1)\n",
    "            \n",
    "            if target_tile not in visited_tiles:\n",
    "                predicted *= brain.UNVISITED_TILE_BONUS\n",
    "            \n",
    "            if target_tile in obstructions:\n",
    "                predicted *= brain.OBSTRUCTION_PENALTY\n",
    "            \n",
    "            if brain.is_position_banned(current_map, raw_x, raw_y, action_direction):\n",
    "                predicted *= 0.05\n",
    "            \n",
    "            if transition_attraction > 0.3 and best_transition and coverage > 0.5:\n",
    "                trans_pos = tuple(best_transition['position']) if isinstance(best_transition['position'], list) else best_transition['position']\n",
    "                if manhattan_distance(target_tile, trans_pos) < manhattan_distance(current_pos, trans_pos):\n",
    "                    predicted *= (1.0 + transition_attraction)\n",
    "            \n",
    "            if probe_action == a.action and probe_dir is not None:\n",
    "                predicted *= 2.0\n",
    "            \n",
    "            predicted *= (0.9 + random.random() * 0.2)\n",
    "        \n",
    "        elif a.group == \"interact\":\n",
    "            predicted = max(predicted, min_interact_prob)\n",
    "            \n",
    "            if a.action == 'B':\n",
    "                predicted *= brain.menu_trap_b_boost\n",
    "            \n",
    "            if a.action == 'A':\n",
    "                if tile_needs_probing and probe_action == 'A':\n",
    "                    predicted *= 3.0\n",
    "                elif tile_needs_probing:\n",
    "                    predicted *= 0.5\n",
    "                else:\n",
    "                    predicted *= 0.3\n",
    "        \n",
    "        action_scores[a.action] = (a, predicted)\n",
    "\n",
    "    # Find best in-mode and best out-of-mode\n",
    "    if mode == \"battle\":\n",
    "        preferred_group = \"interact\"\n",
    "    elif mode == \"interact\":\n",
    "        preferred_group = \"interact\"\n",
    "    else:\n",
    "        preferred_group = \"move\"\n",
    "    \n",
    "    in_mode = [(a, s) for name, (a, s) in action_scores.items() if a.group == preferred_group and s > 0]\n",
    "    out_mode = [(a, s) for name, (a, s) in action_scores.items() if a.group != preferred_group and s > 0 and a.action not in ['Start', 'Select']]\n",
    "    \n",
    "    best_in_mode = max(in_mode, key=lambda x: x[1]) if in_mode else None\n",
    "    best_out_mode = max(out_mode, key=lambda x: x[1]) if out_mode else None\n",
    "    \n",
    "    # Decide: override or stay in mode\n",
    "    chosen = None\n",
    "    \n",
    "    if best_in_mode and best_out_mode:\n",
    "        if best_out_mode[1] > best_in_mode[1] * override_threshold:\n",
    "            chosen = best_out_mode[0]\n",
    "        else:\n",
    "            chosen = best_in_mode[0]\n",
    "    elif best_in_mode:\n",
    "        chosen = best_in_mode[0]\n",
    "    elif best_out_mode:\n",
    "        chosen = best_out_mode[0]\n",
    "    else:\n",
    "        chosen = max(actions_list, key=lambda a: a.utility)\n",
    "    \n",
    "    brain.record_action_execution(chosen.action)\n",
    "    brain.track_consecutive_action(chosen.action)\n",
    "    \n",
    "    if chosen.action == 'A' and tile_needs_probing:\n",
    "        brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "    \n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963a80e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded exploration memory: 0 maps\n",
      "  âš ï¸ Model file empty or invalid, starting fresh\n",
      "ðŸŽ“ TRANSFER: Loaded model from timestep 0\n",
      "   Utilities: ['UP:1.000', 'DOWN:1.000', 'LEFT:1.000', 'RIGHT:1.000', 'A:1.000', 'B:1.000', 'Start:1.000', 'Select:1.000']\n",
      "  Merged: 0 transitions, 0 interactables\n",
      "  ðŸ“š Loaded taught transitions:\n",
      "     Batches: 0\n",
      "     Frames: 0\n",
      "     Action changes: 0\n",
      "     Maps visited: []\n",
      "======================================================================\n",
      "AI CONTROL - v8.0 (Hybrid Markov + Curiosity)\n",
      "======================================================================\n",
      "MARKOV SYSTEM:\n",
      "  - Familiarity threshold: 0.6\n",
      "  - Immediate weight: 0.5\n",
      "  - Sequential weight: 0.3\n",
      "  - Partial weight: 0.2\n",
      "  - Taught batches: 0\n",
      "  - Taught frames: 0\n",
      "  - Action changes recorded: 0\n",
      "  - Maps in teaching: []\n",
      "======================================================================\n",
      "CURIOSITY SYSTEM:\n",
      "  - Forced random exploration: 18%\n",
      "  - 'Both' mode threshold: stagnation > 35\n",
      "  - Unvisited tile bonus: 1.5x\n",
      "  - Obstruction penalty: 0.25x\n",
      "======================================================================\n",
      "PERSISTENT MEMORY: C:\\Users\\HP\\Documents\\cogai\\exploration_memory.json\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Step 0 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: move | Battle: 0 | Stagnation: 0\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 1 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 0 | Obstructions: 0 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "\n",
      "  â³ Pending: DOWN (0/3)\n",
      "\n",
      "  âš¡ Utilities: UP:1.00 DOWN:1.00 LEFT:1.00 RIGHT:1.00 A:1.00 B:1.00 Start:1.00 Select:1.00\n",
      "  ðŸ“ STAGNATION PENALTY: DOWN 0.528 â†’ 0.370\n",
      "\n",
      "======================================================================\n",
      "Step 100 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 13\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 34 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (28 frames)\n",
      "\n",
      "  â³ Pending: RIGHT (1/3)\n",
      "\n",
      "  âš¡ Utilities: B:0.59 Start:0.59 Select:0.59 RIGHT:0.54 A:0.53 UP:0.49 LEFT:0.44 DOWN:0.37\n",
      "\n",
      "  âš ï¸ STAGNATION WARNING: 13/20\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x12\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.528 â†’ 0.370\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.476 â†’ 0.333\n",
      "\n",
      "======================================================================\n",
      "Step 200 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 6\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 67 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (57 frames)\n",
      "\n",
      "  â³ Pending: A (2/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.48 A:0.43 RIGHT:0.36 DOWN:0.33 LEFT:0.32 UP:0.29\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x4\n",
      "  ðŸ“ STAGNATION PENALTY: B 0.476 â†’ 0.333\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.347 â†’ 0.243\n",
      "\n",
      "======================================================================\n",
      "Step 300 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: move | Battle: 0 | Stagnation: 0\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 101 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/1\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (89 frames)\n",
      "\n",
      "  â³ Pending: A (0/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.39 RIGHT:0.36 DOWN:0.33 LEFT:0.29 UP:0.26 A:0.24\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x11\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.347 â†’ 0.243\n",
      "\n",
      "======================================================================\n",
      "Step 400 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 13\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 134 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/2\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (116 frames)\n",
      "\n",
      "  â³ Pending: RIGHT (1/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.39 A:0.35 UP:0.26 LEFT:0.23 RIGHT:0.21 DOWN:0.19\n",
      "\n",
      "  âš ï¸ STAGNATION WARNING: 13/20\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x9\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.347 â†’ 0.243\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.312 â†’ 0.218\n",
      "\n",
      "======================================================================\n",
      "Step 500 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 6\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 167 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/2\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (145 frames)\n",
      "\n",
      "  â³ Pending: A (2/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.35 A:0.31 RIGHT:0.19 DOWN:0.17 LEFT:0.17 UP:0.15\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x10\n",
      "\n",
      "######################################################################\n",
      "# MILESTONE 500\n",
      "# Maps explored: 1\n",
      "# Tiles visited: 1 | Obstructions: 1\n",
      "# Interactables: 0 | Transitions: 0\n",
      "# Tiles probed: 1 | Exhausted: 0\n",
      "#\n",
      "# HYBRID DECISION STATS:\n",
      "#   Markov (imitation): 0 (0.0%)\n",
      "#   Curiosity (explore): 167 (100.0%)\n",
      "#   Taught transitions: 0\n",
      "######################################################################\n",
      "# Model saved\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.227 â†’ 0.159\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.166 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 600 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: move | Battle: 0 | Stagnation: 0\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 201 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/5\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (176 frames)\n",
      "\n",
      "  â³ Pending: A (0/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.18 DOWN:0.17 UP:0.15 A:0.15 LEFT:0.14 RIGHT:0.14\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x5\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.166 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 700 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 13\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 234 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/5\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (204 frames)\n",
      "\n",
      "  â³ Pending: A (1/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.18 A:0.17 UP:0.15 RIGHT:0.12 DOWN:0.11 LEFT:0.11\n",
      "\n",
      "  âš ï¸ STAGNATION WARNING: 13/20\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x3\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.166 â†’ 0.150\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.166 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 800 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 6\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 267 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/6\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (232 frames)\n",
      "\n",
      "  â³ Pending: A (2/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.17 A:0.15 LEFT:0.11 RIGHT:0.10 UP:0.10 DOWN:0.10\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x10\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 900 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: move | Battle: 0 | Stagnation: 0\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 301 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/6\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (262 frames)\n",
      "\n",
      "  â³ Pending: A (0/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 B:0.17 A:0.15 DOWN:0.10 LEFT:0.10 RIGHT:0.10 UP:0.09\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 1000 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 13\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 334 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/7\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (292 frames)\n",
      "\n",
      "  â³ Pending: A (1/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 A:0.15 B:0.15 UP:0.10 DOWN:0.10 LEFT:0.10 RIGHT:0.10\n",
      "\n",
      "  âš ï¸ STAGNATION WARNING: 13/20\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x3\n",
      "\n",
      "######################################################################\n",
      "# MILESTONE 1000\n",
      "# Maps explored: 1\n",
      "# Tiles visited: 1 | Obstructions: 1\n",
      "# Interactables: 0 | Transitions: 0\n",
      "# Tiles probed: 1 | Exhausted: 0\n",
      "#\n",
      "# HYBRID DECISION STATS:\n",
      "#   Markov (imitation): 0 (0.0%)\n",
      "#   Curiosity (explore): 334 (100.0%)\n",
      "#   Taught transitions: 0\n",
      "######################################################################\n",
      "# Model saved\n",
      "  ðŸ“ STAGNATION PENALTY: UP 0.100 â†’ 0.070\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 1100 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 6\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 367 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/7\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (322 frames)\n",
      "\n",
      "  â³ Pending: A (2/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 A:0.15 B:0.15 UP:0.10 DOWN:0.10 LEFT:0.10 RIGHT:0.10\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n",
      "  ðŸ“ STAGNATION PENALTY: DOWN 0.100 â†’ 0.070\n",
      "\n",
      "======================================================================\n",
      "Step 1200 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: move | Battle: 0 | Stagnation: 0\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 401 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/7\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (352 frames)\n",
      "\n",
      "  â³ Pending: A (0/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 A:0.15 B:0.15 UP:0.10 LEFT:0.10 RIGHT:0.10 DOWN:0.07\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n",
      "\n",
      "======================================================================\n",
      "Step 1300 | Map 0 | Pos (0, 0) facing DOWN\n",
      "  Mode: interact | Battle: 0 | Stagnation: 13\n",
      "\n",
      "  ðŸ§  DECISION MODE:\n",
      "     Markov: 0 (0.0%) | Curiosity: 434 (100.0%)\n",
      "     Last Markov score: 0.000 (threshold: 0.6)\n",
      "\n",
      "  ðŸ“Š EXPLORATION:\n",
      "     Visited: 1 | Obstructions: 1 | Coverage: 0%\n",
      "     Interactables found: 0\n",
      "\n",
      "  ðŸŽ¯ TILE PROBING:\n",
      "     Tiles probed: 1 | Exhausted: 0 | With success: 0\n",
      "     Current tile: READY TO PROBE (facing untried direction)\n",
      "     Direction results: DOWN:0/7\n",
      "\n",
      "  ðŸ’³ DEBT: map=10.00/10.0, temp=15.00\n",
      "\n",
      "  ðŸ”’ MENU TRAP: B boost 3.00x (383 frames)\n",
      "\n",
      "  â³ Pending: A (1/3)\n",
      "\n",
      "  âš¡ Utilities: Start:0.59 Select:0.59 A:0.15 B:0.15 UP:0.10 DOWN:0.10 LEFT:0.10 RIGHT:0.10\n",
      "\n",
      "  âš ï¸ STAGNATION WARNING: 13/20\n",
      "  ðŸ”„ PATTERN DETECTED (1): A x6\n",
      "  ðŸ“ STAGNATION PENALTY: A 0.150 â†’ 0.150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 230\u001b[39m\n\u001b[32m    227\u001b[39m     brain.save_model_checkpoint(BASE_PATH / \u001b[33m\"\u001b[39m\u001b[33mmodel_checkpoint.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    228\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m# Model saved\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[32m    233\u001b[39m next_context, next_palette, next_tiles, dead, next_raw_position = read_game_state()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Main Loop - With Markov System\n",
    "# ============================================================================\n",
    "# CHANGES FROM PREVIOUS VERSION:\n",
    "# 1. Added loading of taught_transitions.json\n",
    "# 2. Added Markov stats to logging\n",
    "# 3. Added Markov ratio display at milestones\n",
    "# ============================================================================\n",
    "\n",
    "brain = Brain()\n",
    "\n",
    "for b in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"move\"))\n",
    "for b in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"interact\"))\n",
    "\n",
    "TAUGHT_MODEL_PATH = BASE_PATH / \"model_checkpoint.json\"\n",
    "TAUGHT_EXPLORATION_PATH = BASE_PATH / \"taught_exploration_memory.json\"\n",
    "\n",
    "if TAUGHT_MODEL_PATH.exists():\n",
    "    loaded_ts = brain.load_taught_model(TAUGHT_MODEL_PATH)\n",
    "    print(f\"ðŸŽ“ TRANSFER: Loaded model from timestep {loaded_ts}\")\n",
    "    print(f\"   Utilities: {[f'{a.action}:{a.utility:.3f}' for a in brain.actions()]}\")\n",
    "    brain.merge_taught_exploration(TAUGHT_EXPLORATION_PATH)\n",
    "else:\n",
    "    print(\"No taught model found - starting fresh\")\n",
    "\n",
    "# === NEW: LOAD TAUGHT TRANSITIONS FOR MARKOV ===\n",
    "brain.load_taught_transitions(TAUGHT_TRANSITIONS_FILE)\n",
    "\n",
    "exploration_weight = 1.3\n",
    "forced_explore_prob = 0.18\n",
    "prev_context_state = None\n",
    "prev_raw_position = None\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AI CONTROL - v8.0 (Hybrid Markov + Curiosity)\")\n",
    "print(\"=\"*70)\n",
    "print(\"MARKOV SYSTEM:\")\n",
    "print(f\"  - Familiarity threshold: {MARKOV_FAMILIARITY_THRESHOLD}\")\n",
    "print(f\"  - Immediate weight: {MARKOV_IMMEDIATE_WEIGHT}\")\n",
    "print(f\"  - Sequential weight: {MARKOV_SEQUENTIAL_WEIGHT}\")\n",
    "print(f\"  - Partial weight: {MARKOV_PARTIAL_WEIGHT}\")\n",
    "print(f\"  - Taught batches: {len(brain.taught_batches)}\")\n",
    "print(f\"  - Taught frames: {len(brain.taught_transitions)}\")\n",
    "if brain.taught_metadata:\n",
    "    print(f\"  - Action changes recorded: {brain.taught_metadata.get('action_changes', 0)}\")\n",
    "    print(f\"  - Maps in teaching: {brain.taught_metadata.get('maps_visited', [])}\")\n",
    "print(\"=\"*70)\n",
    "print(\"CURIOSITY SYSTEM:\")\n",
    "print(f\"  - Forced random exploration: {forced_explore_prob:.0%}\")\n",
    "print(f\"  - 'Both' mode threshold: stagnation > {brain.BOTH_MODE_STAGNATION_THRESHOLD}\")\n",
    "print(f\"  - Unvisited tile bonus: {brain.UNVISITED_TILE_BONUS}x\")\n",
    "print(f\"  - Obstruction penalty: {brain.OBSTRUCTION_PENALTY}x\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PERSISTENT MEMORY: {brain.EXPLORATION_MEMORY_FILE}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "while True:\n",
    "    # Read state\n",
    "    context_state, palette_state, tile_state, dead, raw_position = read_game_state()\n",
    "    \n",
    "    raw_x, raw_y = raw_position\n",
    "    in_battle = context_state[3]\n",
    "    current_map = int(context_state[2])\n",
    "    current_dir = int(context_state[5])\n",
    "    \n",
    "    brain.update_position(raw_x, raw_y)\n",
    "\n",
    "    derived = compute_derived_features(context_state, prev_context_state)\n",
    "    learning_state = build_learning_state(derived, palette_state, tile_state, in_battle)\n",
    "    \n",
    "    brain.log_state(learning_state, context_state)\n",
    "    \n",
    "    # Action execution confirmation\n",
    "    brain.confirm_action_executed(context_state, prev_context_state)\n",
    "\n",
    "    if brain.should_send_new_action():\n",
    "        action = anticipatory_action(\n",
    "            brain, learning_state, context_state,\n",
    "            exploration_weight=exploration_weight,\n",
    "            raw_position=raw_position,\n",
    "            forced_explore_prob=forced_explore_prob\n",
    "        )\n",
    "\n",
    "        if action is not None:\n",
    "            write_action(action.action)\n",
    "            brain.last_action = action.action\n",
    "            brain.set_pending_action(action.action)\n",
    "            brain.update_menu_trap_tracking(context_state, action.action, raw_position=raw_position)\n",
    "        else:\n",
    "            write_action(\"NONE\")\n",
    "    else:\n",
    "        if brain.pending_action:\n",
    "            write_action(brain.pending_action)\n",
    "\n",
    "    # === LOGGING ===\n",
    "    if brain.timestep % 100 == 0:\n",
    "        memory = brain.get_current_map_memory(current_map)\n",
    "        visited_count = len(memory['visited_tiles'])\n",
    "        obs_count = len(memory['obstructions'])\n",
    "        interactables = len(memory['interactable_objects'])\n",
    "        coverage = brain.get_exploration_coverage(current_map)\n",
    "        transitions = memory.get('transitions', [])\n",
    "        tile_stats = brain.get_tile_interaction_stats(current_map)\n",
    "        \n",
    "        tile_needs_probing = brain.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "        probe_action, probe_dir = brain.get_best_probe_action(raw_x, raw_y, current_map, current_dir)\n",
    "        \n",
    "        dir_name = brain.DIRECTION_NAMES.get(current_dir, '?')\n",
    "        mode = brain.control_mode\n",
    "        \n",
    "        is_both_mode = brain.should_use_both_mode()\n",
    "        mode_display = \"BOTH âš¡\" if is_both_mode else mode\n",
    "        \n",
    "        # === NEW: MARKOV STATS ===\n",
    "        total_actions = brain.markov_action_count + brain.curiosity_action_count\n",
    "        markov_ratio = brain.markov_action_count / max(1, total_actions)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Step {brain.timestep} | Map {current_map} | Pos ({raw_x}, {raw_y}) facing {dir_name}\")\n",
    "        print(f\"  Mode: {mode_display} | Battle: {int(in_battle)} | Stagnation: {brain.state_stagnation_count}\")\n",
    "        \n",
    "        # === NEW: MARKOV STATUS ===\n",
    "        print(f\"\\n  ðŸ§  DECISION MODE:\")\n",
    "        print(f\"     Markov: {brain.markov_action_count} ({markov_ratio:.1%}) | Curiosity: {brain.curiosity_action_count} ({1-markov_ratio:.1%})\")\n",
    "        print(f\"     Last Markov score: {brain.last_markov_score:.3f} (threshold: {MARKOV_FAMILIARITY_THRESHOLD})\")\n",
    "        if brain.last_markov_action:\n",
    "            print(f\"     Last Markov suggestion: {brain.last_markov_action}\")\n",
    "        \n",
    "        # Exploration status\n",
    "        print(f\"\\n  ðŸ“Š EXPLORATION:\")\n",
    "        print(f\"     Visited: {visited_count} | Obstructions: {obs_count} | Coverage: {coverage:.0%}\")\n",
    "        print(f\"     Interactables found: {interactables}\")\n",
    "        \n",
    "        # Tile probing\n",
    "        print(f\"\\n  ðŸŽ¯ TILE PROBING:\")\n",
    "        print(f\"     Tiles probed: {tile_stats['probed']} | Exhausted: {tile_stats['exhausted']} | With success: {tile_stats['with_success']}\")\n",
    "        \n",
    "        if tile_needs_probing:\n",
    "            if probe_action == 'A':\n",
    "                print(f\"     Current tile: READY TO PROBE (facing untried direction)\")\n",
    "            elif probe_action:\n",
    "                print(f\"     Current tile: NEED TO TURN {probe_action} first\")\n",
    "            else:\n",
    "                print(f\"     Current tile: NEEDS PROBING (checking directions)\")\n",
    "        else:\n",
    "            print(f\"     Current tile: EXHAUSTED or fully probed\")\n",
    "        \n",
    "        tile_state_data = brain.get_tile_interaction_state(raw_x, raw_y, current_map)\n",
    "        success_info = []\n",
    "        for d in range(4):\n",
    "            attempts = tile_state_data['direction_attempts'].get(d, 0)\n",
    "            successes = tile_state_data['direction_successes'].get(d, 0)\n",
    "            if attempts > 0:\n",
    "                success_info.append(f\"{brain.DIRECTION_NAMES.get(d, '?')}:{successes}/{attempts}\")\n",
    "        if success_info:\n",
    "            print(f\"     Direction results: {', '.join(success_info)}\")\n",
    "        \n",
    "        # Transitions\n",
    "        if transitions:\n",
    "            print(f\"\\n  ðŸšª TRANSITIONS: {len(transitions)} known\")\n",
    "            for t in transitions[:3]:\n",
    "                pos = tuple(t['position']) if isinstance(t['position'], list) else t['position']\n",
    "                banned = \"ðŸš«\" if brain.is_transition_banned(current_map, pos, t['direction']) else \"\"\n",
    "                print(f\"     ({pos[0]},{pos[1]}) â†’ Map {t['destination_map']} (used {t['use_count']}x) {banned}\")\n",
    "        \n",
    "        # Debt info\n",
    "        map_debt = brain.map_novelty_debt.get(current_map, 0.0)\n",
    "        temp_debt = brain.get_temp_debt(current_map)\n",
    "        \n",
    "        if map_debt > 0.1 or temp_debt > 0.1:\n",
    "            print(f\"\\n  ðŸ’³ DEBT: map={map_debt:.2f}/{brain.MAX_MAP_DEBT}, temp={temp_debt:.2f}\")\n",
    "        \n",
    "        # Menu trap status\n",
    "        if brain.menu_trap_b_boost > 1.0:\n",
    "            print(f\"\\n  ðŸ”’ MENU TRAP: B boost {brain.menu_trap_b_boost:.2f}x ({brain.menu_trap_frames} frames)\")\n",
    "        \n",
    "        # \"Both\" mode status\n",
    "        if is_both_mode:\n",
    "            print(f\"\\n  âš¡ BOTH MODE ACTIVE: stagnation={brain.state_stagnation_count}, swaps={brain.unproductive_swap_count}\")\n",
    "        \n",
    "        # Pending action\n",
    "        if brain.pending_action:\n",
    "            print(f\"\\n  â³ Pending: {brain.pending_action} ({brain.pending_action_frames}/{brain.ACTION_CONFIRM_FRAMES})\")\n",
    "        \n",
    "        # Utilities\n",
    "        action_utils = sorted([(a.action, a.utility) for a in brain.actions()], key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\n  âš¡ Utilities: {' '.join([f'{k}:{v:.2f}' for k,v in action_utils])}\")\n",
    "        \n",
    "        # Warnings\n",
    "        if brain.state_stagnation_count > 10:\n",
    "            print(f\"\\n  âš ï¸ STAGNATION WARNING: {brain.state_stagnation_count}/{brain.STATE_STAGNATION_THRESHOLD}\")\n",
    "        if brain.detected_pattern:\n",
    "            pattern_str = '-'.join(str(a) for a in brain.detected_pattern)\n",
    "            print(f\"  ðŸ”„ PATTERN DETECTED ({len(brain.detected_pattern)}): {pattern_str} x{brain.pattern_repeat_count}\")\n",
    "\n",
    "    # === MILESTONES ===\n",
    "    if brain.timestep % 500 == 0 and brain.timestep > 0:\n",
    "        total_visited = sum(len(m['visited_tiles']) for m in brain.exploration_memory.values())\n",
    "        total_obs = sum(len(m['obstructions']) for m in brain.exploration_memory.values())\n",
    "        total_interactables = sum(len(m['interactable_objects']) for m in brain.exploration_memory.values())\n",
    "        total_transitions = sum(len(m.get('transitions', [])) for m in brain.exploration_memory.values())\n",
    "        total_probed = sum(len(m.get('tile_interactions', {})) for m in brain.exploration_memory.values())\n",
    "        total_exhausted = sum(\n",
    "            sum(1 for t in m.get('tile_interactions', {}).values() if t.get('exhausted', False))\n",
    "            for m in brain.exploration_memory.values()\n",
    "        )\n",
    "        \n",
    "        # Markov stats\n",
    "        total_actions = brain.markov_action_count + brain.curiosity_action_count\n",
    "        markov_ratio = brain.markov_action_count / max(1, total_actions)\n",
    "        \n",
    "        print(f\"\\n{'#'*70}\")\n",
    "        print(f\"# MILESTONE {brain.timestep}\")\n",
    "        print(f\"# Maps explored: {len(brain.exploration_memory)}\")\n",
    "        print(f\"# Tiles visited: {total_visited} | Obstructions: {total_obs}\")\n",
    "        print(f\"# Interactables: {total_interactables} | Transitions: {total_transitions}\")\n",
    "        print(f\"# Tiles probed: {total_probed} | Exhausted: {total_exhausted}\")\n",
    "        print(f\"#\")\n",
    "        print(f\"# HYBRID DECISION STATS:\")\n",
    "        print(f\"#   Markov (imitation): {brain.markov_action_count} ({markov_ratio:.1%})\")\n",
    "        print(f\"#   Curiosity (explore): {brain.curiosity_action_count} ({1-markov_ratio:.1%})\")\n",
    "        print(f\"#   Taught transitions: {len(brain.taught_transitions)}\")\n",
    "        print(f\"{'#'*70}\")\n",
    "\n",
    "        brain.save_model_checkpoint(BASE_PATH / \"model_checkpoint.json\")\n",
    "        print(f\"# Model saved\")\n",
    "\n",
    "    time.sleep(0.02)\n",
    "\n",
    "    # Learn\n",
    "    next_context, next_palette, next_tiles, dead, next_raw_position = read_game_state()\n",
    "    next_in_battle = next_context[3]\n",
    "    next_derived = compute_derived_features(next_context, context_state)\n",
    "    next_learning_state = build_learning_state(next_derived, next_palette, next_tiles, next_in_battle)\n",
    "\n",
    "    brain.learn(learning_state, next_learning_state, context_state, next_context, dead=dead, \n",
    "                raw_position=raw_position, next_raw_position=next_raw_position)\n",
    "\n",
    "    prev_context_state = context_state.copy()\n",
    "    prev_raw_position = raw_position\n",
    "    brain.timestep += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
