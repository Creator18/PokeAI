{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ae718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: State Management & Utilities\n",
    "# ============================================================================\n",
    "# CHANGES:\n",
    "# 1. read_game_state now handles BOTH key formats:\n",
    "#    - Long keys: \"state\", \"palette\", \"tiles\" (old format)\n",
    "#    - Short keys: \"s\", \"p\", \"t\" (Lua teaching/AI format)\n",
    "# 2. Added EXPLORATION_MEMORY_FILE and MODEL_CHECKPOINT_FILE paths\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "BASE_PATH = Path(\"C:/Users/HP/Documents/cogai/\")\n",
    "ACTION_FILE = BASE_PATH / \"action.json\"\n",
    "STATE_FILE = BASE_PATH / \"game_state.json\"\n",
    "TAUGHT_TRANSITIONS_FILE = BASE_PATH / \"taught_transitions.json\"\n",
    "EXPLORATION_MEMORY_FILE = BASE_PATH / \"exploration_memory.json\"\n",
    "MODEL_CHECKPOINT_FILE = BASE_PATH / \"model_checkpoint.json\"\n",
    "TAUGHT_EXPLORATION_FILE = BASE_PATH / \"taught_exploration_memory.json\"\n",
    "\n",
    "# === MARKOV SIMILARITY WEIGHTS ===\n",
    "MARKOV_IMMEDIATE_WEIGHT = 0.5\n",
    "MARKOV_SEQUENTIAL_WEIGHT = 0.3\n",
    "MARKOV_PARTIAL_WEIGHT = 0.2\n",
    "MARKOV_FAMILIARITY_THRESHOLD = 0.6\n",
    "\n",
    "MARKOV_SEQ_FULL_WEIGHT = 1.0\n",
    "MARKOV_SEQ_MEDIUM_WEIGHT = 0.6\n",
    "MARKOV_SEQ_SHORT_WEIGHT = 0.3\n",
    "\n",
    "MARKOV_POS_EXACT_BONUS = 0.35\n",
    "MARKOV_POS_NEAR_BONUS = 0.25\n",
    "MARKOV_POS_FAR_BONUS = 0.1\n",
    "MARKOV_POS_MAX_DIST = 5\n",
    "\n",
    "EXPECTED_STATE_DIM = 6\n",
    "PALETTE_DIM = 768\n",
    "TILE_DIM = 600\n",
    "\n",
    "def normalize_game_state(raw_state):\n",
    "    if len(raw_state) < 6:\n",
    "        return raw_state\n",
    "    normalized = raw_state.copy()\n",
    "    normalized[0] = raw_state[0] / 255.0\n",
    "    normalized[1] = raw_state[1] / 255.0\n",
    "    normalized[2] = np.clip(raw_state[2], 0, 255)\n",
    "    normalized[3] = 1.0 if raw_state[3] > 0 else 0.0\n",
    "    normalized[4] = 1.0 if raw_state[4] > 0 else 0.0\n",
    "    normalized[5] = int(raw_state[5]) % 4\n",
    "    return normalized\n",
    "\n",
    "def compute_derived_features(current, prev):\n",
    "    if prev is None:\n",
    "        return np.zeros(8)\n",
    "    vel_x = current[0] - prev[0]\n",
    "    vel_y = current[1] - prev[1]\n",
    "    map_changed = 1.0 if abs(current[2] - prev[2]) > 0.5 else 0.0\n",
    "    battle_started = 1.0 if current[3] > prev[3] else 0.0\n",
    "    battle_ended = 1.0 if current[3] < prev[3] else 0.0\n",
    "    menu_opened = 1.0 if current[4] > prev[4] else 0.0\n",
    "    menu_closed = 1.0 if current[4] < prev[4] else 0.0\n",
    "    direction_changed = 1.0 if current[5] != prev[5] else 0.0\n",
    "    return np.array([vel_x, vel_y, map_changed, battle_started, battle_ended,\n",
    "                     menu_opened, menu_closed, direction_changed])\n",
    "\n",
    "def build_learning_state(derived, palette, tiles, in_battle):\n",
    "    if in_battle > 0.5:\n",
    "        state = np.concatenate([derived, palette])\n",
    "    else:\n",
    "        state = np.concatenate([derived, tiles, palette])\n",
    "    noise = np.random.randn(len(state)) * 0.0001\n",
    "    return state + noise\n",
    "\n",
    "def _pad_or_trim(arr, target_dim):\n",
    "    if arr.shape[0] < target_dim:\n",
    "        return np.pad(arr, (0, target_dim - arr.shape[0]))\n",
    "    elif arr.shape[0] > target_dim:\n",
    "        return arr[:target_dim]\n",
    "    return arr\n",
    "\n",
    "def parse_game_state_data(data):\n",
    "    \"\"\"Parse game state dict handling both long and short key formats.\"\"\"\n",
    "    raw = data.get(\"state\") or data.get(\"s\") or []\n",
    "    palette_raw = data.get(\"palette\") or data.get(\"p\") or []\n",
    "    tiles_raw = data.get(\"tiles\") or data.get(\"t\") or []\n",
    "    dead = bool(data.get(\"dead\", False))\n",
    "    return raw, palette_raw, tiles_raw, dead\n",
    "\n",
    "def read_game_state(max_retries=3):\n",
    "    if not STATE_FILE.exists():\n",
    "        return np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), np.zeros(TILE_DIM), False, (0, 0)\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            with open(STATE_FILE, \"r\") as f:\n",
    "                data = json.loads(f.read())\n",
    "\n",
    "            raw, palette_raw, tiles_raw, dead = parse_game_state_data(data)\n",
    "\n",
    "            raw_x = int(raw[0]) if len(raw) > 0 else 0\n",
    "            raw_y = int(raw[1]) if len(raw) > 1 else 0\n",
    "            raw_position = (raw_x, raw_y)\n",
    "\n",
    "            context_state = normalize_game_state(np.array(raw, dtype=float))\n",
    "            palette_state = np.array(palette_raw, dtype=float) if palette_raw else np.zeros(PALETTE_DIM)\n",
    "            tile_state = np.array(tiles_raw, dtype=float) if tiles_raw else np.zeros(TILE_DIM)\n",
    "\n",
    "            context_state = _pad_or_trim(context_state, EXPECTED_STATE_DIM)\n",
    "            palette_state = _pad_or_trim(palette_state, PALETTE_DIM)\n",
    "            tile_state = _pad_or_trim(tile_state, TILE_DIM)\n",
    "\n",
    "            return context_state, palette_state, tile_state, dead, raw_position\n",
    "\n",
    "        except (json.JSONDecodeError, ValueError):\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(0.001)\n",
    "                continue\n",
    "            return np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), np.zeros(TILE_DIM), False, (0, 0)\n",
    "        except Exception:\n",
    "            return np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), np.zeros(TILE_DIM), False, (0, 0)\n",
    "\n",
    "def write_action(action_name):\n",
    "    if action_name:\n",
    "        action_name = action_name.upper()\n",
    "    try:\n",
    "        with open(ACTION_FILE, \"w\") as f:\n",
    "            json.dump({\"action\": action_name}, f)\n",
    "            f.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write action: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Perceptron Classes\n",
    "# ============================================================================\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, kind, action=None, group=None, entity_type=None):\n",
    "        self.kind = kind\n",
    "        self.action = action\n",
    "        self.group = group\n",
    "        self.entity_type = entity_type\n",
    "        \n",
    "        self.utility = 1.0\n",
    "        self.weights = None\n",
    "        \n",
    "        self.eligibility_fast = 0.0\n",
    "        self.eligibility_slow = 0.0\n",
    "        \n",
    "        self.familiarity = 0.0\n",
    "        self.activation_history = deque(maxlen=10)\n",
    "        \n",
    "        self.learning_rate = 0.01\n",
    "        self.prediction_errors = deque(maxlen=50)\n",
    "\n",
    "    def ensure_weights(self, dim):\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(dim) * 0.001\n",
    "\n",
    "    def predict(self, state):\n",
    "        self.ensure_weights(len(state))\n",
    "        \n",
    "        # Handle dimension mismatch\n",
    "        if len(self.weights) != len(state):\n",
    "            min_dim = min(len(self.weights), len(state))\n",
    "            raw_activation = np.dot(self.weights[:min_dim], state[:min_dim])\n",
    "        else:\n",
    "            raw_activation = np.dot(self.weights, state)\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            novelty_factor = 1.0 / (1.0 + np.sqrt(self.familiarity * 0.5))\n",
    "            decayed_activation = raw_activation * novelty_factor\n",
    "            self.activation_history.append(abs(raw_activation))\n",
    "            return decayed_activation\n",
    "        else:\n",
    "            return raw_activation\n",
    "\n",
    "    def adapt_learning_rate(self):\n",
    "        if len(self.prediction_errors) >= 50:\n",
    "            avg_error = np.mean(self.prediction_errors)\n",
    "            \n",
    "            if avg_error < 0.1:\n",
    "                self.learning_rate = max(0.001, self.learning_rate * 0.99)\n",
    "            elif avg_error > 0.5:\n",
    "                self.learning_rate = min(0.05, self.learning_rate * 1.01)\n",
    "\n",
    "    def update(self, state, error, gamma_fast=0.5, gamma_slow=0.95, stagnation=0.0):\n",
    "        self.ensure_weights(len(state))\n",
    "        \n",
    "        # Handle dimension mismatch\n",
    "        if len(self.weights) != len(state):\n",
    "            min_dim = min(len(self.weights), len(state))\n",
    "            state = state[:min_dim]\n",
    "            self.weights = self.weights[:min_dim]\n",
    "        \n",
    "        self.eligibility_fast = gamma_fast * self.eligibility_fast + 1.0\n",
    "        self.eligibility_slow = gamma_slow * self.eligibility_slow + 1.0\n",
    "        \n",
    "        self.adapt_learning_rate()\n",
    "        \n",
    "        fast_update = 0.7 * self.learning_rate * error * state * self.eligibility_fast\n",
    "        slow_update = 0.3 * self.learning_rate * error * state * self.eligibility_slow\n",
    "        self.weights += fast_update + slow_update\n",
    "\n",
    "        if self.kind == \"action\":\n",
    "            if error > 0.01:\n",
    "                if stagnation > 0.5:\n",
    "                    self.utility *= 0.97\n",
    "                elif error > 0.2:\n",
    "                    self.utility = min(self.utility * 1.02, 2.0)\n",
    "                else:\n",
    "                    self.utility *= 0.995\n",
    "            \n",
    "            if self.group == \"move\":\n",
    "                self.utility = np.clip(self.utility, 0.1, 2.0)\n",
    "            else:\n",
    "                self.utility = np.clip(self.utility, 0.01, 2.0)\n",
    "        \n",
    "        if self.kind == \"entity\" and len(self.activation_history) > 0:\n",
    "            recent_avg = np.mean(self.activation_history)\n",
    "            if recent_avg > 0.1:\n",
    "                self.familiarity += 0.03\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            prediction = self.predict(state)\n",
    "            self.prediction_errors.append(abs(prediction - error))\n",
    "\n",
    "\n",
    "class ControlSwapPerceptron(Perceptron):\n",
    "    def __init__(self):\n",
    "        super().__init__(kind=\"control_swap\")\n",
    "        self.swap_history = deque(maxlen=100)\n",
    "        self.confidence = 0.0\n",
    "        \n",
    "    def should_swap(self, state, movement_stagnation):\n",
    "        if self.weights is None:\n",
    "            return False, 0.0\n",
    "        \n",
    "        self.ensure_weights(len(state))\n",
    "        swap_score = np.dot(self.weights, state)\n",
    "        stagnation_factor = np.tanh(movement_stagnation / 5.0)\n",
    "        combined_score = swap_score * 0.7 + stagnation_factor * 0.3\n",
    "        \n",
    "        return combined_score > 0.5, abs(combined_score)\n",
    "    \n",
    "    def record_swap_outcome(self, state, swapped, novelty_gained):\n",
    "        self.swap_history.append((swapped, novelty_gained))\n",
    "        \n",
    "        if len(self.swap_history) >= 20:\n",
    "            recent = list(self.swap_history)[-20:]\n",
    "            successful = sum(1 for swap, nov in recent if swap and nov > 0.2)\n",
    "            self.confidence = successful / 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980759f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3 PART 1: Brain Class - Initialization & Markov System\n",
    "# ============================================================================\n",
    "# CHANGES:\n",
    "# 1. Added taught_reference dict and blend tracking variables\n",
    "# 2. Added load_taught_reference() - loads taught model as read-only\n",
    "# 3. Added blend_from_taught() - blends utilities/weights at given ratio\n",
    "# 4. compute_markov_similarity() accepts optional taught_frames\n",
    "# 5. get_markov_action() passes taught_frames through\n",
    "# ============================================================================\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.perceptrons = []\n",
    "        \n",
    "        self.prev_learning_states = deque(maxlen=50)\n",
    "        self.prev_context_states = deque(maxlen=10)\n",
    "        self.last_positions = deque(maxlen=30)\n",
    "        self.action_history = deque(maxlen=100)\n",
    "        \n",
    "        self.control_mode = \"move\"\n",
    "        self.timestep = 0\n",
    "        self.last_action = None\n",
    "        self.last_direction = 0\n",
    "        \n",
    "        self.MOVE_UTILITY_FLOOR = 0.05\n",
    "        self.INTERACT_UTILITY_FLOOR = 0.15\n",
    "        \n",
    "        # === PERSISTENT EXPLORATION MEMORY ===\n",
    "        self.EXPLORATION_MEMORY_FILE = BASE_PATH / \"exploration_memory.json\"\n",
    "        self.exploration_memory = {}\n",
    "        self.current_map_id = None\n",
    "        self.SAVE_INTERVAL = 100\n",
    "        \n",
    "        self.DIRECTION_NAMES = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        self.DIRECTION_TO_INT = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "        self.INT_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        \n",
    "        self.DIRECTION_DELTAS_INT = {0: (0, 1), 1: (0, -1), 2: (-1, 0), 3: (1, 0)}\n",
    "        self.ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "        self.DELTA_TO_DIRECTION = {(0, 1): 0, (0, -1): 1, (-1, 0): 2, (1, 0): 3}\n",
    "        \n",
    "        self.load_exploration_memory()\n",
    "        \n",
    "        # === MARKOV TRANSITION SYSTEM ===\n",
    "        self.taught_transitions = []\n",
    "        self.taught_batches = []\n",
    "        self.taught_metadata = {}\n",
    "        self.markov_enabled = True\n",
    "        self.markov_action_count = 0\n",
    "        self.curiosity_action_count = 0\n",
    "        self.last_markov_score = 0.0\n",
    "        self.last_markov_action = None\n",
    "        \n",
    "        # === TAUGHT MODEL REFERENCE (read-only, for stagnation blending) ===\n",
    "        self.taught_reference = {\n",
    "            'utilities': {},     # action_name -> float\n",
    "            'weights': {},       # action_name -> np.array\n",
    "            'loaded': False\n",
    "        }\n",
    "        \n",
    "        # === BLEND SYSTEM ===\n",
    "        self.blend_tier = 0                 # 0=none, 1=light, 2=medium, 3=hard\n",
    "        self.last_blend_timestep = 0\n",
    "        self.BLEND_COOLDOWN = 50            # Min steps between blends\n",
    "        self.blend_count = 0                # Total blends performed\n",
    "        \n",
    "        # Blend ratios: (ai_weight, taught_weight)\n",
    "        self.BLEND_RATIOS = {\n",
    "            1: (0.80, 0.20),   # Light: 80% AI, 20% taught\n",
    "            2: (0.60, 0.40),   # Medium: 60% AI, 40% taught\n",
    "            3: (0.40, 0.60)    # Hard: 40% AI, 60% taught\n",
    "        }\n",
    "        \n",
    "        # Tier trigger thresholds\n",
    "        self.BLEND_TIER_TRIGGERS = {\n",
    "            1: {'pattern_repeats': 3, 'pos_stagnation': 8, 'consecutive': 12},\n",
    "            2: {'pattern_repeats': 6, 'pos_stagnation': 15, 'consecutive': 15},\n",
    "            3: {'pattern_repeats': 10, 'state_stagnation_mult': 2.0}\n",
    "        }\n",
    "        \n",
    "        # === ACTION EXECUTION CONFIRMATION ===\n",
    "        self.pending_action = None\n",
    "        self.pending_action_frames = 0\n",
    "        self.ACTION_CONFIRM_FRAMES = 3\n",
    "        self.last_confirmed_action = None\n",
    "        \n",
    "        # === TILE INTERACTION PROBING ===\n",
    "        self.INTERACTION_VERIFY_FRAMES = 8\n",
    "        self.MIN_SUCCESS_RATE_THRESHOLD = 0.1\n",
    "        self.pending_interaction_verify = None\n",
    "        self.interaction_verify_countdown = 0\n",
    "        \n",
    "        # === MENU ESCAPE B-BOOST ===\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.B_BOOST_INCREMENT = 0.15\n",
    "        self.B_BOOST_MAX = 3.0\n",
    "        self.MENU_TRAP_THRESHOLD = 5\n",
    "        self.original_b_utility = None\n",
    "        \n",
    "        # === ADAPTIVE MODE SWAPPING ===\n",
    "        self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD = 15\n",
    "        self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD = 25\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.THRESHOLD_INCREMENT = 15\n",
    "        self.MAX_THRESHOLD = 150\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.swap_chain_count = 0\n",
    "        self.position_at_mode_swap = None\n",
    "        self.last_map_id = None\n",
    "        self.last_battle_state = None\n",
    "        \n",
    "        # === UNPRODUCTIVE MODE SWAP TRACKING ===\n",
    "        self.UNPRODUCTIVE_SWAP_THRESHOLD = 3\n",
    "        self.unproductive_swap_count = 0\n",
    "        self.utilities_before_swapping = {}\n",
    "        self.swap_chain_active = False\n",
    "        \n",
    "        # === STATE STAGNATION DETECTION ===\n",
    "        self.STATE_STAGNATION_THRESHOLD = 20\n",
    "        self.state_stagnation_count = 0\n",
    "        self.last_context_state_hash = None\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.STAGNATION_INITIATOR_PENALTY = 0.7\n",
    "        \n",
    "        # === \"BOTH\" MODE THRESHOLDS ===\n",
    "        self.BOTH_MODE_STAGNATION_THRESHOLD = 35\n",
    "        self.BOTH_MODE_SWAP_THRESHOLD = 5\n",
    "        \n",
    "        # === TURN AS PROGRESS TRACKING ===\n",
    "        self.last_direction_for_progress = None\n",
    "        self.direction_change_counts_as_progress = True\n",
    "        \n",
    "        # === NOVELTY WEIGHTS ===\n",
    "        self.UNVISITED_TILE_BONUS = 1.5\n",
    "        self.OBSTRUCTION_PENALTY = 0.25\n",
    "        \n",
    "        # === TRANSITION SYSTEM ===\n",
    "        self.TRANSITION_ATTRACTION_WEIGHT = 0.6\n",
    "        self.TEMP_DEBT_ACCUMULATION = 0.5\n",
    "        self.TEMP_DEBT_DECAY = 0.02\n",
    "        self.TEMP_DEBT_MAX = 15.0\n",
    "        \n",
    "        # === DEBT CAPS AND DECAY ===\n",
    "        self.MAX_MAP_DEBT = 10.0\n",
    "        self.MAX_LOCATION_DEBT = 5.0\n",
    "        self.DEBT_DECAY_RATE = 0.005\n",
    "        \n",
    "        # === TRANSITION BAN SYSTEM ===\n",
    "        self.transition_bans = {}\n",
    "        self.BAN_VICINITY_RADIUS = 3\n",
    "        self.BAN_COVERAGE_LIFT_THRESHOLD = 0.6\n",
    "        self.BAN_TIMEOUT_STEPS = 300\n",
    "        \n",
    "        # Multi-scale memory\n",
    "        self.visited_maps = {}\n",
    "        self.map_novelty_debt = {}\n",
    "        self.location_memory = {}\n",
    "        self.location_novelty = {}\n",
    "        self.action_execution_count = {}\n",
    "        \n",
    "        self.swap_perceptron = ControlSwapPerceptron()\n",
    "        self.error_history = deque(maxlen=1000)\n",
    "        self.numeric_error_history = deque(maxlen=1000)\n",
    "        self.visual_error_history = deque(maxlen=1000)\n",
    "        self._entity_norms_cache = {}\n",
    "        self._cache_valid = False\n",
    "        self.innate_entities_spawned = False\n",
    "        \n",
    "        # === REPETITION CORRECTION ===\n",
    "        self.consecutive_action_count = 0\n",
    "        self.current_repeated_action = None\n",
    "        self.LEARNING_SLOWDOWN_START = 3\n",
    "        self.LEARNING_SLOWDOWN_MAX = 10\n",
    "        self.PENALTY_THRESHOLD = 12\n",
    "        self.HARD_RESET_THRESHOLD = 18\n",
    "        \n",
    "        # === PATTERN DETECTION ===\n",
    "        self.PATTERN_CHECK_WINDOW = 50\n",
    "        self.PATTERN_MIN_REPEATS = 3\n",
    "        self.PATTERN_MAX_LENGTH = 10\n",
    "        self.detected_pattern = None\n",
    "        self.pattern_repeat_count = 0\n",
    "\n",
    "        # === PROBE ACTION CACHE ===\n",
    "        self._cached_probe_action = None\n",
    "        self._cached_probe_dir = None\n",
    "        self._probe_cache_position = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # TAUGHT MODEL REFERENCE\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_taught_reference(self, filepath):\n",
    "        \"\"\"\n",
    "        Load taught model as a READ-ONLY reference for stagnation blending.\n",
    "        Does NOT overwrite AI's own utilities or weights.\n",
    "        Stores taught values separately in self.taught_reference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not Path(filepath).exists():\n",
    "                print(f\"  No taught reference model found at {filepath}\")\n",
    "                return\n",
    "            \n",
    "            with open(filepath, 'r') as f:\n",
    "                model = json.load(f)\n",
    "            \n",
    "            if \"perceptrons\" not in model:\n",
    "                print(f\"  ‚ö†Ô∏è Taught reference model empty or invalid\")\n",
    "                return\n",
    "            \n",
    "            # Store taught utilities\n",
    "            for saved_action in model[\"perceptrons\"].get(\"actions\", []):\n",
    "                action_name = saved_action.get(\"action\")\n",
    "                if action_name:\n",
    "                    self.taught_reference['utilities'][action_name] = saved_action.get(\"utility\", 1.0)\n",
    "                    \n",
    "                    # Store taught weights if available\n",
    "                    if saved_action.get(\"weights_nonzero\"):\n",
    "                        dim = saved_action.get(\"weights_shape\", 1376)\n",
    "                        w = np.zeros(dim)\n",
    "                        for idx, val in saved_action[\"weights_nonzero\"]:\n",
    "                            if idx < dim:\n",
    "                                w[idx] = val\n",
    "                        self.taught_reference['weights'][action_name] = w\n",
    "            \n",
    "            self.taught_reference['loaded'] = True\n",
    "            \n",
    "            print(f\"  üìñ Taught reference loaded:\")\n",
    "            print(f\"     Actions: {list(self.taught_reference['utilities'].keys())}\")\n",
    "            print(f\"     Utilities: {', '.join(f'{k}:{v:.3f}' for k, v in self.taught_reference['utilities'].items())}\")\n",
    "            print(f\"     Weights available: {list(self.taught_reference['weights'].keys())}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error loading taught reference: {e}\")\n",
    "    \n",
    "    def blend_from_taught(self, tier):\n",
    "        \"\"\"\n",
    "        Blend AI's current utilities (and optionally weights) toward taught values.\n",
    "        \n",
    "        tier 1 (light):  80% AI / 20% taught ‚Äî utilities only\n",
    "        tier 2 (medium): 60% AI / 40% taught ‚Äî utilities only\n",
    "        tier 3 (hard):   40% AI / 60% taught ‚Äî utilities + weights\n",
    "        \n",
    "        Protections:\n",
    "        - If human valued an action > 1.0, result can't go below human_value * 0.5\n",
    "        - Only blends actions that exist in both models\n",
    "        - Entities, debt, exploration memory untouched\n",
    "        \"\"\"\n",
    "        if not self.taught_reference['loaded']:\n",
    "            return\n",
    "        \n",
    "        if tier not in self.BLEND_RATIOS:\n",
    "            return\n",
    "        \n",
    "        # Cooldown check\n",
    "        if self.timestep - self.last_blend_timestep < self.BLEND_COOLDOWN:\n",
    "            return\n",
    "        \n",
    "        ai_weight, taught_weight = self.BLEND_RATIOS[tier]\n",
    "        blend_weights = (tier == 3)  # Only blend weights at tier 3\n",
    "        \n",
    "        blended_actions = []\n",
    "        \n",
    "        for a in self.actions():\n",
    "            if a.action not in self.taught_reference['utilities']:\n",
    "                continue\n",
    "            \n",
    "            taught_util = self.taught_reference['utilities'][a.action]\n",
    "            old_util = a.utility\n",
    "            \n",
    "            # Blend utility\n",
    "            a.utility = ai_weight * a.utility + taught_weight * taught_util\n",
    "            \n",
    "            # Protection: if human valued it highly, enforce floor\n",
    "            if taught_util > 1.0:\n",
    "                a.utility = max(a.utility, taught_util * 0.5)\n",
    "            \n",
    "            # Standard floors still apply\n",
    "            floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "            a.utility = max(a.utility, floor)\n",
    "            \n",
    "            # Cap at 2.0\n",
    "            a.utility = min(a.utility, 2.0)\n",
    "            \n",
    "            blended_actions.append(f\"{a.action}:{old_util:.3f}‚Üí{a.utility:.3f}\")\n",
    "            \n",
    "            # Blend weights at tier 3\n",
    "            if blend_weights and a.action in self.taught_reference['weights']:\n",
    "                taught_w = self.taught_reference['weights'][a.action]\n",
    "                if a.weights is not None:\n",
    "                    min_dim = min(len(a.weights), len(taught_w))\n",
    "                    a.weights[:min_dim] = (\n",
    "                        ai_weight * a.weights[:min_dim] + \n",
    "                        taught_weight * taught_w[:min_dim]\n",
    "                    )\n",
    "        \n",
    "        self.last_blend_timestep = self.timestep\n",
    "        self.blend_tier = tier\n",
    "        self.blend_count += 1\n",
    "        \n",
    "        tier_names = {1: \"LIGHT\", 2: \"MEDIUM\", 3: \"HARD\"}\n",
    "        print(f\"  üîÄ BLEND [{tier_names.get(tier, '?')}] ({ai_weight:.0%} AI / {taught_weight:.0%} taught)\"\n",
    "              f\" | Blend #{self.blend_count}\")\n",
    "        for ba in blended_actions:\n",
    "            print(f\"     {ba}\")\n",
    "        if blend_weights:\n",
    "            print(f\"     + Weights blended for: {list(self.taught_reference['weights'].keys())}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # MARKOV TRANSITION SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_taught_transitions(self, filepath=None):\n",
    "        filepath = filepath or TAUGHT_TRANSITIONS_FILE\n",
    "        try:\n",
    "            if Path(filepath).exists():\n",
    "                with open(filepath, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                self.taught_transitions = []\n",
    "                self.taught_batches = data.get('batches', [])\n",
    "                \n",
    "                for batch in self.taught_batches:\n",
    "                    batch_type = batch.get('batch_type', 'steady')\n",
    "                    trigger_action = batch.get('trigger_action')\n",
    "                    \n",
    "                    for frame in batch.get('frames', []):\n",
    "                        transition = {\n",
    "                            'state': frame.get('state', {}),\n",
    "                            'action': frame.get('action'),\n",
    "                            'recent_actions': frame.get('recent_actions', []),\n",
    "                            'frame_offset': frame.get('frame_offset', 0),\n",
    "                            'batch_type': batch_type,\n",
    "                            'trigger_action': trigger_action\n",
    "                        }\n",
    "                        self.taught_transitions.append(transition)\n",
    "                \n",
    "                self.taught_metadata = data.get('metadata', {})\n",
    "                \n",
    "                print(f\"  üìö Loaded taught transitions:\")\n",
    "                print(f\"     Batches: {len(self.taught_batches)}\")\n",
    "                print(f\"     Frames: {len(self.taught_transitions)}\")\n",
    "                print(f\"     Action changes: {self.taught_metadata.get('action_changes', 0)}\")\n",
    "                print(f\"     Maps visited: {self.taught_metadata.get('maps_visited', [])}\")\n",
    "            else:\n",
    "                self.taught_transitions = []\n",
    "                self.taught_batches = []\n",
    "                self.taught_metadata = {}\n",
    "                print(f\"  No taught transitions file found at {filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading taught transitions: {e}\")\n",
    "            self.taught_transitions = []\n",
    "            self.taught_batches = []\n",
    "            self.taught_metadata = {}\n",
    "    \n",
    "    def extract_partial_context(self, context_state, raw_position=None):\n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        \n",
    "        movement_blocked = self.get_position_stagnation() > 3\n",
    "        \n",
    "        near_transition = False\n",
    "        memory = self.get_current_map_memory(current_map)\n",
    "        for t in memory.get('transitions', []):\n",
    "            t_pos = tuple(t['position']) if isinstance(t['position'], list) else t['position']\n",
    "            if abs(raw_x - t_pos[0]) + abs(raw_y - t_pos[1]) <= 2:\n",
    "                near_transition = True\n",
    "                break\n",
    "        \n",
    "        tile_probed = not self.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "        \n",
    "        return {\n",
    "            'in_battle': context_state[3] > 0.5,\n",
    "            'in_menu': context_state[4] > 0.5,\n",
    "            'movement_blocked': movement_blocked,\n",
    "            'near_transition': near_transition,\n",
    "            'tile_probed': tile_probed\n",
    "        }\n",
    "    \n",
    "    def compute_markov_similarity(self, context_state, raw_position=None, taught_frames=None):\n",
    "        frames = taught_frames if taught_frames is not None else self.taught_transitions\n",
    "        skip_map_check = taught_frames is not None\n",
    "        \n",
    "        if not frames:\n",
    "            return 0.0, None, -1\n",
    "        \n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        current_dir = int(context_state[5])\n",
    "        in_battle = context_state[3] > 0.5\n",
    "        in_menu = context_state[4] > 0.5\n",
    "        \n",
    "        current_actions = list(self.action_history)\n",
    "        current_partial = self.extract_partial_context(context_state, raw_position)\n",
    "        \n",
    "        best_score = 0.0\n",
    "        best_action = None\n",
    "        best_idx = -1\n",
    "        \n",
    "        for idx, transition in enumerate(frames):\n",
    "            t_state = transition.get('state', {})\n",
    "            t_action = transition.get('action')\n",
    "            t_recent = transition.get('recent_actions', [])\n",
    "            batch_type = transition.get('batch_type', 'steady')\n",
    "            \n",
    "            if not t_action or t_action == \"NONE\":\n",
    "                continue\n",
    "            \n",
    "            immediate_score = 0.0\n",
    "            \n",
    "            if not skip_map_check:\n",
    "                if t_state.get('map_id') != current_map:\n",
    "                    continue\n",
    "            immediate_score += 0.25\n",
    "            \n",
    "            t_x = t_state.get('x', 0)\n",
    "            t_y = t_state.get('y', 0)\n",
    "            pos_dist = abs(raw_x - t_x) + abs(raw_y - t_y)\n",
    "            \n",
    "            if pos_dist == 0:\n",
    "                immediate_score += MARKOV_POS_EXACT_BONUS\n",
    "            elif pos_dist <= 2:\n",
    "                immediate_score += MARKOV_POS_NEAR_BONUS\n",
    "            elif pos_dist <= MARKOV_POS_MAX_DIST:\n",
    "                immediate_score += MARKOV_POS_FAR_BONUS\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if t_state.get('direction') == current_dir:\n",
    "                immediate_score += 0.2\n",
    "            \n",
    "            t_in_battle = t_state.get('in_battle', 0) == 1\n",
    "            t_in_menu = t_state.get('in_menu', 0) == 1\n",
    "            \n",
    "            if t_in_battle == in_battle:\n",
    "                immediate_score += 0.1\n",
    "            if t_in_menu == in_menu:\n",
    "                immediate_score += 0.1\n",
    "            \n",
    "            sequential_score = 0.0\n",
    "            \n",
    "            if t_recent and current_actions:\n",
    "                if len(current_actions) >= 8 and len(t_recent) >= 8:\n",
    "                    if list(current_actions)[-8:] == t_recent[-8:]:\n",
    "                        sequential_score = MARKOV_SEQ_FULL_WEIGHT\n",
    "                \n",
    "                if sequential_score < MARKOV_SEQ_MEDIUM_WEIGHT:\n",
    "                    if len(current_actions) >= 5 and len(t_recent) >= 5:\n",
    "                        if list(current_actions)[-5:] == t_recent[-5:]:\n",
    "                            sequential_score = MARKOV_SEQ_MEDIUM_WEIGHT\n",
    "                \n",
    "                if sequential_score < MARKOV_SEQ_SHORT_WEIGHT:\n",
    "                    if len(current_actions) >= 3 and len(t_recent) >= 3:\n",
    "                        if list(current_actions)[-3:] == t_recent[-3:]:\n",
    "                            sequential_score = MARKOV_SEQ_SHORT_WEIGHT\n",
    "            \n",
    "            partial_score = 0.0\n",
    "            partial_matches = 0\n",
    "            partial_total = 2\n",
    "            \n",
    "            if t_in_battle == current_partial['in_battle']:\n",
    "                partial_matches += 1\n",
    "            if t_in_menu == current_partial['in_menu']:\n",
    "                partial_matches += 1\n",
    "            \n",
    "            partial_score = partial_matches / partial_total\n",
    "            \n",
    "            total_score = (\n",
    "                MARKOV_IMMEDIATE_WEIGHT * immediate_score +\n",
    "                MARKOV_SEQUENTIAL_WEIGHT * sequential_score +\n",
    "                MARKOV_PARTIAL_WEIGHT * partial_score\n",
    "            )\n",
    "            \n",
    "            if batch_type == \"action_change\":\n",
    "                total_score *= 1.2\n",
    "            \n",
    "            if transition.get('frame_offset', 0) == 0:\n",
    "                total_score *= 1.1\n",
    "            \n",
    "            if total_score > best_score:\n",
    "                best_score = total_score\n",
    "                best_action = t_action\n",
    "                best_idx = idx\n",
    "        \n",
    "        return best_score, best_action, best_idx\n",
    "    \n",
    "    def get_markov_action(self, context_state, raw_position=None, taught_frames=None):\n",
    "        if not self.markov_enabled:\n",
    "            return False, None, 0.0\n",
    "        \n",
    "        frames = taught_frames if taught_frames is not None else self.taught_transitions\n",
    "        if not frames:\n",
    "            return False, None, 0.0\n",
    "        \n",
    "        score, action, idx = self.compute_markov_similarity(\n",
    "            context_state, raw_position, taught_frames=frames\n",
    "        )\n",
    "        \n",
    "        self.last_markov_score = score\n",
    "        \n",
    "        if score >= MARKOV_FAMILIARITY_THRESHOLD:\n",
    "            self.last_markov_action = action\n",
    "            return True, action, score\n",
    "        \n",
    "        return False, None, score\n",
    "\n",
    "    # =========================================================================\n",
    "    # ACTION EXECUTION CONFIRMATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def set_pending_action(self, action_name):\n",
    "        self.pending_action = action_name\n",
    "        self.pending_action_frames = 0\n",
    "    \n",
    "    def confirm_action_executed(self, context_state, prev_context_state):\n",
    "        if self.pending_action is None:\n",
    "            return True\n",
    "        self.pending_action_frames += 1\n",
    "        action_executed = False\n",
    "        if prev_context_state is not None:\n",
    "            if self.pending_action in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "                pos_changed = (context_state[0] != prev_context_state[0] or \n",
    "                              context_state[1] != prev_context_state[1])\n",
    "                dir_changed = context_state[5] != prev_context_state[5]\n",
    "                action_executed = pos_changed or dir_changed\n",
    "            elif self.pending_action in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "                menu_changed = abs(context_state[4] - prev_context_state[4]) > 0.1\n",
    "                battle_changed = context_state[3] != prev_context_state[3]\n",
    "                map_changed = context_state[2] != prev_context_state[2]\n",
    "                action_executed = menu_changed or battle_changed or map_changed\n",
    "        if action_executed or self.pending_action_frames >= self.ACTION_CONFIRM_FRAMES:\n",
    "            self.last_confirmed_action = self.pending_action\n",
    "            self.pending_action = None\n",
    "            self.pending_action_frames = 0\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def should_send_new_action(self):\n",
    "        return self.pending_action is None or self.pending_action_frames >= self.ACTION_CONFIRM_FRAMES\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION MEMORY PERSISTENCE\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_exploration_memory(self):\n",
    "        try:\n",
    "            if self.EXPLORATION_MEMORY_FILE.exists():\n",
    "                with open(self.EXPLORATION_MEMORY_FILE, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.exploration_memory = {}\n",
    "                    for map_key, map_data in data.items():\n",
    "                        map_id = int(map_key.replace('map_', ''))\n",
    "                        self.exploration_memory[map_id] = self._deserialize_map_memory(map_data)\n",
    "                print(f\"  Loaded exploration memory: {len(self.exploration_memory)} maps\")\n",
    "            else:\n",
    "                self.exploration_memory = {}\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading exploration memory: {e}\")\n",
    "            self.exploration_memory = {}\n",
    "\n",
    "    def _deserialize_map_memory(self, map_data):\n",
    "        memory = {\n",
    "            'visited_tiles': set(tuple(t) for t in map_data.get('visited_tiles', [])),\n",
    "            'obstructions': set(tuple(t) for t in map_data.get('obstructions', [])),\n",
    "            'interactable_objects': map_data.get('interactable_objects', []),\n",
    "            'last_visited_timestep': map_data.get('last_visited_timestep', 0),\n",
    "            'transitions': map_data.get('transitions', []),\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': {}\n",
    "        }\n",
    "        for tile_key, tile_data in map_data.get('tile_interactions', {}).items():\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(tile_data.get('directions_tried', [])),\n",
    "                'direction_attempts': {int(k): v for k, v in tile_data.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {int(k): v for k, v in tile_data.get('direction_successes', {}).items()},\n",
    "                'exhausted': tile_data.get('exhausted', False)\n",
    "            }\n",
    "        return memory\n",
    "\n",
    "    def save_exploration_memory(self):\n",
    "        try:\n",
    "            data = {f'map_{mid}': self._serialize_map_memory(md) for mid, md in self.exploration_memory.items()}\n",
    "            with open(self.EXPLORATION_MEMORY_FILE, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving exploration memory: {e}\")\n",
    "\n",
    "    def _serialize_map_memory(self, map_data):\n",
    "        serialized_ti = {}\n",
    "        for tile_key, td in map_data.get('tile_interactions', {}).items():\n",
    "            serialized_ti[tile_key] = {\n",
    "                'directions_tried': list(td.get('directions_tried', set())),\n",
    "                'direction_attempts': {str(k): v for k, v in td.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {str(k): v for k, v in td.get('direction_successes', {}).items()},\n",
    "                'exhausted': td.get('exhausted', False)\n",
    "            }\n",
    "        return {\n",
    "            'visited_tiles': list(map_data['visited_tiles']),\n",
    "            'obstructions': list(map_data['obstructions']),\n",
    "            'interactable_objects': map_data['interactable_objects'],\n",
    "            'last_visited_timestep': map_data['last_visited_timestep'],\n",
    "            'transitions': map_data.get('transitions', []),\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': serialized_ti\n",
    "        }\n",
    "\n",
    "    def get_current_map_memory(self, map_id):\n",
    "        if map_id not in self.exploration_memory:\n",
    "            self.exploration_memory[map_id] = {\n",
    "                'visited_tiles': set(), 'obstructions': set(), 'interactable_objects': [],\n",
    "                'last_visited_timestep': self.timestep, 'transitions': [], 'temp_debt': 0.0,\n",
    "                'tile_interactions': {}\n",
    "            }\n",
    "        return self.exploration_memory[map_id]\n",
    "\n",
    "    def record_visited_tile(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['visited_tiles'].add((int(x), int(y)))\n",
    "        memory['last_visited_timestep'] = self.timestep\n",
    "\n",
    "    def record_obstruction(self, x, y, map_id, direction):\n",
    "        dx, dy = self.DIRECTION_DELTAS_INT.get(direction, (0, 0))\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['obstructions'].add((int(x + dx), int(y + dy)))\n",
    "\n",
    "    # =========================================================================\n",
    "    # TILE-BASED INTERACTION PROBING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_tile_interaction_key(self, x, y):\n",
    "        return f\"{int(x)}_{int(y)}\"\n",
    "    \n",
    "    def get_tile_interaction_state(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_key = self.get_tile_interaction_key(x, y)\n",
    "        if tile_key not in memory['tile_interactions']:\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(),\n",
    "                'direction_attempts': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'direction_successes': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'exhausted': False\n",
    "            }\n",
    "        return memory['tile_interactions'][tile_key]\n",
    "    \n",
    "    def should_interact_at_tile(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if tile_state['exhausted']:\n",
    "            return False\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return True\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            successes = tile_state['direction_successes'].get(d, 0)\n",
    "            if attempts > 0 and successes / attempts >= self.MIN_SUCCESS_RATE_THRESHOLD:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_untried_directions(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        return [d for d in range(4) if d not in tile_state['directions_tried']]\n",
    "    \n",
    "    def get_best_interaction_direction(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        untried = self.get_untried_directions(x, y, map_id)\n",
    "        if untried:\n",
    "            return untried[0]\n",
    "        best_dir, best_rate = None, 0.0\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            if attempts > 0:\n",
    "                rate = tile_state['direction_successes'].get(d, 0) / attempts\n",
    "                if rate > best_rate:\n",
    "                    best_rate, best_dir = rate, d\n",
    "        return best_dir\n",
    "    \n",
    "    def get_best_probe_action(self, raw_x, raw_y, current_map, current_dir):\n",
    "        \"\"\"Cached version - returns (action, target_direction) for tile probing.\"\"\"\n",
    "        cache_key = (raw_x, raw_y, current_map, current_dir)\n",
    "        \n",
    "        if self._probe_cache_position == cache_key:\n",
    "            return self._cached_probe_action, self._cached_probe_dir\n",
    "        \n",
    "        if not self.should_interact_at_tile(raw_x, raw_y, current_map):\n",
    "            result = (None, None)\n",
    "        else:\n",
    "            untried = self.get_untried_directions(raw_x, raw_y, current_map)\n",
    "            if not untried:\n",
    "                best_dir = self.get_best_interaction_direction(raw_x, raw_y, current_map)\n",
    "                if best_dir is not None:\n",
    "                    result = ('A', current_dir) if current_dir == best_dir else (self.INT_TO_ACTION[best_dir], best_dir)\n",
    "                else:\n",
    "                    result = (None, None)\n",
    "            elif current_dir in untried:\n",
    "                result = ('A', current_dir)\n",
    "            else:\n",
    "                target_dir = untried[0]\n",
    "                result = (self.INT_TO_ACTION[target_dir], target_dir)\n",
    "        \n",
    "        self._probe_cache_position = cache_key\n",
    "        self._cached_probe_action, self._cached_probe_dir = result\n",
    "        return result\n",
    "    \n",
    "    def record_tile_interaction_attempt(self, x, y, map_id, direction, success):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        tile_state['directions_tried'].add(direction)\n",
    "        tile_state['direction_attempts'][direction] = tile_state['direction_attempts'].get(direction, 0) + 1\n",
    "        if success:\n",
    "            tile_state['direction_successes'][direction] = tile_state['direction_successes'].get(direction, 0) + 1\n",
    "            memory = self.get_current_map_memory(map_id)\n",
    "            dir_name = self.DIRECTION_NAMES.get(direction, str(direction))\n",
    "            interactable = [int(x), int(y), dir_name]\n",
    "            if interactable not in memory['interactable_objects']:\n",
    "                memory['interactable_objects'].append(interactable)\n",
    "                print(f\"  üéØ INTERACTABLE FOUND: ({x}, {y}) facing {dir_name}\")\n",
    "        self._check_tile_exhaustion(x, y, map_id)\n",
    "    \n",
    "    def _check_tile_exhaustion(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return\n",
    "        if not any(tile_state['direction_successes'].get(d, 0) > 0 for d in range(4)):\n",
    "            tile_state['exhausted'] = True\n",
    "            print(f\"  ‚úì Tile ({x}, {y}) exhausted - no interactions found\")\n",
    "    \n",
    "    def get_direction_success_rate(self, x, y, map_id, direction):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        attempts = tile_state['direction_attempts'].get(direction, 0)\n",
    "        if attempts == 0:\n",
    "            return None\n",
    "        return tile_state['direction_successes'].get(direction, 0) / attempts\n",
    "    \n",
    "    def start_interaction_verification(self, x, y, map_id, direction):\n",
    "        self.pending_interaction_verify = {'x': x, 'y': y, 'map_id': map_id, 'direction': direction}\n",
    "        self.interaction_verify_countdown = self.INTERACTION_VERIFY_FRAMES\n",
    "    \n",
    "    def check_interaction_verification(self, context_state, prev_context_state):\n",
    "        if self.pending_interaction_verify is None:\n",
    "            return\n",
    "        self.interaction_verify_countdown -= 1\n",
    "        success = False\n",
    "        if prev_context_state is not None:\n",
    "            menu_changed = abs(context_state[4] - prev_context_state[4]) > 0.1\n",
    "            battle_started = context_state[3] > 0.5 and prev_context_state[3] <= 0.5\n",
    "            map_changed = int(context_state[2]) != int(prev_context_state[2])\n",
    "            success = menu_changed or battle_started or map_changed\n",
    "        if success or self.interaction_verify_countdown <= 0:\n",
    "            info = self.pending_interaction_verify\n",
    "            self.record_tile_interaction_attempt(info['x'], info['y'], info['map_id'], info['direction'], success)\n",
    "            self.pending_interaction_verify = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITION SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def record_transition(self, from_pos, from_map, to_map, direction, action_type):\n",
    "        memory = self.get_current_map_memory(from_map)\n",
    "        for t in memory['transitions']:\n",
    "            if t['position'] == from_pos and t['direction'] == direction:\n",
    "                t['use_count'] += 1\n",
    "                t['last_used'] = self.timestep\n",
    "                return\n",
    "        memory['transitions'].append({\n",
    "            'position': from_pos, 'direction': direction, 'action': action_type,\n",
    "            'destination_map': to_map, 'use_count': 1, 'last_used': self.timestep\n",
    "        })\n",
    "        print(f\"  üö™ TRANSITION FOUND: Map {from_map} ({from_pos}) ‚Üí Map {to_map}\")\n",
    "\n",
    "    def get_transition_attraction(self, current_map):\n",
    "        memory = self.get_current_map_memory(current_map)\n",
    "        transitions = memory.get('transitions', [])\n",
    "        if not transitions:\n",
    "            return 0.0, None\n",
    "        current_debt = self.map_novelty_debt.get(current_map, 0.0)\n",
    "        current_temp_debt = self.get_temp_debt(current_map)\n",
    "        current_coverage = self.get_exploration_coverage(current_map)\n",
    "        best_attraction, best_transition = 0.0, None\n",
    "        for t in transitions:\n",
    "            if self.is_transition_banned(current_map, t['position'], t['direction']):\n",
    "                continue\n",
    "            dest_map = t['destination_map']\n",
    "            dest_debt = self.map_novelty_debt.get(dest_map, 0.0)\n",
    "            dest_temp_debt = self.get_temp_debt(dest_map)\n",
    "            dest_coverage = self.get_exploration_coverage(dest_map)\n",
    "            debt_diff = (current_debt + current_temp_debt * 2.0) - (dest_debt + dest_temp_debt * 2.0)\n",
    "            coverage_diff = current_coverage - dest_coverage\n",
    "            attraction = debt_diff * 0.5 + coverage_diff * 0.5\n",
    "            if t['use_count'] < 3:\n",
    "                attraction *= 1.5\n",
    "            if attraction > best_attraction:\n",
    "                best_attraction, best_transition = attraction, t\n",
    "        return best_attraction * self.TRANSITION_ATTRACTION_WEIGHT, best_transition\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITION BAN SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def create_transition_ban(self, map_id, tile_pos, direction_back):\n",
    "        self.transition_bans[map_id] = {\n",
    "            'banned_tile': tile_pos, 'banned_direction': direction_back,\n",
    "            'vicinity_radius': self.BAN_VICINITY_RADIUS, 'vicinity_active': False,\n",
    "            'created_at': self.timestep\n",
    "        }\n",
    "        print(f\"  üö´ TRANSITION BAN: Map {map_id} at {tile_pos} facing {self.DIRECTION_NAMES.get(direction_back, '?')}\")\n",
    "    \n",
    "    def is_transition_banned(self, map_id, position, direction):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return False\n",
    "        ban = self.transition_bans[map_id]\n",
    "        banned_tile = tuple(ban['banned_tile']) if isinstance(ban['banned_tile'], list) else ban['banned_tile']\n",
    "        position = tuple(position) if isinstance(position, list) else position\n",
    "        if position == banned_tile and direction == ban['banned_direction']:\n",
    "            return True\n",
    "        if ban['vicinity_active']:\n",
    "            dist = abs(position[0] - banned_tile[0]) + abs(position[1] - banned_tile[1])\n",
    "            if dist <= ban['vicinity_radius'] and direction == ban['banned_direction']:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_position_banned(self, map_id, x, y, direction):\n",
    "        return self.is_transition_banned(map_id, (x, y), direction)\n",
    "    \n",
    "    def update_transition_ban(self, map_id, current_pos):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return\n",
    "        ban = self.transition_bans[map_id]\n",
    "        banned_tile = tuple(ban['banned_tile']) if isinstance(ban['banned_tile'], list) else ban['banned_tile']\n",
    "        if not ban['vicinity_active'] and abs(current_pos[0] - banned_tile[0]) + abs(current_pos[1] - banned_tile[1]) >= 3:\n",
    "            ban['vicinity_active'] = True\n",
    "            print(f\"  üö´ VICINITY BAN ACTIVE: Map {map_id}\")\n",
    "    \n",
    "    def check_ban_lift_conditions(self, map_id):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return\n",
    "        ban = self.transition_bans[map_id]\n",
    "        should_lift, reason = False, \"\"\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        non_banned = [t for t in memory.get('transitions', []) if not self.is_transition_banned(map_id, t['position'], t['direction'])]\n",
    "        if non_banned:\n",
    "            should_lift, reason = True, \"alternative transition found\"\n",
    "        elif self.get_exploration_coverage(map_id) >= self.BAN_COVERAGE_LIFT_THRESHOLD:\n",
    "            should_lift, reason = True, f\"coverage reached\"\n",
    "        elif self.timestep - ban['created_at'] >= self.BAN_TIMEOUT_STEPS:\n",
    "            should_lift, reason = True, \"timeout\"\n",
    "        if should_lift:\n",
    "            del self.transition_bans[map_id]\n",
    "            print(f\"  ‚úÖ BAN LIFTED: Map {map_id} - {reason}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # DEBT SYSTEMS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        raw_debt = memory.get('temp_debt', 0.0)\n",
    "        if map_id != self.current_map_id:\n",
    "            steps_away = self.timestep - memory.get('last_visited_timestep', 0)\n",
    "            return max(0.0, raw_debt - steps_away * self.TEMP_DEBT_DECAY)\n",
    "        return raw_debt\n",
    "\n",
    "    def accumulate_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['temp_debt'] = min(self.TEMP_DEBT_MAX, memory.get('temp_debt', 0.0) + self.TEMP_DEBT_ACCUMULATION)\n",
    "\n",
    "    def decay_all_debts(self):\n",
    "        \"\"\"Decay debts for non-current locations.\"\"\"\n",
    "        for map_id in list(self.map_novelty_debt.keys()):\n",
    "            if map_id != self.current_map_id:\n",
    "                self.map_novelty_debt[map_id] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.map_novelty_debt[map_id] < 0.1:\n",
    "                    del self.map_novelty_debt[map_id]\n",
    "        \n",
    "        current_loc = None\n",
    "        if self.current_map_id is not None and len(self.last_positions) > 0:\n",
    "            pos = self.last_positions[-1]\n",
    "            current_loc = self.get_location_key(pos[0], pos[1], self.current_map_id)\n",
    "        \n",
    "        for loc in list(self.location_novelty.keys()):\n",
    "            if loc != current_loc:\n",
    "                self.location_novelty[loc] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.location_novelty[loc] < 0.1:\n",
    "                    del self.location_novelty[loc]\n",
    "\n",
    "    def get_exploration_coverage(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        visited = len(memory['visited_tiles'])\n",
    "        obstructions = len(memory['obstructions'])\n",
    "        if visited == 0 or visited + obstructions < 10:\n",
    "            return 0.0\n",
    "        return visited / (visited + obstructions)\n",
    "\n",
    "    def detect_obstruction(self, prev_context, context_state, raw_position, prev_raw_position):\n",
    "        if prev_context is None or prev_raw_position is None:\n",
    "            return False\n",
    "        if self.last_action not in ['UP', 'DOWN', 'LEFT', 'RIGHT']:\n",
    "            return False\n",
    "        if raw_position == prev_raw_position:\n",
    "            self.record_obstruction(raw_position[0], raw_position[1], int(context_state[2]), int(context_state[5]))\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # =========================================================================\n",
    "    # MENU TRAP B-BOOST\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_menu_trap_tracking(self, context_state, action_taken, raw_position=None):\n",
    "        current_pos = raw_position if raw_position else (round(context_state[0] * 255), round(context_state[1] * 255))\n",
    "        if self.menu_trap_position is not None and current_pos != self.menu_trap_position:\n",
    "            self.reset_menu_trap_boost()\n",
    "            return\n",
    "        if self.get_context_state_hash(context_state) == self.last_context_state_hash:\n",
    "            if action_taken in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "                self.menu_trap_frames += 1\n",
    "                self.menu_trap_position = current_pos\n",
    "                if self.menu_trap_frames > self.MENU_TRAP_THRESHOLD:\n",
    "                    if self.original_b_utility is None:\n",
    "                        for a in self.actions():\n",
    "                            if a.action == 'B':\n",
    "                                self.original_b_utility = a.utility\n",
    "                                break\n",
    "                    self.menu_trap_b_boost = min(self.B_BOOST_MAX, self.menu_trap_b_boost + self.B_BOOST_INCREMENT)\n",
    "        elif current_pos != self.menu_trap_position:\n",
    "            self.reset_menu_trap_boost()\n",
    "\n",
    "    def reset_menu_trap_boost(self):\n",
    "        if self.menu_trap_b_boost > 1.0 and self.original_b_utility is not None:\n",
    "            for a in self.actions():\n",
    "                if a.action == 'B':\n",
    "                    a.utility = self.original_b_utility\n",
    "                    break\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.original_b_utility = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # STANDARD METHODS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def add(self, p):\n",
    "        self.perceptrons.append(p)\n",
    "        self._cache_valid = False\n",
    "\n",
    "    def actions(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"action\"]\n",
    "\n",
    "    def entities(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"entity\"]\n",
    "\n",
    "    def get_location_key(self, x, y, map_id, bin_size=5):\n",
    "        return (int(map_id), int(x // bin_size) * bin_size, int(y // bin_size) * bin_size)\n",
    "\n",
    "    def is_near_map_edge(self, x, y):\n",
    "        return x < 10 or x > 245 or y < 10 or y > 245\n",
    "\n",
    "    def record_action_execution(self, action_name):\n",
    "        if action_name:\n",
    "            self.action_execution_count[action_name] = self.action_execution_count.get(action_name, 0) + 1\n",
    "\n",
    "    def get_position_stagnation(self):\n",
    "        if len(self.last_positions) < 2:\n",
    "            return 0\n",
    "        current_pos = self.last_positions[-1]\n",
    "        return sum(1 for pos in reversed(list(self.last_positions)[:-1]) if pos == current_pos)\n",
    "\n",
    "    def get_group_weight(self, group):\n",
    "        return sum(a.utility for a in self.actions() if a.group == group)\n",
    "\n",
    "    # ============================================================================\n",
    "# CELL 3 PART 3: Brain - Stagnation, Blend Triggers, Learning, Save/Load\n",
    "# ============================================================================\n",
    "# CHANGES:\n",
    "# 1. get_blend_tier() determines current blend tier from stagnation metrics\n",
    "# 2. should_force_random() also triggers blend before randomizing\n",
    "# 3. on_productive_change() resets blend tier to 0\n",
    "# 4. Harder pattern/repetition penalties\n",
    "# 5. stagnation_level() handles dimension mismatch\n",
    "# 6. Movement boost gated on repetition count\n",
    "# ============================================================================\n",
    "\n",
    "    # =========================================================================\n",
    "    # BLEND TIER DETECTION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_blend_tier(self):\n",
    "        \"\"\"\n",
    "        Determine blend tier based on current stagnation metrics.\n",
    "        Returns 0 (no blend), 1 (light), 2 (medium), or 3 (hard).\n",
    "        Higher tier = more taught model influence.\n",
    "        \"\"\"\n",
    "        # Tier 3 (hard): extreme stagnation\n",
    "        t3 = self.BLEND_TIER_TRIGGERS[3]\n",
    "        if (self.detected_pattern and self.pattern_repeat_count >= t3['pattern_repeats']):\n",
    "            return 3\n",
    "        if self.state_stagnation_count >= self.STATE_STAGNATION_THRESHOLD * t3['state_stagnation_mult']:\n",
    "            return 3\n",
    "        \n",
    "        # Tier 2 (medium): significant stagnation\n",
    "        t2 = self.BLEND_TIER_TRIGGERS[2]\n",
    "        if (self.detected_pattern and self.pattern_repeat_count >= t2['pattern_repeats']):\n",
    "            return 2\n",
    "        if self.get_position_stagnation() >= t2['pos_stagnation']:\n",
    "            return 2\n",
    "        if self.consecutive_action_count >= t2['consecutive']:\n",
    "            return 2\n",
    "        \n",
    "        # Tier 1 (light): early stagnation\n",
    "        t1 = self.BLEND_TIER_TRIGGERS[1]\n",
    "        if (self.detected_pattern and self.pattern_repeat_count >= t1['pattern_repeats']):\n",
    "            return 1\n",
    "        if self.get_position_stagnation() >= t1['pos_stagnation']:\n",
    "            return 1\n",
    "        if self.consecutive_action_count >= t1['consecutive']:\n",
    "            return 1\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def try_blend_if_needed(self):\n",
    "        \"\"\"\n",
    "        Check if blend should trigger. Called from action selection.\n",
    "        Returns True if a blend was performed.\n",
    "        \"\"\"\n",
    "        if not self.taught_reference['loaded']:\n",
    "            return False\n",
    "        \n",
    "        tier = self.get_blend_tier()\n",
    "        \n",
    "        if tier == 0:\n",
    "            return False\n",
    "        \n",
    "        # Only blend if tier escalated or cooldown passed\n",
    "        if tier <= self.blend_tier and (self.timestep - self.last_blend_timestep) < self.BLEND_COOLDOWN:\n",
    "            return False\n",
    "        \n",
    "        self.blend_from_taught(tier)\n",
    "        return True\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODE SWAP & STAGNATION  \n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_context_state_hash(self, context_state):\n",
    "        return (round(context_state[0], 2), round(context_state[1], 2), int(context_state[2]),\n",
    "                int(context_state[3]), round(context_state[4], 2), int(context_state[5]))\n",
    "\n",
    "    def check_state_stagnation(self, context_state):\n",
    "        current_hash = self.get_context_state_hash(context_state)\n",
    "        if current_hash == self.last_context_state_hash:\n",
    "            self.state_stagnation_count += 1\n",
    "            if self.state_stagnation_count == 1 and self.last_action:\n",
    "                self.stagnation_initiator_action = self.last_action\n",
    "        else:\n",
    "            self.state_stagnation_count = 0\n",
    "            self.stagnation_initiator_action = None\n",
    "        self.last_context_state_hash = current_hash\n",
    "        return self.state_stagnation_count >= self.STATE_STAGNATION_THRESHOLD\n",
    "\n",
    "    def check_position_stagnation(self):\n",
    "        return self.get_position_stagnation()\n",
    "\n",
    "    def should_force_random(self):\n",
    "        \"\"\"\n",
    "        Returns True if the agent is badly stuck and needs forced randomization.\n",
    "        Also triggers blend from taught model before randomizing.\n",
    "        \"\"\"\n",
    "        force = False\n",
    "        \n",
    "        if self.get_position_stagnation() >= 8:\n",
    "            force = True\n",
    "        if self.consecutive_action_count >= 15:\n",
    "            force = True\n",
    "        if self.detected_pattern and self.pattern_repeat_count >= 4:\n",
    "            force = True\n",
    "        if self.state_stagnation_count >= self.STATE_STAGNATION_THRESHOLD * 2:\n",
    "            force = True\n",
    "        \n",
    "        if force:\n",
    "            # Attempt blend before randomizing ‚Äî blend fixes priorities,\n",
    "            # random breaks the immediate loop\n",
    "            self.try_blend_if_needed()\n",
    "        \n",
    "        return force\n",
    "\n",
    "    def get_forced_random_action_name(self):\n",
    "        \"\"\"Pick a random action that ISN'T the currently repeated one or in the pattern.\"\"\"\n",
    "        candidates = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"A\", \"B\"]\n",
    "        \n",
    "        if self.current_repeated_action and self.current_repeated_action in candidates:\n",
    "            candidates.remove(self.current_repeated_action)\n",
    "        \n",
    "        if self.detected_pattern:\n",
    "            for a in self.detected_pattern:\n",
    "                if a in candidates:\n",
    "                    candidates.remove(a)\n",
    "        \n",
    "        if not candidates:\n",
    "            candidates = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "            if self.current_repeated_action in candidates:\n",
    "                candidates.remove(self.current_repeated_action)\n",
    "        \n",
    "        if not candidates:\n",
    "            candidates = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "        \n",
    "        return random.choice(candidates)\n",
    "\n",
    "    def check_direction_change_progress(self, context_state):\n",
    "        current_dir = int(context_state[5])\n",
    "        if self.last_direction_for_progress is None:\n",
    "            self.last_direction_for_progress = current_dir\n",
    "            return False\n",
    "        changed = current_dir != self.last_direction_for_progress\n",
    "        self.last_direction_for_progress = current_dir\n",
    "        return changed\n",
    "\n",
    "    def apply_stagnation_initiator_penalty(self):\n",
    "        if self.stagnation_initiator_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.stagnation_initiator_action:\n",
    "                old_util = a.utility\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                a.utility = max(floor, a.utility * 0.5)\n",
    "                print(f\"  üìç STAGNATION PENALTY: {self.stagnation_initiator_action} {old_util:.3f} ‚Üí {a.utility:.3f}\")\n",
    "                break\n",
    "        self.stagnation_initiator_action = None\n",
    "\n",
    "    def check_productive_change(self, context_state):\n",
    "        current_map = int(context_state[2])\n",
    "        current_battle = context_state[3] > 0.5\n",
    "        current_pos = (context_state[0], context_state[1])\n",
    "        productive, reason = False, \"\"\n",
    "        \n",
    "        if self.last_map_id is not None and current_map != self.last_map_id:\n",
    "            productive, reason = True, \"map change\"\n",
    "        if self.last_battle_state is not None and current_battle != self.last_battle_state:\n",
    "            productive, reason = True, \"battle change\"\n",
    "        if self.position_at_mode_swap is not None:\n",
    "            dist = np.sqrt((current_pos[0] - self.position_at_mode_swap[0])**2 + \n",
    "                          (current_pos[1] - self.position_at_mode_swap[1])**2)\n",
    "            if dist > 0.03:\n",
    "                productive, reason = True, f\"moved {dist*255:.1f} tiles\"\n",
    "        \n",
    "        if self.direction_change_counts_as_progress and self.check_direction_change_progress(context_state):\n",
    "            self.state_stagnation_count = max(0, self.state_stagnation_count - 5)\n",
    "        \n",
    "        self.last_map_id = current_map\n",
    "        self.last_battle_state = current_battle\n",
    "        return productive, reason\n",
    "\n",
    "    def on_productive_change(self, reason):\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.swap_chain_count = 0\n",
    "        self.state_stagnation_count = 0\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.unproductive_swap_count = 0\n",
    "        \n",
    "        # Reset blend tier on productive progress\n",
    "        if self.blend_tier > 0:\n",
    "            print(f\"  ‚úÖ Blend tier reset: {self.blend_tier} ‚Üí 0 ({reason})\")\n",
    "            self.blend_tier = 0\n",
    "\n",
    "    def on_mode_swap(self, from_mode, to_mode):\n",
    "        self.swap_chain_count += 1\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.unproductive_swap_count += 1\n",
    "        if self.unproductive_swap_count >= self.UNPRODUCTIVE_SWAP_THRESHOLD:\n",
    "            self._reset_highest_to_third(to_mode)\n",
    "            self.unproductive_swap_count = 0\n",
    "        if to_mode == \"interact\":\n",
    "            self.interact_to_move_threshold = min(self.MAX_THRESHOLD, self.interact_to_move_threshold + self.THRESHOLD_INCREMENT)\n",
    "        else:\n",
    "            self.move_to_interact_threshold = min(self.MAX_THRESHOLD, self.move_to_interact_threshold + self.THRESHOLD_INCREMENT)\n",
    "\n",
    "    def _reset_highest_to_third(self, mode):\n",
    "        if mode in [\"battle\", \"both\"]:\n",
    "            return\n",
    "        group = \"move\" if mode == \"move\" else \"interact\"\n",
    "        group_actions = [a for a in self.actions() if a.group == group]\n",
    "        if len(group_actions) < 3:\n",
    "            return\n",
    "        sorted_actions = sorted(group_actions, key=lambda a: a.utility, reverse=True)\n",
    "        floor = self.INTERACT_UTILITY_FLOOR if group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "        sorted_actions[0].utility = max(sorted_actions[2].utility * 0.9, floor)\n",
    "\n",
    "    def should_use_both_mode(self):\n",
    "        return (self.state_stagnation_count > self.BOTH_MODE_STAGNATION_THRESHOLD or \n",
    "                self.unproductive_swap_count > self.BOTH_MODE_SWAP_THRESHOLD)\n",
    "\n",
    "    def determine_control_mode(self, context_state, raw_position=None):\n",
    "        if context_state[3] > 0.5:\n",
    "            return \"battle\"\n",
    "        \n",
    "        self.frames_in_current_mode += 1\n",
    "        position_stagnation = self.get_position_stagnation()\n",
    "        \n",
    "        productive, reason = self.check_productive_change(context_state)\n",
    "        if productive:\n",
    "            self.on_productive_change(reason)\n",
    "        \n",
    "        if self.should_use_both_mode():\n",
    "            return \"both\"\n",
    "        \n",
    "        if self.check_state_stagnation(context_state):\n",
    "            self.apply_stagnation_initiator_penalty()\n",
    "            new_mode = \"interact\" if self.control_mode == \"move\" else \"move\"\n",
    "            self.control_mode = new_mode\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.on_mode_swap(self.control_mode, new_mode)\n",
    "            self.state_stagnation_count = 0\n",
    "            return self.control_mode\n",
    "        \n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        \n",
    "        tile_needs_probing = self.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "        untried_directions = self.get_untried_directions(raw_x, raw_y, current_map)\n",
    "        \n",
    "        if tile_needs_probing and untried_directions and self.control_mode == \"move\" and self.frames_in_current_mode >= 3:\n",
    "            self.control_mode = \"interact\"\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.frames_in_current_mode = 0\n",
    "            return self.control_mode\n",
    "        \n",
    "        if self.control_mode == \"move\" and position_stagnation >= self.move_to_interact_threshold:\n",
    "            self.control_mode = \"interact\"\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.on_mode_swap(\"move\", \"interact\")\n",
    "        elif self.control_mode == \"interact\":\n",
    "            if (not tile_needs_probing or not untried_directions) and self.frames_in_current_mode >= 5:\n",
    "                self.control_mode = \"move\"\n",
    "                self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "                self.frames_in_current_mode = 0\n",
    "            elif self.frames_in_current_mode >= self.interact_to_move_threshold:\n",
    "                self.control_mode = \"move\"\n",
    "                self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "                self.on_mode_swap(\"interact\", \"move\")\n",
    "        \n",
    "        return self.control_mode\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION TRACKING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_exploration_tracking(self, context_state, prev_context_state, raw_position=None, prev_raw_position=None):\n",
    "        current_map = int(context_state[2])\n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_pos = (raw_x, raw_y)\n",
    "        \n",
    "        if self.current_map_id is not None and current_map != self.current_map_id:\n",
    "            prev_map = self.current_map_id\n",
    "            if prev_context_state is not None and prev_raw_position is not None:\n",
    "                self.record_transition(prev_raw_position, prev_map, current_map,\n",
    "                    int(prev_context_state[5]), 'interact' if self.last_action == 'A' else 'walk')\n",
    "            if prev_raw_position is not None:\n",
    "                entry_dir = int(context_state[5]) if prev_context_state is not None else 0\n",
    "                self.create_transition_ban(current_map, current_pos, (entry_dir + 2) % 4)\n",
    "            self.on_map_change(current_map)\n",
    "        \n",
    "        self.current_map_id = current_map\n",
    "        self.record_visited_tile(raw_x, raw_y, current_map)\n",
    "        self.accumulate_temp_debt(current_map)\n",
    "        self.update_transition_ban(current_map, current_pos)\n",
    "        self.check_ban_lift_conditions(current_map)\n",
    "        \n",
    "        if prev_context_state is not None and prev_raw_position is not None:\n",
    "            self.detect_obstruction(prev_context_state, context_state, raw_position, prev_raw_position)\n",
    "        \n",
    "        self.check_interaction_verification(context_state, prev_context_state)\n",
    "        self.last_direction = int(context_state[5])\n",
    "        \n",
    "        if self.timestep % 300 == 0:\n",
    "            self.decay_all_debts()\n",
    "\n",
    "    def on_map_change(self, new_map):\n",
    "        self.save_exploration_memory()\n",
    "        self.control_mode = \"move\"\n",
    "        self.frames_in_current_mode = 0\n",
    "        memory = self.get_current_map_memory(new_map)\n",
    "        tile_interactions = memory.get('tile_interactions', {})\n",
    "        print(f\"  üó∫Ô∏è MAP CHANGE ‚Üí {new_map}: {len(memory['visited_tiles'])} visited, {len(memory['obstructions'])} obs\")\n",
    "        print(f\"     Tiles probed: {len(tile_interactions)}, exhausted: {sum(1 for t in tile_interactions.values() if t.get('exhausted', False))}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # REPETITION & PATTERN HANDLING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def track_consecutive_action(self, action_name):\n",
    "        if action_name == self.current_repeated_action:\n",
    "            self.consecutive_action_count += 1\n",
    "        else:\n",
    "            self.current_repeated_action = action_name\n",
    "            self.consecutive_action_count = 1\n",
    "\n",
    "    def get_learning_multiplier(self, action_name):\n",
    "        if action_name != self.current_repeated_action or self.consecutive_action_count < self.LEARNING_SLOWDOWN_START:\n",
    "            return 1.0\n",
    "        progress = min(1.0, (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) / \n",
    "                       (self.LEARNING_SLOWDOWN_MAX - self.LEARNING_SLOWDOWN_START))\n",
    "        return max(0.05, 1.0 - 0.95 * progress)\n",
    "\n",
    "    def get_nth_highest_utility(self, group, n=3):\n",
    "        utilities = sorted([a.utility for a in self.actions() if a.group == group], reverse=True)\n",
    "        if len(utilities) < n:\n",
    "            return self.INTERACT_UTILITY_FLOOR if group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "        return utilities[n-1]\n",
    "\n",
    "    def detect_pattern(self):\n",
    "        if len(self.action_history) < 6:\n",
    "            return None, 0\n",
    "        recent = list(self.action_history)[-self.PATTERN_CHECK_WINDOW:]\n",
    "        for pattern_len in range(1, self.PATTERN_MAX_LENGTH + 1):\n",
    "            if len(recent) < pattern_len * self.PATTERN_MIN_REPEATS:\n",
    "                continue\n",
    "            candidate = tuple(recent[-pattern_len:])\n",
    "            repeat_count, idx = 0, len(recent) - pattern_len\n",
    "            while idx >= 0 and tuple(recent[idx:idx + pattern_len]) == candidate:\n",
    "                repeat_count += 1\n",
    "                idx -= pattern_len\n",
    "            if repeat_count >= self.PATTERN_MIN_REPEATS:\n",
    "                return candidate, repeat_count\n",
    "        return None, 0\n",
    "\n",
    "    def apply_pattern_penalty(self):\n",
    "        pattern, repeat_count = self.detect_pattern()\n",
    "        if pattern is None:\n",
    "            self.detected_pattern, self.pattern_repeat_count = None, 0\n",
    "            return\n",
    "        self.detected_pattern, self.pattern_repeat_count = pattern, repeat_count\n",
    "        \n",
    "        penalty_factor = max(0.3, 1.0 - repeat_count * 0.15)\n",
    "        \n",
    "        for action_name in set(pattern):\n",
    "            for a in self.actions():\n",
    "                if a.action == action_name:\n",
    "                    floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                    a.utility = max(floor, a.utility * penalty_factor)\n",
    "                    break\n",
    "\n",
    "    def apply_repetition_penalty(self):\n",
    "        if self.current_repeated_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.current_repeated_action:\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                if self.consecutive_action_count >= self.HARD_RESET_THRESHOLD:\n",
    "                    a.utility = floor\n",
    "                    self.consecutive_action_count = 0\n",
    "                    print(f\"  üî® HARD RESET: {a.action} ‚Üí {floor:.3f}\")\n",
    "                elif self.consecutive_action_count >= self.PENALTY_THRESHOLD:\n",
    "                    a.utility = max(a.utility * 0.5, floor)\n",
    "                break\n",
    "\n",
    "    # =========================================================================\n",
    "    # ENTITY & LEARNING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def spawn_innate_entities(self, learning_state):\n",
    "        if self.innate_entities_spawned:\n",
    "            return\n",
    "        for etype, indices in [(\"sense_menu\", [5, 6]), (\"sense_battle\", [3, 4]), \n",
    "                                (\"sense_movement\", [0, 1]), (\"sense_map_transition\", [2])]:\n",
    "            entity = Perceptron(\"entity\", entity_type=etype)\n",
    "            entity.ensure_weights(len(learning_state))\n",
    "            entity.weights = np.zeros(len(learning_state))\n",
    "            for idx in indices:\n",
    "                entity.weights[idx] = 0.5 if len(indices) > 1 else 1.0\n",
    "            self.add(entity)\n",
    "        self.innate_entities_spawned = True\n",
    "\n",
    "    def enforce_utility_floors(self):\n",
    "        for a in self.actions():\n",
    "            floor = self.MOVE_UTILITY_FLOOR if a.group == \"move\" else self.INTERACT_UTILITY_FLOOR\n",
    "            a.utility = max(a.utility, floor)\n",
    "\n",
    "    def get_spawn_threshold_adaptive(self, error_type='combined', percentile=50):\n",
    "        history = {'numeric': self.numeric_error_history, 'visual': self.visual_error_history}.get(error_type, self.error_history)\n",
    "        return max(0.001, np.percentile(history, percentile)) if len(history) >= 100 else 0.0005\n",
    "\n",
    "    def stagnation_level(self, window=10):\n",
    "        if len(self.prev_learning_states) < window:\n",
    "            return 0.0\n",
    "        recent = list(self.prev_learning_states)[-window:]\n",
    "        diffs = []\n",
    "        for i in range(1, len(recent)):\n",
    "            a, b = recent[i], recent[i-1]\n",
    "            min_dim = min(len(a), len(b))\n",
    "            diffs.append(np.linalg.norm(a[:min_dim] - b[:min_dim]))\n",
    "        return 1.0 - np.tanh(np.mean(diffs) * 2.0)\n",
    "\n",
    "    def predict_future_error(self, state, action, context_state, raw_position=None):\n",
    "        entity_novelty = np.mean([e.predict(state) * e.utility for e in self.entities()]) if self.entities() else 0.5\n",
    "        combined = entity_novelty * 0.7 + action.utility * 0.3\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        \n",
    "        map_debt = min(self.map_novelty_debt.get(current_map, 0.0), self.MAX_MAP_DEBT)\n",
    "        loc_debt = min(self.location_novelty.get(loc, 0.0), self.MAX_LOCATION_DEBT)\n",
    "        total_debt = map_debt + self.get_temp_debt(current_map) + loc_debt * 0.5\n",
    "        combined *= 1.0 / (1.0 + total_debt * 5.0)\n",
    "        \n",
    "        if action.action == self.current_repeated_action and self.consecutive_action_count > self.LEARNING_SLOWDOWN_START:\n",
    "            combined *= 1.0 / (1.0 + (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) * 0.15)\n",
    "        if self.detected_pattern and action.action in self.detected_pattern:\n",
    "            combined *= 1.0 / (1.0 + self.pattern_repeat_count * 0.2)\n",
    "        \n",
    "        return combined + np.random.randn() * 0.05\n",
    "\n",
    "    def compute_multi_modal_error(self, state, next_state):\n",
    "        diffs = [abs(next_state[i] - state[i]) for i in range(min(8, len(state), len(next_state)))]\n",
    "        weights = [0.5, 0.5, 10.0, 5.0, 3.0, 2.0, 1.5, 0.3]\n",
    "        weighted = sum(d * w for d, w in zip(diffs, weights)) + np.linalg.norm(next_state[8:] - state[8:]) * 2.0\n",
    "        numeric = sum(diffs)\n",
    "        visual = np.linalg.norm(next_state[8:] - state[8:])\n",
    "        return weighted, numeric, visual\n",
    "\n",
    "    def learn(self, learning_state, next_learning_state, context_state, next_context_state, dead=False,\n",
    "              raw_position=None, next_raw_position=None):\n",
    "        if learning_state.shape != next_learning_state.shape:\n",
    "            max_dim = max(len(learning_state), len(next_learning_state))\n",
    "            learning_state = np.pad(learning_state, (0, max(0, max_dim - len(learning_state))))\n",
    "            next_learning_state = np.pad(next_learning_state, (0, max(0, max_dim - len(next_learning_state))))\n",
    "        \n",
    "        if not self.innate_entities_spawned:\n",
    "            self.spawn_innate_entities(learning_state)\n",
    "        \n",
    "        prev_context = self.prev_context_states[-1] if self.prev_context_states else None\n",
    "        prev_raw = getattr(self, '_last_raw_position', None)\n",
    "        self.update_exploration_tracking(context_state, prev_context, raw_position, prev_raw)\n",
    "        self._last_raw_position = raw_position\n",
    "        \n",
    "        weighted_error, numeric_error, visual_error = self.compute_multi_modal_error(learning_state, next_learning_state)\n",
    "        self.error_history.append(weighted_error)\n",
    "        self.numeric_error_history.append(numeric_error)\n",
    "        self.visual_error_history.append(visual_error)\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        \n",
    "        self.visited_maps[current_map] = self.visited_maps.get(current_map, 0) + 1\n",
    "        self.location_memory[loc] = self.location_memory.get(loc, 0) + 1\n",
    "        \n",
    "        if self.visited_maps[current_map] > 10:\n",
    "            self.map_novelty_debt[current_map] = min(self.MAX_MAP_DEBT, \n",
    "                self.map_novelty_debt.get(current_map, 0.0) + 0.05 * (self.visited_maps[current_map] - 10))\n",
    "        if self.location_memory[loc] > 15:\n",
    "            self.location_novelty[loc] = min(self.MAX_LOCATION_DEBT,\n",
    "                self.location_novelty.get(loc, 0.0) + 0.1 * (self.location_memory[loc] - 15))\n",
    "        \n",
    "        if self.visited_maps[current_map] > 30:\n",
    "            weighted_error *= 0.5\n",
    "        if self.location_memory[loc] > 25:\n",
    "            weighted_error *= 0.7\n",
    "        \n",
    "        stagnation = self.stagnation_level()\n",
    "        learning_mult = self.get_learning_multiplier(self.last_action) if self.last_action else 1.0\n",
    "        if self.detected_pattern and self.last_action in self.detected_pattern:\n",
    "            learning_mult *= 0.5\n",
    "        \n",
    "        for p in self.perceptrons:\n",
    "            mult = learning_mult if (p.kind == \"action\" and p.action == self.last_action) else 1.0\n",
    "            if p.kind == \"action\" and self.detected_pattern and p.action in self.detected_pattern:\n",
    "                mult *= 0.5\n",
    "            p.update(learning_state, weighted_error * mult, stagnation=stagnation)\n",
    "        \n",
    "        for a in self.actions():\n",
    "            if a.action in ['Start', 'Select'] and a.weights is not None:\n",
    "                a.weights *= 0.999\n",
    "        \n",
    "        self.apply_repetition_penalty()\n",
    "        self.apply_pattern_penalty()\n",
    "        self.enforce_utility_floors()\n",
    "        \n",
    "        # Movement boost - ONLY if not stuck in repetition\n",
    "        if prev_context is not None and np.linalg.norm(context_state[:2] - prev_context[:2]) > 0.001:\n",
    "            if self.last_action and self.consecutive_action_count < self.PENALTY_THRESHOLD:\n",
    "                for a in self.actions():\n",
    "                    if a.action == self.last_action:\n",
    "                        boost = 1.15 if raw_position and self.is_near_map_edge(*raw_position) else 1.08\n",
    "                        a.utility = min(a.utility * boost, 2.0)\n",
    "                        break\n",
    "        \n",
    "        if self.timestep % self.SAVE_INTERVAL == 0:\n",
    "            self.save_exploration_memory()\n",
    "        \n",
    "        self.action_history.append(self.last_action)\n",
    "\n",
    "    def log_state(self, learning_state, context_state):\n",
    "        self.prev_learning_states.append(learning_state)\n",
    "        self.prev_context_states.append(context_state)\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        self.last_positions.append((int(x), int(y)))\n",
    "    \n",
    "    def get_tile_interaction_stats(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_interactions = memory.get('tile_interactions', {})\n",
    "        return {\n",
    "            'probed': len(tile_interactions),\n",
    "            'exhausted': sum(1 for t in tile_interactions.values() if t.get('exhausted', False)),\n",
    "            'with_success': sum(1 for t in tile_interactions.values() if any(t.get('direction_successes', {}).get(d, 0) > 0 for d in range(4)))\n",
    "        }\n",
    "\n",
    "    def load_taught_model(self, filepath):\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                model = json.load(f)\n",
    "            \n",
    "            if \"perceptrons\" not in model:\n",
    "                print(f\"  ‚ö†Ô∏è Model file empty or invalid, starting fresh\")\n",
    "                return 0\n",
    "            \n",
    "            for saved_action in model[\"perceptrons\"][\"actions\"]:\n",
    "                for a in self.actions():\n",
    "                    if a.action == saved_action[\"action\"]:\n",
    "                        a.utility = saved_action[\"utility\"]\n",
    "                        a.learning_rate = saved_action.get(\"learning_rate\", 0.01)\n",
    "                        a.familiarity = saved_action.get(\"familiarity\", 0.0)\n",
    "                        if saved_action.get(\"weights_nonzero\"):\n",
    "                            dim = saved_action.get(\"weights_shape\", 1376)\n",
    "                            a.weights = np.zeros(dim)\n",
    "                            for idx, val in saved_action[\"weights_nonzero\"]:\n",
    "                                if idx < dim:\n",
    "                                    a.weights[idx] = val\n",
    "                        break\n",
    "                    if a.action in ['Start', 'Select'] and a.weights is not None:\n",
    "                        a.weights = np.zeros(len(a.weights))\n",
    "                        a.utility = 0.05\n",
    "            \n",
    "            for saved_entity in model[\"perceptrons\"].get(\"entities\", []):\n",
    "                for e in self.entities():\n",
    "                    if e.entity_type == saved_entity[\"entity_type\"]:\n",
    "                        e.utility = saved_entity.get(\"utility\", 1.0)\n",
    "                        e.familiarity = saved_entity.get(\"familiarity\", 0.0)\n",
    "                        if saved_entity.get(\"weights_nonzero\"):\n",
    "                            dim = saved_entity.get(\"weights_shape\", 1376)\n",
    "                            e.weights = np.zeros(dim)\n",
    "                            for idx, val in saved_entity[\"weights_nonzero\"]:\n",
    "                                if idx < dim:\n",
    "                                    e.weights[idx] = val\n",
    "                        break\n",
    "            \n",
    "            if \"debt_tracking\" in model:\n",
    "                debt = model[\"debt_tracking\"]\n",
    "                self.map_novelty_debt = {int(k): v for k, v in debt.get(\"map_novelty_debt\", {}).items()}\n",
    "                self.visited_maps = {int(k): v for k, v in debt.get(\"visited_maps\", {}).items()}\n",
    "                for k, v in debt.get(\"location_novelty\", {}).items():\n",
    "                    self.location_novelty[eval(k)] = v\n",
    "            \n",
    "            loaded_timestep = model.get(\"timestep\", 0)\n",
    "            self.timestep = loaded_timestep\n",
    "            return loaded_timestep\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error loading model: {e}, starting fresh\")\n",
    "            return 0\n",
    "\n",
    "    def merge_taught_exploration(self, taught_filepath):\n",
    "        if not Path(taught_filepath).exists():\n",
    "            print(f\"  No taught exploration memory found at {taught_filepath}\")\n",
    "            return\n",
    "        \n",
    "        with open(taught_filepath, 'r') as f:\n",
    "            taught_data = json.load(f)\n",
    "        \n",
    "        transitions_added = 0\n",
    "        interactables_added = 0\n",
    "        \n",
    "        for map_key, taught_map in taught_data.items():\n",
    "            map_id = int(map_key.replace('map_', ''))\n",
    "            ai_map = self.get_current_map_memory(map_id)\n",
    "            \n",
    "            for t_trans in taught_map.get('transitions', []):\n",
    "                t_pos = tuple(t_trans['position'])\n",
    "                t_dir = t_trans['direction']\n",
    "                exists = any(\n",
    "                    tuple(existing['position']) == t_pos and existing['direction'] == t_dir\n",
    "                    for existing in ai_map['transitions']\n",
    "                )\n",
    "                if not exists:\n",
    "                    ai_map['transitions'].append(t_trans)\n",
    "                    transitions_added += 1\n",
    "            \n",
    "            for t_inter in taught_map.get('interactable_objects', []):\n",
    "                if t_inter not in ai_map['interactable_objects']:\n",
    "                    ai_map['interactable_objects'].append(t_inter)\n",
    "                    interactables_added += 1\n",
    "        \n",
    "        print(f\"  Merged: {transitions_added} transitions, {interactables_added} interactables\")\n",
    "    \n",
    "    def save_model_checkpoint(self, filepath):\n",
    "        model = {\n",
    "            \"timestep\": self.timestep,\n",
    "            \"perceptrons\": {\"actions\": [], \"entities\": []},\n",
    "            \"debt_tracking\": {\n",
    "                \"map_novelty_debt\": {str(k): v for k, v in self.map_novelty_debt.items()},\n",
    "                \"location_novelty\": {str(k): v for k, v in self.location_novelty.items()},\n",
    "                \"visited_maps\": {str(k): v for k, v in self.visited_maps.items()}\n",
    "            },\n",
    "            \"control_mode\": self.control_mode,\n",
    "            \"markov_stats\": {\n",
    "                \"markov_action_count\": self.markov_action_count,\n",
    "                \"curiosity_action_count\": self.curiosity_action_count\n",
    "            },\n",
    "            \"blend_stats\": {\n",
    "                \"blend_count\": self.blend_count,\n",
    "                \"last_blend_tier\": self.blend_tier\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for a in self.actions():\n",
    "            action_data = {\n",
    "                \"action\": a.action,\n",
    "                \"group\": a.group,\n",
    "                \"utility\": float(a.utility),\n",
    "                \"weights_shape\": len(a.weights) if a.weights is not None else 0,\n",
    "                \"weights_nonzero\": [[i, float(v)] for i, v in enumerate(a.weights) if abs(v) > 1e-10] if a.weights is not None else [],\n",
    "                \"learning_rate\": float(a.learning_rate),\n",
    "                \"familiarity\": float(a.familiarity)\n",
    "            }\n",
    "            model[\"perceptrons\"][\"actions\"].append(action_data)\n",
    "        \n",
    "        for e in self.entities():\n",
    "            entity_data = {\n",
    "                \"entity_type\": e.entity_type,\n",
    "                \"utility\": float(e.utility),\n",
    "                \"weights_shape\": len(e.weights) if e.weights is not None else 0,\n",
    "                \"weights_nonzero\": [[i, float(v)] for i, v in enumerate(e.weights) if abs(v) > 1e-10] if e.weights is not None else [],\n",
    "                \"familiarity\": float(e.familiarity)\n",
    "            }\n",
    "            model[\"perceptrons\"][\"entities\"].append(entity_data)\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(model, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1df11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Action Selection - With Markov + Cache Integration\n",
    "# ============================================================================\n",
    "# CHANGES FOR CACHE SYSTEM:\n",
    "# 1. anticipatory_action() accepts optional taught_frames parameter\n",
    "# 2. Passes taught_frames to brain.get_markov_action() for map-filtered scan\n",
    "# 3. No other changes\n",
    "# ============================================================================\n",
    "\n",
    "import random\n",
    "\n",
    "GBA_ACTIONS = [\"Up\", \"Down\", \"Left\", \"Right\", \"A\", \"B\", \"Start\", \"Select\"]\n",
    "ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "DIRECTION_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "ACTION_TO_DIRECTION = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "\n",
    "def manhattan_distance(pos1, pos2):\n",
    "    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "\n",
    "def anticipatory_action(brain, learning_state, context_state, \n",
    "                       exploration_weight=1.3, min_interact_prob=0.15,\n",
    "                       raw_position=None,\n",
    "                       forced_explore_prob=0.18,\n",
    "                       override_threshold=1.5,\n",
    "                       taught_frames=None,\n",
    "                       map_density=None):\n",
    "    \"\"\"\n",
    "    HYBRID ACTION SELECTION:\n",
    "    1. Forced random if stuck\n",
    "    2. Check Markov (imitation) - threshold adapts to data density\n",
    "    3. Otherwise Curiosity (exploration) - explore prob adapts to density\n",
    "    \n",
    "    map_density: dict from CacheManager.get_map_density()\n",
    "    \"\"\"\n",
    "    # === ADAPT THRESHOLDS TO DATA DENSITY ===\n",
    "    density = map_density or {'taught_frames': 0, 'tier': 'sparse', 'coverage': 0.0, 'visited': 0}\n",
    "    tier = density['tier']\n",
    "    \n",
    "    # Markov threshold: trust sparse data less (higher threshold), dense data more\n",
    "    markov_threshold = {\n",
    "        'sparse': 0.72,   # Very picky - few examples, need high confidence\n",
    "        'thin':   0.65,   # Slightly picky\n",
    "        'medium': 0.58,   # Standard\n",
    "        'dense':  0.50    # Trust the data\n",
    "    }.get(tier, MARKOV_FAMILIARITY_THRESHOLD)\n",
    "    \n",
    "    # Forced explore: explore MORE when sparse, less when dense\n",
    "    adapted_explore_prob = {\n",
    "        'sparse': 0.30,   # Lots to discover\n",
    "        'thin':   0.24,\n",
    "        'medium': 0.18,   # Standard  \n",
    "        'dense':  0.12    # Trust what we know\n",
    "    }.get(tier, forced_explore_prob)\n",
    "    \n",
    "    # Exploration weight: boost novelty-seeking when sparse\n",
    "    adapted_exploration_weight = {\n",
    "        'sparse': 1.8,\n",
    "        'thin':   1.5,\n",
    "        'medium': 1.3,\n",
    "        'dense':  1.1\n",
    "    }.get(tier, exploration_weight)\n",
    "    \n",
    "    # Transition attraction: scale up when well-explored, down when sparse\n",
    "    transition_weight_mult = {\n",
    "        'sparse': 0.3,    # Don't chase transitions, explore first\n",
    "        'thin':   0.6,\n",
    "        'medium': 1.0,\n",
    "        'dense':  1.4     # Actively seek exits when area is known\n",
    "    }.get(tier, 1.0)\n",
    "    actions_list = brain.actions()\n",
    "    if not actions_list:\n",
    "        return Perceptron(\"action\", action=\"UP\", group=\"move\")\n",
    "\n",
    "    # === FORCED RANDOMIZATION CHECK (before Markov or Curiosity) ===\n",
    "    # Always check stagnation first - this runs regardless of Markov\n",
    "    brain.check_state_stagnation(context_state)\n",
    "    \n",
    "    if brain.should_force_random():\n",
    "        forced_name = brain.get_forced_random_action_name()\n",
    "        for a in actions_list:\n",
    "            if a.action == forced_name:\n",
    "                brain.curiosity_action_count += 1\n",
    "                brain.record_action_execution(a.action)\n",
    "                brain.track_consecutive_action(a.action)\n",
    "                print(f\"  üé≤ FORCED RANDOM: {forced_name} (pos_stag={brain.get_position_stagnation()}, \"\n",
    "                      f\"repeat={brain.consecutive_action_count}, pattern={brain.pattern_repeat_count})\")\n",
    "                return a\n",
    "\n",
    "    # === MARKOV CHECK (with adapted threshold) ===\n",
    "    use_markov = False\n",
    "    markov_action = None\n",
    "    markov_confidence = 0.0\n",
    "    \n",
    "    if brain.markov_enabled and taught_frames:\n",
    "        score, action, idx = brain.compute_markov_similarity(\n",
    "            context_state, raw_position, taught_frames=taught_frames\n",
    "        )\n",
    "        brain.last_markov_score = score\n",
    "        \n",
    "        if score >= markov_threshold and action:\n",
    "            use_markov = True\n",
    "            markov_action = action\n",
    "            markov_confidence = score\n",
    "            brain.last_markov_action = action\n",
    "    \n",
    "    if use_markov and markov_action:\n",
    "        for a in actions_list:\n",
    "            if a.action == markov_action:\n",
    "                brain.markov_action_count += 1\n",
    "                brain.record_action_execution(a.action)\n",
    "                brain.track_consecutive_action(a.action)\n",
    "                \n",
    "                if a.action == 'A':\n",
    "                    raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "                    raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "                    current_map = int(context_state[2])\n",
    "                    if brain.should_interact_at_tile(raw_x, raw_y, current_map):\n",
    "                        brain.start_interaction_verification(\n",
    "                            raw_x, raw_y, current_map, int(context_state[5])\n",
    "                        )\n",
    "                \n",
    "                return a\n",
    "\n",
    "    # === CURIOSITY-DRIVEN SELECTION (unchanged) ===\n",
    "    brain.curiosity_action_count += 1\n",
    "    \n",
    "    mode = brain.determine_control_mode(context_state, raw_position=raw_position)\n",
    "    current_map = int(context_state[2])\n",
    "    current_dir = int(context_state[5])\n",
    "    \n",
    "    raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "    raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "    current_pos = (raw_x, raw_y)\n",
    "    \n",
    "    memory = brain.get_current_map_memory(current_map)\n",
    "    visited_tiles = memory['visited_tiles']\n",
    "    obstructions = memory['obstructions']\n",
    "    \n",
    "    tile_needs_probing = brain.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "    probe_action, probe_dir = brain.get_best_probe_action(raw_x, raw_y, current_map, current_dir)\n",
    "    \n",
    "    transition_attraction, best_transition = brain.get_transition_attraction(current_map)\n",
    "    coverage = brain.get_exploration_coverage(current_map)\n",
    "\n",
    "    # Forced random exploration (adapted to density)\n",
    "    if random.random() < adapted_explore_prob:\n",
    "        valid = [a for a in actions_list if a.action not in ['Start', 'Select']]\n",
    "        chosen = random.choice(valid)\n",
    "        brain.record_action_execution(chosen.action)\n",
    "        brain.track_consecutive_action(chosen.action)\n",
    "        if chosen.action == 'A' and tile_needs_probing:\n",
    "            brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "        return chosen\n",
    "\n",
    "    action_scores = {}\n",
    "    \n",
    "    for a in actions_list:\n",
    "        if a.action in ['Start', 'Select']:\n",
    "            action_scores[a.action] = (a, 0.0)\n",
    "            continue\n",
    "            \n",
    "        predicted = brain.predict_future_error(learning_state, a, context_state, raw_position=raw_position)\n",
    "        \n",
    "        if a.group == \"move\":\n",
    "            predicted *= adapted_exploration_weight\n",
    "            \n",
    "            dx, dy = ACTION_DELTAS.get(a.action, (0, 0))\n",
    "            target_tile = (raw_x + dx, raw_y + dy)\n",
    "            action_direction = ACTION_TO_DIRECTION.get(a.action, -1)\n",
    "            \n",
    "            if target_tile not in visited_tiles:\n",
    "                predicted *= brain.UNVISITED_TILE_BONUS\n",
    "            \n",
    "            if target_tile in obstructions:\n",
    "                predicted *= brain.OBSTRUCTION_PENALTY\n",
    "            \n",
    "            if brain.is_position_banned(current_map, raw_x, raw_y, action_direction):\n",
    "                predicted *= 0.05\n",
    "            \n",
    "            if transition_attraction > 0.3 and best_transition and coverage > 0.5:\n",
    "                trans_pos = tuple(best_transition['position']) if isinstance(best_transition['position'], list) else best_transition['position']\n",
    "                if manhattan_distance(target_tile, trans_pos) < manhattan_distance(current_pos, trans_pos):\n",
    "                    predicted *= (1.0 + transition_attraction * transition_weight_mult)\n",
    "            \n",
    "            if probe_action == a.action and probe_dir is not None:\n",
    "                predicted *= 2.0\n",
    "            \n",
    "            predicted *= (0.9 + random.random() * 0.2)\n",
    "        \n",
    "        elif a.group == \"interact\":\n",
    "            predicted = max(predicted, min_interact_prob)\n",
    "            \n",
    "            if a.action == 'B':\n",
    "                predicted *= brain.menu_trap_b_boost\n",
    "            \n",
    "            if a.action == 'A':\n",
    "                if tile_needs_probing and probe_action == 'A':\n",
    "                    predicted *= 3.0\n",
    "                elif tile_needs_probing:\n",
    "                    predicted *= 0.5\n",
    "                else:\n",
    "                    predicted *= 0.3\n",
    "        \n",
    "        action_scores[a.action] = (a, predicted)\n",
    "\n",
    "    if mode == \"battle\":\n",
    "        preferred_group = \"interact\"\n",
    "    elif mode == \"interact\":\n",
    "        preferred_group = \"interact\"\n",
    "    else:\n",
    "        preferred_group = \"move\"\n",
    "    \n",
    "    in_mode = [(a, s) for name, (a, s) in action_scores.items() if a.group == preferred_group and s > 0]\n",
    "    out_mode = [(a, s) for name, (a, s) in action_scores.items() if a.group != preferred_group and s > 0 and a.action not in ['Start', 'Select']]\n",
    "    \n",
    "    best_in_mode = max(in_mode, key=lambda x: x[1]) if in_mode else None\n",
    "    best_out_mode = max(out_mode, key=lambda x: x[1]) if out_mode else None\n",
    "    \n",
    "    chosen = None\n",
    "    \n",
    "    if best_in_mode and best_out_mode:\n",
    "        if best_out_mode[1] > best_in_mode[1] * override_threshold:\n",
    "            chosen = best_out_mode[0]\n",
    "        else:\n",
    "            chosen = best_in_mode[0]\n",
    "    elif best_in_mode:\n",
    "        chosen = best_in_mode[0]\n",
    "    elif best_out_mode:\n",
    "        chosen = best_out_mode[0]\n",
    "    else:\n",
    "        chosen = max(actions_list, key=lambda a: a.utility)\n",
    "    \n",
    "    brain.record_action_execution(chosen.action)\n",
    "    brain.track_consecutive_action(chosen.action)\n",
    "    \n",
    "    if chosen.action == 'A' and tile_needs_probing:\n",
    "        brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "    \n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2694c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Cache System - MapCache, CacheManager, IOThread\n",
    "# ============================================================================\n",
    "# NEW CELL - Layer on top of existing code, no rewrites\n",
    "#\n",
    "# MapCache: Per-map data container (exploration + taught transitions + live state)\n",
    "# CacheManager: Indexes all maps at startup, handles switching\n",
    "# IOThread: Background file I/O decoupled from Brain\n",
    "# ============================================================================\n",
    "\n",
    "import threading\n",
    "import gc\n",
    "\n",
    "class MapCache:\n",
    "    \"\"\"Thread-safe container for one map's data.\"\"\"\n",
    "\n",
    "    def __init__(self, map_id):\n",
    "        self.map_id = map_id\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "        # From exploration_memory[map_id] - synced to/from Brain\n",
    "        self.exploration_data = None  # Set by CacheManager\n",
    "\n",
    "        # From taught_transitions filtered by map_id\n",
    "        self.taught_frames = []\n",
    "\n",
    "        # Live state (IOThread writes, Brain reads)\n",
    "        self.current_state = np.zeros(EXPECTED_STATE_DIM)\n",
    "        self.palette = np.zeros(PALETTE_DIM)\n",
    "        self.tiles = np.zeros(TILE_DIM)\n",
    "        self.raw_position = (0, 0)\n",
    "        self.dead = False\n",
    "        self.state_fresh = False  # True when IOThread wrote new state\n",
    "        self.state_version = 0    # Increments each IOThread write\n",
    "\n",
    "        # Pending action (Brain writes, IOThread reads)\n",
    "        self.pending_action_out = None  # Action to write to file\n",
    "\n",
    "    def get_state(self):\n",
    "        with self.lock:\n",
    "            return (\n",
    "                self.current_state.copy(),\n",
    "                self.palette.copy(),\n",
    "                self.tiles.copy(),\n",
    "                self.dead,\n",
    "                self.raw_position\n",
    "            )\n",
    "\n",
    "    def update_state(self, context_state, palette, tiles, dead, raw_position):\n",
    "        with self.lock:\n",
    "            self.current_state = context_state\n",
    "            self.palette = palette\n",
    "            self.tiles = tiles\n",
    "            self.dead = dead\n",
    "            self.raw_position = raw_position\n",
    "            self.state_fresh = True\n",
    "            self.state_version += 1\n",
    "\n",
    "    def is_fresh(self):\n",
    "        with self.lock:\n",
    "            return self.state_fresh\n",
    "\n",
    "    def mark_consumed(self):\n",
    "        with self.lock:\n",
    "            self.state_fresh = False\n",
    "\n",
    "    def get_version(self):\n",
    "        with self.lock:\n",
    "            return self.state_version\n",
    "\n",
    "    def set_pending_action(self, action_name):\n",
    "        with self.lock:\n",
    "            self.pending_action_out = action_name\n",
    "\n",
    "    def get_pending_action(self):\n",
    "        with self.lock:\n",
    "            a = self.pending_action_out\n",
    "            self.pending_action_out = None\n",
    "            return a\n",
    "\n",
    "    def get_taught_frames(self):\n",
    "        \"\"\"Return taught transitions for this map (no lock needed, read-only after init).\"\"\"\n",
    "        return self.taught_frames\n",
    "\n",
    "\n",
    "class CacheManager:\n",
    "    \"\"\"Manages all MapCaches. Pre-indexes at startup, handles map switching.\"\"\"\n",
    "\n",
    "    def __init__(self, brain):\n",
    "        self.brain = brain\n",
    "        self.caches = {}        # map_id -> MapCache\n",
    "        self.active_cache = None\n",
    "        self.active_map_id = None\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def load_all(self, exploration_path=None, taught_path=None):\n",
    "        \"\"\"\n",
    "        Startup: Load exploration memory + taught transitions,\n",
    "        create MapCache for each map, index taught frames by map.\n",
    "        \"\"\"\n",
    "        exploration_path = exploration_path or EXPLORATION_MEMORY_FILE\n",
    "        taught_path = taught_path or TAUGHT_TRANSITIONS_FILE\n",
    "\n",
    "        # 1. Exploration memory is already loaded in Brain\n",
    "        for map_id, mem_data in self.brain.exploration_memory.items():\n",
    "            cache = self._get_or_create(map_id)\n",
    "            cache.exploration_data = mem_data\n",
    "\n",
    "        # 2. Index taught transitions by map_id\n",
    "        taught_by_map = {}\n",
    "        for t in self.brain.taught_transitions:\n",
    "            t_map = t.get('state', {}).get('map_id')\n",
    "            if t_map is not None:\n",
    "                taught_by_map.setdefault(t_map, []).append(t)\n",
    "\n",
    "        for map_id, frames in taught_by_map.items():\n",
    "            cache = self._get_or_create(map_id)\n",
    "            cache.taught_frames = frames\n",
    "\n",
    "        total_maps = len(self.caches)\n",
    "        total_taught = sum(len(c.taught_frames) for c in self.caches.values())\n",
    "        print(f\"  üì¶ CacheManager: {total_maps} maps cached, {total_taught} taught frames indexed\")\n",
    "\n",
    "    def _get_or_create(self, map_id):\n",
    "        if map_id not in self.caches:\n",
    "            self.caches[map_id] = MapCache(map_id)\n",
    "        return self.caches[map_id]\n",
    "\n",
    "    def get_active(self):\n",
    "        return self.active_cache\n",
    "\n",
    "    def detect_and_set_initial_map(self):\n",
    "        \"\"\"Read game_state.json once to determine starting map.\"\"\"\n",
    "        ctx, pal, til, dead, raw_pos = read_game_state()\n",
    "        map_id = int(ctx[2])\n",
    "        self._switch_to(map_id)\n",
    "        # Seed the active cache with the initial state\n",
    "        self.active_cache.update_state(ctx, pal, til, dead, raw_pos)\n",
    "        print(f\"  üì¶ Initial map: {map_id}\")\n",
    "        return map_id\n",
    "\n",
    "    def switch_map(self, new_map_id):\n",
    "        \"\"\"Called by main thread when map changes.\"\"\"\n",
    "        if new_map_id == self.active_map_id:\n",
    "            return\n",
    "        self._sync_from_brain()  # Save Brain's exploration data back to current cache\n",
    "        self._switch_to(new_map_id)\n",
    "        self._sync_to_brain()    # Load new cache's data into Brain\n",
    "\n",
    "    def _switch_to(self, map_id):\n",
    "        with self.lock:\n",
    "            cache = self._get_or_create(map_id)\n",
    "            self.active_cache = cache\n",
    "            self.active_map_id = map_id\n",
    "\n",
    "    def _sync_to_brain(self):\n",
    "        \"\"\"Push active cache's exploration data into Brain.\"\"\"\n",
    "        cache = self.active_cache\n",
    "        if cache and cache.exploration_data is not None:\n",
    "            self.brain.exploration_memory[cache.map_id] = cache.exploration_data\n",
    "\n",
    "    def _sync_from_brain(self):\n",
    "        \"\"\"Pull Brain's exploration data into active cache.\"\"\"\n",
    "        cache = self.active_cache\n",
    "        if cache and cache.map_id in self.brain.exploration_memory:\n",
    "            cache.exploration_data = self.brain.exploration_memory[cache.map_id]\n",
    "\n",
    "    def sync_all_from_brain(self):\n",
    "        \"\"\"Sync ALL maps from Brain back to caches (for saving).\"\"\"\n",
    "        for map_id, mem_data in self.brain.exploration_memory.items():\n",
    "            cache = self._get_or_create(map_id)\n",
    "            cache.exploration_data = mem_data\n",
    "\n",
    "    def save_exploration_memory(self):\n",
    "        \"\"\"Save all maps' exploration data to disk.\"\"\"\n",
    "        self._sync_from_brain()  # Make sure current map is synced\n",
    "        self.brain.save_exploration_memory()\n",
    "\n",
    "    def get_active_taught_frames(self):\n",
    "        \"\"\"Return taught frames for current map only (for fast Markov scan).\"\"\"\n",
    "        if self.active_cache:\n",
    "            return self.active_cache.get_taught_frames()\n",
    "        return []\n",
    "\n",
    "    def get_map_density(self):\n",
    "        \"\"\"\n",
    "        Returns a density dict for the active map, used to adapt thresholds.\n",
    "        \n",
    "        Density tiers:\n",
    "          sparse:  < 50 taught frames\n",
    "          thin:    50-200\n",
    "          medium:  200-1000\n",
    "          dense:   1000+\n",
    "          \n",
    "        Also includes exploration coverage and visited tile count.\n",
    "        \"\"\"\n",
    "        if not self.active_cache:\n",
    "            return {'taught_frames': 0, 'tier': 'sparse', 'coverage': 0.0, 'visited': 0}\n",
    "        \n",
    "        n_frames = len(self.active_cache.get_taught_frames())\n",
    "        map_id = self.active_map_id\n",
    "        \n",
    "        # Exploration data from brain\n",
    "        coverage = self.brain.get_exploration_coverage(map_id) if map_id is not None else 0.0\n",
    "        memory = self.brain.get_current_map_memory(map_id) if map_id is not None else {}\n",
    "        visited = len(memory.get('visited_tiles', set()))\n",
    "        \n",
    "        if n_frames < 50:\n",
    "            tier = 'sparse'\n",
    "        elif n_frames < 200:\n",
    "            tier = 'thin'\n",
    "        elif n_frames < 1000:\n",
    "            tier = 'medium'\n",
    "        else:\n",
    "            tier = 'dense'\n",
    "        \n",
    "        return {\n",
    "            'taught_frames': n_frames,\n",
    "            'tier': tier,\n",
    "            'coverage': coverage,\n",
    "            'visited': visited\n",
    "        }\n",
    "\n",
    "\n",
    "class IOThread(threading.Thread):\n",
    "    \"\"\"Background thread: reads game_state.json, writes action.json.\"\"\"\n",
    "\n",
    "    def __init__(self, cache_manager, interval=0.02, gc_interval=300):\n",
    "        super().__init__(daemon=True)\n",
    "        self.cm = cache_manager\n",
    "        self.interval = interval\n",
    "        self.gc_interval = gc_interval  # GC every N iterations\n",
    "        self.running = False\n",
    "        self._iteration = 0\n",
    "\n",
    "    def run(self):\n",
    "        self.running = True\n",
    "        print(f\"  üîÑ IOThread started (interval={self.interval*1000:.0f}ms)\")\n",
    "\n",
    "        while self.running:\n",
    "            try:\n",
    "                cache = self.cm.get_active()\n",
    "                if cache is None:\n",
    "                    time.sleep(self.interval)\n",
    "                    continue\n",
    "\n",
    "                # --- READ game_state.json ---\n",
    "                ctx, pal, til, dead, raw_pos = read_game_state()\n",
    "                cache.update_state(ctx, pal, til, dead, raw_pos)\n",
    "\n",
    "                # --- WRITE action.json ---\n",
    "                action = cache.get_pending_action()\n",
    "                if action is not None:\n",
    "                    write_action(action)\n",
    "\n",
    "                # --- PERIODIC GC ---\n",
    "                self._iteration += 1\n",
    "                if self._iteration % self.gc_interval == 0:\n",
    "                    gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  [IOThread ERROR] {e}\")\n",
    "\n",
    "            time.sleep(self.interval)\n",
    "\n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        print(\"  üîÑ IOThread stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Main Loop - Cache System + Taught Reference Blending\n",
    "# ============================================================================\n",
    "# CHANGES:\n",
    "# 1. Loads taught reference model separately (read-only)\n",
    "# 2. Loads AI's own model as primary\n",
    "# 3. CacheManager pre-indexes all maps\n",
    "# 4. IOThread handles background file I/O\n",
    "# 5. Blend stats in logging\n",
    "# 6. State version gating (one decision per new state)\n",
    "# ============================================================================\n",
    "\n",
    "brain = Brain()\n",
    "\n",
    "for b in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"move\"))\n",
    "for b in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"interact\"))\n",
    "\n",
    "# === FILE PATHS ===\n",
    "TAUGHT_MODEL_PATH = BASE_PATH / \"taught_model_checkpoint.json\"\n",
    "TAUGHT_EXPLORATION_PATH = BASE_PATH / \"taught_exploration_memory.json\"\n",
    "\n",
    "# === LOAD AI'S OWN MODEL (primary) ===\n",
    "if MODEL_CHECKPOINT_FILE.exists():\n",
    "    loaded_ts = brain.load_taught_model(MODEL_CHECKPOINT_FILE)\n",
    "    print(f\"ü§ñ AI MODEL: Loaded from timestep {loaded_ts}\")\n",
    "    print(f\"   Utilities: {[f'{a.action}:{a.utility:.3f}' for a in brain.actions()]}\")\n",
    "else:\n",
    "    print(\"ü§ñ AI MODEL: No existing model ‚Äî starting fresh\")\n",
    "\n",
    "# === LOAD TAUGHT REFERENCE (read-only, for stagnation blending) ===\n",
    "brain.load_taught_reference(TAUGHT_MODEL_PATH)\n",
    "\n",
    "# === MERGE TAUGHT EXPLORATION (additive) ===\n",
    "brain.merge_taught_exploration(TAUGHT_EXPLORATION_PATH)\n",
    "\n",
    "# === LOAD TAUGHT TRANSITIONS FOR MARKOV ===\n",
    "brain.load_taught_transitions(TAUGHT_TRANSITIONS_FILE)\n",
    "\n",
    "# === INITIALIZE CACHE SYSTEM ===\n",
    "cache_manager = CacheManager(brain)\n",
    "cache_manager.load_all()\n",
    "cache_manager.detect_and_set_initial_map()\n",
    "\n",
    "# === START I/O THREAD ===\n",
    "io_thread = IOThread(cache_manager, interval=0.02, gc_interval=300)\n",
    "io_thread.start()\n",
    "\n",
    "exploration_weight = 1.3\n",
    "forced_explore_prob = 0.18\n",
    "prev_context_state = None\n",
    "prev_raw_position = None\n",
    "last_processed_version = -1\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"AI CONTROL - v9.1 (Hybrid Markov + Curiosity + Cache + Blend)\")\n",
    "print(\"=\"*70)\n",
    "print(\"MODELS:\")\n",
    "print(f\"  - AI model: {MODEL_CHECKPOINT_FILE}\")\n",
    "print(f\"  - Taught reference: {TAUGHT_MODEL_PATH} ({'loaded' if brain.taught_reference['loaded'] else 'NOT FOUND'})\")\n",
    "if brain.taught_reference['loaded']:\n",
    "    taught_utils = ', '.join(f\"{k}:{v:.3f}\" for k, v in brain.taught_reference['utilities'].items())\n",
    "    print(f\"  - Taught utilities: {taught_utils}\")\n",
    "print(\"=\"*70)\n",
    "print(\"CACHE SYSTEM:\")\n",
    "print(f\"  - Maps cached: {len(cache_manager.caches)}\")\n",
    "print(f\"  - Active map: {cache_manager.active_map_id}\")\n",
    "active_taught = len(cache_manager.get_active_taught_frames())\n",
    "print(f\"  - Active map taught frames: {active_taught}\")\n",
    "print(f\"  - Total taught frames: {len(brain.taught_transitions)}\")\n",
    "print(f\"  - IOThread interval: {io_thread.interval*1000:.0f}ms\")\n",
    "print(\"=\"*70)\n",
    "print(\"BLEND SYSTEM:\")\n",
    "print(f\"  - Tier 1 (light 80/20):  pattern 3+ | pos stuck 8+ | repeat 12+\")\n",
    "print(f\"  - Tier 2 (medium 60/40): pattern 6+ | pos stuck 15+ | repeat 15+\")\n",
    "print(f\"  - Tier 3 (hard 40/60):   pattern 10+ | state stag 2x threshold\")\n",
    "print(f\"  - Cooldown: {brain.BLEND_COOLDOWN} steps between blends\")\n",
    "print(\"=\"*70)\n",
    "print(\"MARKOV SYSTEM:\")\n",
    "print(f\"  - Taught batches: {len(brain.taught_batches)}\")\n",
    "print(f\"  - Taught frames: {len(brain.taught_transitions)}\")\n",
    "print(f\"  - Density-adaptive thresholds: sparse=0.72 thin=0.65 medium=0.58 dense=0.50\")\n",
    "print(\"=\"*70)\n",
    "print(\"CURIOSITY SYSTEM:\")\n",
    "print(f\"  - Forced random exploration: {forced_explore_prob:.0%}\")\n",
    "print(f\"  - Unvisited tile bonus: {brain.UNVISITED_TILE_BONUS}x\")\n",
    "print(f\"  - Obstruction penalty: {brain.OBSTRUCTION_PENALTY}x\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # === WAIT FOR NEW STATE FROM IOTHREAD ===\n",
    "        active_cache = cache_manager.get_active()\n",
    "        current_version = active_cache.get_version()\n",
    "        \n",
    "        if current_version == last_processed_version:\n",
    "            time.sleep(0.005)\n",
    "            continue\n",
    "        \n",
    "        context_state, palette_state, tile_state, dead, raw_position = active_cache.get_state()\n",
    "        last_processed_version = current_version\n",
    "        \n",
    "        # Skip if zero state (IOThread hasn't started yet)\n",
    "        if np.sum(np.abs(context_state)) < 0.001:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "        \n",
    "        raw_x, raw_y = raw_position\n",
    "        in_battle = context_state[3]\n",
    "        current_map = int(context_state[2])\n",
    "        current_dir = int(context_state[5])\n",
    "        \n",
    "        # === MAP CHANGE DETECTION ===\n",
    "        if current_map != cache_manager.active_map_id:\n",
    "            cache_manager.switch_map(current_map)\n",
    "            active_cache = cache_manager.get_active()\n",
    "            print(f\"  üì¶ Cache switched to map {current_map} \"\n",
    "                  f\"(taught frames: {len(active_cache.get_taught_frames())})\")\n",
    "        \n",
    "        brain.update_position(raw_x, raw_y)\n",
    "\n",
    "        derived = compute_derived_features(context_state, prev_context_state)\n",
    "        learning_state = build_learning_state(derived, palette_state, tile_state, in_battle)\n",
    "        \n",
    "        brain.log_state(learning_state, context_state)\n",
    "        \n",
    "        # Action execution confirmation\n",
    "        brain.confirm_action_executed(context_state, prev_context_state)\n",
    "\n",
    "        if brain.should_send_new_action():\n",
    "            # Get current map's taught frames and density for adapted thresholds\n",
    "            taught_frames = cache_manager.get_active_taught_frames()\n",
    "            map_density = cache_manager.get_map_density()\n",
    "            \n",
    "            action = anticipatory_action(\n",
    "                brain, learning_state, context_state,\n",
    "                exploration_weight=exploration_weight,\n",
    "                raw_position=raw_position,\n",
    "                forced_explore_prob=forced_explore_prob,\n",
    "                taught_frames=taught_frames,\n",
    "                map_density=map_density\n",
    "            )\n",
    "\n",
    "            if action is not None:\n",
    "                # Write via cache (IOThread will flush to file)\n",
    "                active_cache.set_pending_action(action.action)\n",
    "                brain.last_action = action.action\n",
    "                brain.set_pending_action(action.action)\n",
    "                brain.update_menu_trap_tracking(context_state, action.action, raw_position=raw_position)\n",
    "            else:\n",
    "                active_cache.set_pending_action(\"NONE\")\n",
    "        else:\n",
    "            if brain.pending_action:\n",
    "                active_cache.set_pending_action(brain.pending_action)\n",
    "\n",
    "        # === LOGGING ===\n",
    "        if brain.timestep % 100 == 0:\n",
    "            memory = brain.get_current_map_memory(current_map)\n",
    "            visited_count = len(memory['visited_tiles'])\n",
    "            obs_count = len(memory['obstructions'])\n",
    "            interactables = len(memory['interactable_objects'])\n",
    "            coverage = brain.get_exploration_coverage(current_map)\n",
    "            transitions = memory.get('transitions', [])\n",
    "            tile_stats = brain.get_tile_interaction_stats(current_map)\n",
    "            \n",
    "            tile_needs_probing = brain.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "            probe_action, probe_dir = brain.get_best_probe_action(raw_x, raw_y, current_map, current_dir)\n",
    "            \n",
    "            dir_name = brain.DIRECTION_NAMES.get(current_dir, '?')\n",
    "            mode = brain.control_mode\n",
    "            is_both_mode = brain.should_use_both_mode()\n",
    "            mode_display = \"BOTH ‚ö°\" if is_both_mode else mode\n",
    "            \n",
    "            total_actions = brain.markov_action_count + brain.curiosity_action_count\n",
    "            markov_ratio = brain.markov_action_count / max(1, total_actions)\n",
    "            \n",
    "            density = cache_manager.get_map_density()\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(f\"Step {brain.timestep} | Map {current_map} | Pos ({raw_x}, {raw_y}) facing {dir_name}\")\n",
    "            print(f\"  Mode: {mode_display} | Battle: {int(in_battle)} | Stagnation: {brain.state_stagnation_count}\")\n",
    "            \n",
    "            print(f\"\\n  üß† DECISION MODE:\")\n",
    "            print(f\"     Markov: {brain.markov_action_count} ({markov_ratio:.1%}) | Curiosity: {brain.curiosity_action_count} ({1-markov_ratio:.1%})\")\n",
    "            print(f\"     Last Markov score: {brain.last_markov_score:.3f} (threshold adapts to density)\")\n",
    "            print(f\"     Map density: {density['tier']} ({density['taught_frames']} frames, {density['visited']} tiles, {density['coverage']:.0%} coverage)\")\n",
    "            if brain.last_markov_action:\n",
    "                print(f\"     Last Markov suggestion: {brain.last_markov_action}\")\n",
    "            \n",
    "            # Blend status\n",
    "            if brain.taught_reference['loaded']:\n",
    "                blend_status = f\"Tier {brain.blend_tier}\" if brain.blend_tier > 0 else \"Inactive\"\n",
    "                print(f\"\\n  üîÄ BLEND: {blend_status} | Total blends: {brain.blend_count}\")\n",
    "                if brain.blend_tier > 0:\n",
    "                    ai_w, taught_w = brain.BLEND_RATIOS[brain.blend_tier]\n",
    "                    print(f\"     Current ratio: {ai_w:.0%} AI / {taught_w:.0%} taught\")\n",
    "            \n",
    "            print(f\"\\n  üìä EXPLORATION:\")\n",
    "            print(f\"     Visited: {visited_count} | Obstructions: {obs_count} | Coverage: {coverage:.0%}\")\n",
    "            print(f\"     Interactables found: {interactables}\")\n",
    "            \n",
    "            print(f\"\\n  üéØ TILE PROBING:\")\n",
    "            print(f\"     Tiles probed: {tile_stats['probed']} | Exhausted: {tile_stats['exhausted']} | With success: {tile_stats['with_success']}\")\n",
    "            \n",
    "            if tile_needs_probing:\n",
    "                if probe_action == 'A':\n",
    "                    print(f\"     Current tile: READY TO PROBE (facing untried direction)\")\n",
    "                elif probe_action:\n",
    "                    print(f\"     Current tile: NEED TO TURN {probe_action} first\")\n",
    "                else:\n",
    "                    print(f\"     Current tile: NEEDS PROBING (checking directions)\")\n",
    "            else:\n",
    "                print(f\"     Current tile: EXHAUSTED or fully probed\")\n",
    "            \n",
    "            tile_state_data = brain.get_tile_interaction_state(raw_x, raw_y, current_map)\n",
    "            success_info = []\n",
    "            for d in range(4):\n",
    "                attempts = tile_state_data['direction_attempts'].get(d, 0)\n",
    "                successes = tile_state_data['direction_successes'].get(d, 0)\n",
    "                if attempts > 0:\n",
    "                    success_info.append(f\"{brain.DIRECTION_NAMES.get(d, '?')}:{successes}/{attempts}\")\n",
    "            if success_info:\n",
    "                print(f\"     Direction results: {', '.join(success_info)}\")\n",
    "            \n",
    "            if transitions:\n",
    "                print(f\"\\n  üö™ TRANSITIONS: {len(transitions)} known\")\n",
    "                for t in transitions[:3]:\n",
    "                    pos = tuple(t['position']) if isinstance(t['position'], list) else t['position']\n",
    "                    banned = \"üö´\" if brain.is_transition_banned(current_map, pos, t['direction']) else \"\"\n",
    "                    print(f\"     ({pos[0]},{pos[1]}) ‚Üí Map {t['destination_map']} (used {t['use_count']}x) {banned}\")\n",
    "            \n",
    "            map_debt = brain.map_novelty_debt.get(current_map, 0.0)\n",
    "            temp_debt = brain.get_temp_debt(current_map)\n",
    "            if map_debt > 0.1 or temp_debt > 0.1:\n",
    "                print(f\"\\n  üí≥ DEBT: map={map_debt:.2f}/{brain.MAX_MAP_DEBT}, temp={temp_debt:.2f}\")\n",
    "            \n",
    "            if brain.menu_trap_b_boost > 1.0:\n",
    "                print(f\"\\n  üîí MENU TRAP: B boost {brain.menu_trap_b_boost:.2f}x ({brain.menu_trap_frames} frames)\")\n",
    "            \n",
    "            if is_both_mode:\n",
    "                print(f\"\\n  ‚ö° BOTH MODE ACTIVE: stagnation={brain.state_stagnation_count}, swaps={brain.unproductive_swap_count}\")\n",
    "            \n",
    "            if brain.pending_action:\n",
    "                print(f\"\\n  ‚è≥ Pending: {brain.pending_action} ({brain.pending_action_frames}/{brain.ACTION_CONFIRM_FRAMES})\")\n",
    "            \n",
    "            action_utils = sorted([(a.action, a.utility) for a in brain.actions()], key=lambda x: x[1], reverse=True)\n",
    "            print(f\"\\n  ‚ö° Utilities: {' '.join([f'{k}:{v:.2f}' for k,v in action_utils])}\")\n",
    "            \n",
    "            if brain.state_stagnation_count > 10:\n",
    "                print(f\"\\n  ‚ö†Ô∏è STAGNATION WARNING: {brain.state_stagnation_count}/{brain.STATE_STAGNATION_THRESHOLD}\")\n",
    "            if brain.detected_pattern:\n",
    "                pattern_str = '-'.join(str(a) for a in brain.detected_pattern)\n",
    "                print(f\"  üîÑ PATTERN DETECTED ({len(brain.detected_pattern)}): {pattern_str} x{brain.pattern_repeat_count}\")\n",
    "\n",
    "        # === MILESTONES ===\n",
    "        if brain.timestep % 500 == 0 and brain.timestep > 0:\n",
    "            total_visited = sum(len(m['visited_tiles']) for m in brain.exploration_memory.values())\n",
    "            total_obs = sum(len(m['obstructions']) for m in brain.exploration_memory.values())\n",
    "            total_interactables = sum(len(m['interactable_objects']) for m in brain.exploration_memory.values())\n",
    "            total_transitions = sum(len(m.get('transitions', [])) for m in brain.exploration_memory.values())\n",
    "            total_probed = sum(len(m.get('tile_interactions', {})) for m in brain.exploration_memory.values())\n",
    "            total_exhausted = sum(\n",
    "                sum(1 for t in m.get('tile_interactions', {}).values() if t.get('exhausted', False))\n",
    "                for m in brain.exploration_memory.values()\n",
    "            )\n",
    "            \n",
    "            total_actions = brain.markov_action_count + brain.curiosity_action_count\n",
    "            markov_ratio = brain.markov_action_count / max(1, total_actions)\n",
    "            \n",
    "            print(f\"\\n{'#'*70}\")\n",
    "            print(f\"# MILESTONE {brain.timestep}\")\n",
    "            print(f\"# Maps explored: {len(brain.exploration_memory)}\")\n",
    "            print(f\"# Tiles visited: {total_visited} | Obstructions: {total_obs}\")\n",
    "            print(f\"# Interactables: {total_interactables} | Transitions: {total_transitions}\")\n",
    "            print(f\"# Tiles probed: {total_probed} | Exhausted: {total_exhausted}\")\n",
    "            print(f\"#\")\n",
    "            print(f\"# CACHE SYSTEM:\")\n",
    "            print(f\"#   Maps cached: {len(cache_manager.caches)}\")\n",
    "            print(f\"#   Active map: {cache_manager.active_map_id} ({len(cache_manager.get_active_taught_frames())} taught frames)\")\n",
    "            print(f\"#\")\n",
    "            print(f\"# HYBRID DECISION STATS:\")\n",
    "            print(f\"#   Markov (imitation): {brain.markov_action_count} ({markov_ratio:.1%})\")\n",
    "            print(f\"#   Curiosity (explore): {brain.curiosity_action_count} ({1-markov_ratio:.1%})\")\n",
    "            print(f\"#   Taught transitions: {len(brain.taught_transitions)}\")\n",
    "            print(f\"#\")\n",
    "            print(f\"# BLEND STATS:\")\n",
    "            print(f\"#   Total blends: {brain.blend_count}\")\n",
    "            print(f\"#   Current tier: {brain.blend_tier}\")\n",
    "            print(f\"#   Taught reference: {'loaded' if brain.taught_reference['loaded'] else 'not loaded'}\")\n",
    "            print(f\"{'#'*70}\")\n",
    "\n",
    "            brain.save_model_checkpoint(BASE_PATH / \"model_checkpoint.json\")\n",
    "            cache_manager.save_exploration_memory()\n",
    "            print(f\"# Model + exploration saved\")\n",
    "\n",
    "        # === WAIT FOR NEXT STATE (new IOThread read) ===\n",
    "        for _ in range(10):  # Wait up to ~50ms for a new version\n",
    "            time.sleep(0.005)\n",
    "            if active_cache.get_version() > last_processed_version:\n",
    "                break\n",
    "\n",
    "        # === LEARN ===\n",
    "        next_ctx, next_pal, next_til, dead, next_raw_pos = active_cache.get_state()\n",
    "        last_processed_version = active_cache.get_version()\n",
    "        \n",
    "        next_in_battle = next_ctx[3]\n",
    "        next_derived = compute_derived_features(next_ctx, context_state)\n",
    "        next_learning_state = build_learning_state(next_derived, next_pal, next_til, next_in_battle)\n",
    "\n",
    "        brain.learn(learning_state, next_learning_state, context_state, next_ctx, dead=dead, \n",
    "                    raw_position=raw_position, next_raw_position=next_raw_pos)\n",
    "\n",
    "        prev_context_state = context_state.copy()\n",
    "        prev_raw_position = raw_position\n",
    "        brain.timestep += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nüõë Stopping AI...\")\n",
    "    io_thread.stop()\n",
    "    io_thread.join(timeout=2)\n",
    "    cache_manager.save_exploration_memory()\n",
    "    brain.save_model_checkpoint(BASE_PATH / \"model_checkpoint.json\")\n",
    "    print(\"‚úÖ Saved and stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
