{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "677ae718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: State Management & Utilities (FIXED - Consistent State Size)\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "BASE_PATH = Path(\"C:/Users/natmaw/Documents/Boston Stuff/CS 5100 Foundations of AI/PokeAI\")\n",
    "ACTION_FILE = BASE_PATH / \"action.json\"\n",
    "STATE_FILE = BASE_PATH / \"game_state.json\"\n",
    "INPUT_FILE = BASE_PATH / \"input_cache.txt\"\n",
    "MODEL_FILE = BASE_PATH / \"model_checkpoint.json\"\n",
    "\n",
    "EXPECTED_STATE_DIM = 6\n",
    "PALETTE_DIM = 768\n",
    "TILE_DIM = 600\n",
    "\n",
    "# FIXED: Consistent state size for all modes\n",
    "# 8 (derived) + 600 (tiles) + 768 (palette) = 1376\n",
    "LEARNING_STATE_DIM = 8 + TILE_DIM + PALETTE_DIM  # 1376\n",
    "\n",
    "# Action code mapping\n",
    "ACTION_MAP = {\n",
    "    'U': 'UP', 'D': 'DOWN', 'L': 'LEFT', 'R': 'RIGHT',\n",
    "    'A': 'A', 'B': 'B', 'S': 'Start', 'E': 'Select'\n",
    "}\n",
    "\n",
    "last_state_mod_time = 0\n",
    "last_input_mod_time = 0\n",
    "\n",
    "def normalize_game_state(raw_state):\n",
    "    \"\"\"Normalize context state for learning.\"\"\"\n",
    "    if len(raw_state) < 6:\n",
    "        raw_state = list(raw_state) + [0] * (6 - len(raw_state))\n",
    "    \n",
    "    normalized = np.array(raw_state, dtype=float)\n",
    "    normalized[0] = raw_state[0] / 255.0\n",
    "    normalized[1] = raw_state[1] / 255.0\n",
    "    normalized[2] = np.clip(raw_state[2], 0, 255)\n",
    "    normalized[3] = 1.0 if raw_state[3] > 0 else 0.0\n",
    "    normalized[4] = 1.0 if raw_state[4] > 0 else 0.0\n",
    "    normalized[5] = int(raw_state[5]) % 4\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def compute_derived_features(current, prev):\n",
    "    \"\"\"Extract temporal features (8D)\"\"\"\n",
    "    if prev is None:\n",
    "        return np.zeros(8)\n",
    "    \n",
    "    vel_x = current[0] - prev[0]\n",
    "    vel_y = current[1] - prev[1]\n",
    "    map_changed = 1.0 if abs(current[2] - prev[2]) > 0.5 else 0.0\n",
    "    battle_started = 1.0 if current[3] > prev[3] else 0.0\n",
    "    battle_ended = 1.0 if current[3] < prev[3] else 0.0\n",
    "    menu_opened = 1.0 if current[4] > prev[4] else 0.0\n",
    "    menu_closed = 1.0 if current[4] < prev[4] else 0.0\n",
    "    direction_changed = 1.0 if current[5] != prev[5] else 0.0\n",
    "    \n",
    "    return np.array([vel_x, vel_y, map_changed, battle_started, battle_ended,\n",
    "                     menu_opened, menu_closed, direction_changed])\n",
    "\n",
    "def build_learning_state(derived, palette, tiles, in_battle):\n",
    "    \"\"\"\n",
    "    Build learning state vector - ALWAYS SAME SIZE (1376).\n",
    "    \n",
    "    Structure: [derived(8)] + [tiles(600)] + [palette(768)]\n",
    "    \n",
    "    In battle: tiles are zeroed but still included for consistent shape.\n",
    "    \"\"\"\n",
    "    # Ensure correct sizes\n",
    "    if len(derived) != 8:\n",
    "        derived = np.zeros(8)\n",
    "    if len(tiles) != TILE_DIM:\n",
    "        tiles = np.zeros(TILE_DIM)\n",
    "    if len(palette) != PALETTE_DIM:\n",
    "        palette = np.zeros(PALETTE_DIM)\n",
    "    \n",
    "    # In battle, zero out tiles (they're just UI) but keep the slots\n",
    "    if in_battle > 0.5:\n",
    "        tiles = np.zeros(TILE_DIM)\n",
    "    \n",
    "    # Always concatenate all three for consistent size\n",
    "    state = np.concatenate([derived, tiles, palette])\n",
    "    \n",
    "    # Add tiny noise\n",
    "    noise = np.random.randn(len(state)) * 0.0001\n",
    "    return state + noise\n",
    "\n",
    "def read_input_cache():\n",
    "    \"\"\"Read and parse the input cache file.\"\"\"\n",
    "    global last_input_mod_time\n",
    "    \n",
    "    if not INPUT_FILE.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        current_mod_time = INPUT_FILE.stat().st_mtime\n",
    "        if current_mod_time == last_input_mod_time:\n",
    "            return []\n",
    "        last_input_mod_time = current_mod_time\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    inputs = []\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 7:\n",
    "                    action_code = parts[0]\n",
    "                    inputs.append({\n",
    "                        'action': ACTION_MAP.get(action_code, action_code),\n",
    "                        'x': int(parts[1]),\n",
    "                        'y': int(parts[2]),\n",
    "                        'map': int(parts[3]),\n",
    "                        'in_battle': int(parts[4]),\n",
    "                        'menu_flag': int(parts[5]),\n",
    "                        'direction': int(parts[6])\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error reading input cache: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def read_game_state_minimal():\n",
    "    \"\"\"Read current game state (minimal version).\"\"\"\n",
    "    global last_state_mod_time\n",
    "    \n",
    "    if not STATE_FILE.exists():\n",
    "        return np.zeros(EXPECTED_STATE_DIM), (0, 0), 0\n",
    "    \n",
    "    try:\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        \n",
    "        raw = data.get('s', [0, 0, 0, 0, 0, 0])\n",
    "        input_count = data.get('ic', 0)\n",
    "        \n",
    "        context_state = normalize_game_state(raw)\n",
    "        raw_position = (int(raw[0]), int(raw[1]))\n",
    "        \n",
    "        return context_state, raw_position, input_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return np.zeros(EXPECTED_STATE_DIM), (0, 0), 0\n",
    "\n",
    "def read_game_state_full():\n",
    "    \"\"\"Read full game state including visuals.\"\"\"\n",
    "    if not STATE_FILE.exists():\n",
    "        return (np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), \n",
    "                np.zeros(TILE_DIM), (0, 0))\n",
    "    \n",
    "    try:\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        \n",
    "        raw = data.get('s', [0, 0, 0, 0, 0, 0])\n",
    "        palette_raw = data.get('p', [])\n",
    "        tiles_raw = data.get('t', [])\n",
    "        \n",
    "        context_state = normalize_game_state(raw)\n",
    "        raw_position = (int(raw[0]), int(raw[1]))\n",
    "        \n",
    "        # Process palette\n",
    "        if palette_raw:\n",
    "            palette_state = np.array(palette_raw, dtype=float)\n",
    "        else:\n",
    "            palette_state = np.zeros(PALETTE_DIM)\n",
    "        \n",
    "        # Process tiles\n",
    "        if tiles_raw:\n",
    "            tile_state = np.array(tiles_raw, dtype=float)\n",
    "        else:\n",
    "            tile_state = np.zeros(TILE_DIM)\n",
    "        \n",
    "        # Ensure correct dimensions\n",
    "        if len(palette_state) < PALETTE_DIM:\n",
    "            palette_state = np.pad(palette_state, (0, PALETTE_DIM - len(palette_state)))\n",
    "        elif len(palette_state) > PALETTE_DIM:\n",
    "            palette_state = palette_state[:PALETTE_DIM]\n",
    "            \n",
    "        if len(tile_state) < TILE_DIM:\n",
    "            tile_state = np.pad(tile_state, (0, TILE_DIM - len(tile_state)))\n",
    "        elif len(tile_state) > TILE_DIM:\n",
    "            tile_state = tile_state[:TILE_DIM]\n",
    "        \n",
    "        return context_state, palette_state, tile_state, raw_position\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error reading full state: {e}\")\n",
    "        return (np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), \n",
    "                np.zeros(TILE_DIM), (0, 0))\n",
    "\n",
    "def process_cached_input(inp):\n",
    "    \"\"\"Convert a cached input dict to normalized state + action.\"\"\"\n",
    "    raw_state = [\n",
    "        inp.get('x', 0),\n",
    "        inp.get('y', 0),\n",
    "        inp.get('map', 0),\n",
    "        inp.get('in_battle', 0),\n",
    "        inp.get('menu_flag', 0),\n",
    "        inp.get('direction', 0)\n",
    "    ]\n",
    "    context_state = normalize_game_state(raw_state)\n",
    "    raw_position = (inp.get('x', 0), inp.get('y', 0))\n",
    "    action = inp.get('action', None)\n",
    "    \n",
    "    return context_state, raw_position, action\n",
    "\n",
    "# Legacy compatibility\n",
    "def read_game_state(max_retries=3):\n",
    "    \"\"\"Legacy function for compatibility.\"\"\"\n",
    "    context, palette, tiles, raw_pos = read_game_state_full()\n",
    "    return context, palette, tiles, False, raw_pos, None\n",
    "\n",
    "def write_action(action_name):\n",
    "    if action_name:\n",
    "        action_name = action_name.upper()\n",
    "    try:\n",
    "        with open(ACTION_FILE, \"w\") as f:\n",
    "            json.dump({\"action\": action_name}, f)\n",
    "            f.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write action: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9dd8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Perceptron Classes (FIXED - Shape Safety)\n",
    "# ============================================================================\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, kind, action=None, group=None, entity_type=None):\n",
    "        self.kind = kind\n",
    "        self.action = action\n",
    "        self.group = group\n",
    "        self.entity_type = entity_type\n",
    "        \n",
    "        self.utility = 1.0\n",
    "        self.weights = None\n",
    "        \n",
    "        self.eligibility_fast = 0.0\n",
    "        self.eligibility_slow = 0.0\n",
    "        \n",
    "        self.familiarity = 0.0\n",
    "        self.activation_history = deque(maxlen=10)\n",
    "        \n",
    "        self.learning_rate = 0.01\n",
    "        self.prediction_errors = deque(maxlen=50)\n",
    "\n",
    "    def ensure_weights(self, dim):\n",
    "        \"\"\"Initialize or resize weights to match dimension.\"\"\"\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(dim) * 0.001\n",
    "        elif len(self.weights) != dim:\n",
    "            # FIXED: Resize weights if dimension changed\n",
    "            old_weights = self.weights\n",
    "            self.weights = np.random.randn(dim) * 0.001\n",
    "            # Copy over what we can from old weights\n",
    "            min_len = min(len(old_weights), dim)\n",
    "            self.weights[:min_len] = old_weights[:min_len]\n",
    "\n",
    "    def predict(self, state):\n",
    "        self.ensure_weights(len(state))\n",
    "        raw_activation = np.dot(self.weights, state)\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            novelty_factor = 1.0 / (1.0 + np.sqrt(self.familiarity * 0.5))\n",
    "            decayed_activation = raw_activation * novelty_factor\n",
    "            self.activation_history.append(abs(raw_activation))\n",
    "            return decayed_activation\n",
    "        else:\n",
    "            return raw_activation\n",
    "\n",
    "    def adapt_learning_rate(self):\n",
    "        if len(self.prediction_errors) >= 50:\n",
    "            avg_error = np.mean(self.prediction_errors)\n",
    "            \n",
    "            if avg_error < 0.1:\n",
    "                self.learning_rate = max(0.001, self.learning_rate * 0.99)\n",
    "            elif avg_error > 0.5:\n",
    "                self.learning_rate = min(0.05, self.learning_rate * 1.01)\n",
    "\n",
    "    def update(self, state, error, gamma_fast=0.5, gamma_slow=0.95, stagnation=0.0):\n",
    "        \"\"\"Update weights - FIXED to handle shape mismatches.\"\"\"\n",
    "        # FIXED: Always ensure weights match state size\n",
    "        self.ensure_weights(len(state))\n",
    "        \n",
    "        # Double-check shapes match (safety)\n",
    "        if len(self.weights) != len(state):\n",
    "            print(f\"[WARN] Shape mismatch: weights={len(self.weights)}, state={len(state)}. Resizing.\")\n",
    "            self.weights = np.random.randn(len(state)) * 0.001\n",
    "        \n",
    "        self.eligibility_fast = gamma_fast * self.eligibility_fast + 1.0\n",
    "        self.eligibility_slow = gamma_slow * self.eligibility_slow + 1.0\n",
    "        \n",
    "        self.adapt_learning_rate()\n",
    "        \n",
    "        fast_update = 0.7 * self.learning_rate * error * state * self.eligibility_fast\n",
    "        slow_update = 0.3 * self.learning_rate * error * state * self.eligibility_slow\n",
    "        self.weights += fast_update + slow_update\n",
    "\n",
    "        if self.kind == \"action\":\n",
    "            if error > 0.01:\n",
    "                if stagnation > 0.5:\n",
    "                    self.utility *= 0.97\n",
    "                elif error > 0.2:\n",
    "                    self.utility = min(self.utility * 1.02, 2.0)\n",
    "                else:\n",
    "                    self.utility *= 0.995\n",
    "            \n",
    "            if self.group == \"move\":\n",
    "                self.utility = np.clip(self.utility, 0.1, 2.0)\n",
    "            else:\n",
    "                self.utility = np.clip(self.utility, 0.01, 2.0)\n",
    "        \n",
    "        if self.kind == \"entity\" and len(self.activation_history) > 0:\n",
    "            recent_avg = np.mean(self.activation_history)\n",
    "            if recent_avg > 0.1:\n",
    "                self.familiarity += 0.03\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            prediction = self.predict(state)\n",
    "            self.prediction_errors.append(abs(prediction - error))\n",
    "\n",
    "\n",
    "class ControlSwapPerceptron(Perceptron):\n",
    "    def __init__(self):\n",
    "        super().__init__(kind=\"control_swap\")\n",
    "        self.swap_history = deque(maxlen=100)\n",
    "        self.confidence = 0.0\n",
    "        \n",
    "    def should_swap(self, state, movement_stagnation):\n",
    "        if self.weights is None:\n",
    "            return False, 0.0\n",
    "        \n",
    "        self.ensure_weights(len(state))\n",
    "        swap_score = np.dot(self.weights, state)\n",
    "        stagnation_factor = np.tanh(movement_stagnation / 5.0)\n",
    "        combined_score = swap_score * 0.7 + stagnation_factor * 0.3\n",
    "        \n",
    "        return combined_score > 0.5, abs(combined_score)\n",
    "    \n",
    "    def record_swap_outcome(self, state, swapped, novelty_gained):\n",
    "        self.swap_history.append((swapped, novelty_gained))\n",
    "        \n",
    "        if len(self.swap_history) >= 20:\n",
    "            recent = list(self.swap_history)[-20:]\n",
    "            successful = sum(1 for swap, nov in recent if swap and nov > 0.2)\n",
    "            self.confidence = successful / 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "980759f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Brain Class - COMPLETE MEMORY SAFE VERSION\n",
    "# ============================================================================\n",
    "\n",
    "import gc\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.perceptrons = []\n",
    "        \n",
    "        # REDUCED DEQUE SIZES\n",
    "        self.prev_learning_states = deque(maxlen=10)\n",
    "        self.prev_context_states = deque(maxlen=5)\n",
    "        self.last_positions = deque(maxlen=15)\n",
    "        self.action_history = deque(maxlen=30)\n",
    "        \n",
    "        self.control_mode = \"move\"\n",
    "        self.timestep = 0\n",
    "        self.last_action = None\n",
    "        self.last_direction = 0\n",
    "        \n",
    "        self.MOVE_UTILITY_FLOOR = 0.05\n",
    "        self.INTERACT_UTILITY_FLOOR = 0.15\n",
    "        \n",
    "        # === EXPLORATION MEMORY ===\n",
    "        self.EXPLORATION_MEMORY_FILE = BASE_PATH / \"exploration_memory.json\"\n",
    "        self.exploration_memory = {}\n",
    "        self.current_map_id = None\n",
    "        self.SAVE_INTERVAL = 100\n",
    "        self.MAX_MAPS_IN_MEMORY = 20\n",
    "        \n",
    "        # Direction mapping\n",
    "        self.DIRECTION_NAMES = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        self.DIRECTION_TO_INT = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "        self.INT_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        \n",
    "        self.DIRECTION_DELTAS_INT = {0: (0, 1), 1: (0, -1), 2: (-1, 0), 3: (1, 0)}\n",
    "        self.ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "        self.DELTA_TO_DIRECTION = {(0, 1): 0, (0, -1): 1, (-1, 0): 2, (1, 0): 3}\n",
    "        \n",
    "        self.load_exploration_memory()\n",
    "        \n",
    "        # === ACTION EXECUTION ===\n",
    "        self.pending_action = None\n",
    "        self.pending_action_frames = 0\n",
    "        self.ACTION_CONFIRM_FRAMES = 3\n",
    "        self.last_confirmed_action = None\n",
    "        \n",
    "        # === TILE INTERACTION ===\n",
    "        self.INTERACTION_VERIFY_FRAMES = 8\n",
    "        self.MIN_SUCCESS_RATE_THRESHOLD = 0.1\n",
    "        self.pending_interaction_verify = None\n",
    "        self.interaction_verify_countdown = 0\n",
    "        \n",
    "        # === MENU TRAP ===\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.B_BOOST_INCREMENT = 0.15\n",
    "        self.B_BOOST_MAX = 3.0\n",
    "        self.MENU_TRAP_THRESHOLD = 5\n",
    "        self.original_b_utility = None\n",
    "        \n",
    "        # === MODE SWAPPING ===\n",
    "        self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD = 15\n",
    "        self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD = 25\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.THRESHOLD_INCREMENT = 15\n",
    "        self.MAX_THRESHOLD = 150\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.swap_chain_count = 0\n",
    "        self.position_at_mode_swap = None\n",
    "        self.last_map_id = None\n",
    "        self.last_battle_state = None\n",
    "        \n",
    "        # === STAGNATION ===\n",
    "        self.STATE_STAGNATION_THRESHOLD = 20\n",
    "        self.state_stagnation_count = 0\n",
    "        self.last_context_state_hash = None\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.STAGNATION_INITIATOR_PENALTY = 0.7\n",
    "        self.unproductive_swap_count = 0\n",
    "        self.UNPRODUCTIVE_SWAP_THRESHOLD = 3\n",
    "        \n",
    "        # === BOTH MODE ===\n",
    "        self.BOTH_MODE_STAGNATION_THRESHOLD = 35\n",
    "        self.BOTH_MODE_SWAP_THRESHOLD = 5\n",
    "        self.last_direction_for_progress = None\n",
    "        self.direction_change_counts_as_progress = True\n",
    "        \n",
    "        # === NOVELTY ===\n",
    "        self.UNVISITED_TILE_BONUS = 1.5\n",
    "        self.OBSTRUCTION_PENALTY = 0.25\n",
    "        \n",
    "        # === TRANSITIONS ===\n",
    "        self.TRANSITION_ATTRACTION_WEIGHT = 0.6\n",
    "        self.TEMP_DEBT_ACCUMULATION = 0.5\n",
    "        self.TEMP_DEBT_DECAY = 0.02\n",
    "        self.TEMP_DEBT_MAX = 15.0\n",
    "        \n",
    "        # === DEBT ===\n",
    "        self.MAX_MAP_DEBT = 10.0\n",
    "        self.MAX_LOCATION_DEBT = 5.0\n",
    "        self.DEBT_DECAY_RATE = 0.005\n",
    "        \n",
    "        # === BANS ===\n",
    "        self.transition_bans = {}\n",
    "        self.BAN_VICINITY_RADIUS = 3\n",
    "        self.BAN_COVERAGE_LIFT_THRESHOLD = 0.6\n",
    "        self.BAN_TIMEOUT_STEPS = 300\n",
    "        \n",
    "        # Multi-scale memory - LIMITED\n",
    "        self.visited_maps = {}\n",
    "        self.map_novelty_debt = {}\n",
    "        self.location_memory = {}\n",
    "        self.location_novelty = {}\n",
    "        self.action_execution_count = {}\n",
    "        self.MAX_LOCATIONS = 500\n",
    "        \n",
    "        self.swap_perceptron = ControlSwapPerceptron()\n",
    "        \n",
    "        # REDUCED ERROR HISTORY\n",
    "        self.error_history = deque(maxlen=100)\n",
    "        self.numeric_error_history = deque(maxlen=100)\n",
    "        self.visual_error_history = deque(maxlen=100)\n",
    "        \n",
    "        self._entity_norms_cache = {}\n",
    "        self._cache_valid = False\n",
    "        self.innate_entities_spawned = False\n",
    "        \n",
    "        # === REPETITION ===\n",
    "        self.consecutive_action_count = 0\n",
    "        self.current_repeated_action = None\n",
    "        self.LEARNING_SLOWDOWN_START = 3\n",
    "        self.LEARNING_SLOWDOWN_MAX = 10\n",
    "        self.PENALTY_THRESHOLD = 12\n",
    "        self.HARD_RESET_THRESHOLD = 18\n",
    "        \n",
    "        # === PATTERN ===\n",
    "        self.PATTERN_CHECK_WINDOW = 30\n",
    "        self.PATTERN_MIN_REPEATS = 3\n",
    "        self.PATTERN_MAX_LENGTH = 10\n",
    "        self.detected_pattern = None\n",
    "        self.pattern_repeat_count = 0\n",
    "\n",
    "        # === PROBE CACHE ===\n",
    "        self._cached_probe_action = None\n",
    "        self._cached_probe_dir = None\n",
    "        self._probe_cache_position = None\n",
    "        \n",
    "        # === TEACHING MODE ===\n",
    "        self.teaching_mode = True\n",
    "        self.demonstration_count = 0\n",
    "        self.context_action_stats = {}\n",
    "        self.MAX_CONTEXT_STATS = 50\n",
    "\n",
    "    # =========================================================================\n",
    "    # CORE METHODS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def add(self, p):\n",
    "        self.perceptrons.append(p)\n",
    "        self._cache_valid = False\n",
    "\n",
    "    def actions(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"action\"]\n",
    "\n",
    "    def entities(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"entity\"]\n",
    "\n",
    "    def get_location_key(self, x, y, map_id, bin_size=5):\n",
    "        return (int(map_id), int(x // bin_size) * bin_size, int(y // bin_size) * bin_size)\n",
    "\n",
    "    def is_near_map_edge(self, x, y):\n",
    "        return x < 10 or x > 245 or y < 10 or y > 245\n",
    "\n",
    "    def record_action_execution(self, action_name):\n",
    "        if action_name:\n",
    "            self.action_execution_count[action_name] = self.action_execution_count.get(action_name, 0) + 1\n",
    "\n",
    "    def get_position_stagnation(self):\n",
    "        if len(self.last_positions) < 2:\n",
    "            return 0\n",
    "        current_pos = self.last_positions[-1]\n",
    "        return sum(1 for pos in reversed(list(self.last_positions)[:-1]) if pos == current_pos)\n",
    "\n",
    "    def get_group_weight(self, group):\n",
    "        return sum(a.utility for a in self.actions() if a.group == group)\n",
    "\n",
    "    def log_state(self, learning_state, context_state):\n",
    "        self.prev_learning_states.append(learning_state)\n",
    "        self.prev_context_states.append(context_state)\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        self.last_positions.append((int(x), int(y)))\n",
    "\n",
    "    # =========================================================================\n",
    "    # MEMORY MANAGEMENT\n",
    "    # =========================================================================\n",
    "    \n",
    "    def cleanup_memory(self):\n",
    "        if len(self.location_memory) > self.MAX_LOCATIONS:\n",
    "            sorted_locs = sorted(self.location_memory.items(), key=lambda x: x[1], reverse=True)\n",
    "            self.location_memory = dict(sorted_locs[:self.MAX_LOCATIONS // 2])\n",
    "            self.location_novelty = {k: v for k, v in self.location_novelty.items() if k in self.location_memory}\n",
    "        \n",
    "        if len(self.context_action_stats) > self.MAX_CONTEXT_STATS:\n",
    "            keys = list(self.context_action_stats.keys())\n",
    "            for k in keys[:-self.MAX_CONTEXT_STATS // 2]:\n",
    "                del self.context_action_stats[k]\n",
    "        \n",
    "        if len(self.exploration_memory) > self.MAX_MAPS_IN_MEMORY:\n",
    "            self.save_exploration_memory()\n",
    "            sorted_maps = sorted(self.exploration_memory.items(),\n",
    "                                key=lambda x: x[1].get('last_visited_timestep', 0), reverse=True)\n",
    "            self.exploration_memory = dict(sorted_maps[:self.MAX_MAPS_IN_MEMORY // 2])\n",
    "        \n",
    "        self._entity_norms_cache.clear()\n",
    "        self._cache_valid = False\n",
    "        gc.collect()\n",
    "    \n",
    "    def get_memory_stats(self):\n",
    "        stats = {\n",
    "            'exploration_maps': len(self.exploration_memory),\n",
    "            'location_memory': len(self.location_memory),\n",
    "            'context_stats': len(self.context_action_stats),\n",
    "            'error_history': len(self.error_history),\n",
    "            'perceptrons': len(self.perceptrons),\n",
    "        }\n",
    "        total_tiles = sum(len(m.get('visited_tiles', set())) for m in self.exploration_memory.values())\n",
    "        stats['total_tiles'] = total_tiles\n",
    "        return stats\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION MEMORY\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_exploration_memory(self):\n",
    "        try:\n",
    "            if self.EXPLORATION_MEMORY_FILE.exists():\n",
    "                with open(self.EXPLORATION_MEMORY_FILE, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.exploration_memory = {}\n",
    "                    items = list(data.items())[-self.MAX_MAPS_IN_MEMORY:]\n",
    "                    for map_key, map_data in items:\n",
    "                        map_id = int(map_key.replace('map_', ''))\n",
    "                        self.exploration_memory[map_id] = self._deserialize_map_memory(map_data)\n",
    "                print(f\"  Loaded exploration memory: {len(self.exploration_memory)} maps\")\n",
    "            else:\n",
    "                self.exploration_memory = {}\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading exploration memory: {e}\")\n",
    "            self.exploration_memory = {}\n",
    "\n",
    "    def _deserialize_map_memory(self, map_data):\n",
    "        tile_interactions = {}\n",
    "        ti_data = map_data.get('tile_interactions', {})\n",
    "        ti_items = list(ti_data.items())[-100:]\n",
    "        for tile_key, tile_data in ti_items:\n",
    "            tile_interactions[tile_key] = {\n",
    "                'directions_tried': set(tile_data.get('directions_tried', [])),\n",
    "                'direction_attempts': {int(k): v for k, v in tile_data.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {int(k): v for k, v in tile_data.get('direction_successes', {}).items()},\n",
    "                'exhausted': tile_data.get('exhausted', False)\n",
    "            }\n",
    "        return {\n",
    "            'visited_tiles': set(tuple(t) for t in map_data.get('visited_tiles', [])[-1000:]),\n",
    "            'obstructions': set(tuple(t) for t in map_data.get('obstructions', [])[-500:]),\n",
    "            'interactable_objects': map_data.get('interactable_objects', [])[-50:],\n",
    "            'last_visited_timestep': map_data.get('last_visited_timestep', 0),\n",
    "            'transitions': map_data.get('transitions', [])[-20:],\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': tile_interactions\n",
    "        }\n",
    "\n",
    "    def save_exploration_memory(self):\n",
    "        try:\n",
    "            data = {f'map_{mid}': self._serialize_map_memory(md) for mid, md in self.exploration_memory.items()}\n",
    "            with open(self.EXPLORATION_MEMORY_FILE, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving exploration memory: {e}\")\n",
    "\n",
    "    def _serialize_map_memory(self, map_data):\n",
    "        serialized_ti = {}\n",
    "        for tile_key, td in list(map_data.get('tile_interactions', {}).items())[-100:]:\n",
    "            serialized_ti[tile_key] = {\n",
    "                'directions_tried': list(td.get('directions_tried', set())),\n",
    "                'direction_attempts': {str(k): v for k, v in td.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {str(k): v for k, v in td.get('direction_successes', {}).items()},\n",
    "                'exhausted': td.get('exhausted', False)\n",
    "            }\n",
    "        return {\n",
    "            'visited_tiles': list(map_data['visited_tiles'])[-1000:],\n",
    "            'obstructions': list(map_data['obstructions'])[-500:],\n",
    "            'interactable_objects': map_data['interactable_objects'][-50:],\n",
    "            'last_visited_timestep': map_data['last_visited_timestep'],\n",
    "            'transitions': map_data.get('transitions', [])[-20:],\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': serialized_ti\n",
    "        }\n",
    "\n",
    "    def get_current_map_memory(self, map_id):\n",
    "        if map_id not in self.exploration_memory:\n",
    "            self.exploration_memory[map_id] = {\n",
    "                'visited_tiles': set(), 'obstructions': set(), 'interactable_objects': [],\n",
    "                'last_visited_timestep': self.timestep, 'transitions': [], 'temp_debt': 0.0,\n",
    "                'tile_interactions': {}\n",
    "            }\n",
    "        return self.exploration_memory[map_id]\n",
    "\n",
    "    def record_visited_tile(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        if len(memory['visited_tiles']) < 1000:\n",
    "            memory['visited_tiles'].add((int(x), int(y)))\n",
    "        memory['last_visited_timestep'] = self.timestep\n",
    "\n",
    "    def record_obstruction(self, x, y, map_id, direction):\n",
    "        dx, dy = self.DIRECTION_DELTAS_INT.get(direction, (0, 0))\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        if len(memory['obstructions']) < 500:\n",
    "            memory['obstructions'].add((int(x + dx), int(y + dy)))\n",
    "\n",
    "    # =========================================================================\n",
    "    # TILE INTERACTION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_tile_interaction_key(self, x, y):\n",
    "        return f\"{int(x)}_{int(y)}\"\n",
    "    \n",
    "    def get_tile_interaction_state(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_key = self.get_tile_interaction_key(x, y)\n",
    "        if tile_key not in memory['tile_interactions']:\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(),\n",
    "                'direction_attempts': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'direction_successes': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'exhausted': False\n",
    "            }\n",
    "        return memory['tile_interactions'][tile_key]\n",
    "    \n",
    "    def should_interact_at_tile(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if tile_state['exhausted']:\n",
    "            return False\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return True\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            successes = tile_state['direction_successes'].get(d, 0)\n",
    "            if attempts > 0 and successes / attempts >= self.MIN_SUCCESS_RATE_THRESHOLD:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_untried_directions(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        return [d for d in range(4) if d not in tile_state['directions_tried']]\n",
    "\n",
    "    def get_exploration_coverage(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        visited = len(memory['visited_tiles'])\n",
    "        obstructions = len(memory['obstructions'])\n",
    "        if visited == 0 or visited + obstructions < 10:\n",
    "            return 0.0\n",
    "        return visited / (visited + obstructions)\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITIONS & DEBT\n",
    "    # =========================================================================\n",
    "    \n",
    "    def record_transition(self, from_pos, from_map, to_map, direction, action_type):\n",
    "        memory = self.get_current_map_memory(from_map)\n",
    "        for t in memory['transitions']:\n",
    "            if t['position'] == from_pos and t['direction'] == direction:\n",
    "                t['use_count'] += 1\n",
    "                t['last_used'] = self.timestep\n",
    "                return\n",
    "        memory['transitions'].append({\n",
    "            'position': from_pos, 'direction': direction, 'action': action_type,\n",
    "            'destination_map': to_map, 'use_count': 1, 'last_used': self.timestep\n",
    "        })\n",
    "        print(f\"  ðŸšª TRANSITION FOUND: Map {from_map} ({from_pos}) â†’ Map {to_map}\")\n",
    "\n",
    "    def get_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        raw_debt = memory.get('temp_debt', 0.0)\n",
    "        if map_id != self.current_map_id:\n",
    "            steps_away = self.timestep - memory.get('last_visited_timestep', 0)\n",
    "            return max(0.0, raw_debt - steps_away * self.TEMP_DEBT_DECAY)\n",
    "        return raw_debt\n",
    "\n",
    "    def accumulate_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['temp_debt'] = min(self.TEMP_DEBT_MAX, memory.get('temp_debt', 0.0) + self.TEMP_DEBT_ACCUMULATION)\n",
    "\n",
    "    def decay_all_debts(self):\n",
    "        for map_id in list(self.map_novelty_debt.keys()):\n",
    "            if map_id != self.current_map_id:\n",
    "                self.map_novelty_debt[map_id] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.map_novelty_debt[map_id] < 0.1:\n",
    "                    del self.map_novelty_debt[map_id]\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION TRACKING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_exploration_tracking(self, context_state, prev_context_state, raw_position=None, prev_raw_position=None):\n",
    "        current_map = int(context_state[2])\n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        \n",
    "        if self.current_map_id is not None and current_map != self.current_map_id:\n",
    "            if prev_context_state is not None and prev_raw_position is not None:\n",
    "                self.record_transition(prev_raw_position, self.current_map_id, current_map,\n",
    "                    int(prev_context_state[5]), 'interact' if self.last_action == 'A' else 'walk')\n",
    "            self.on_map_change(current_map)\n",
    "        \n",
    "        self.current_map_id = current_map\n",
    "        self.record_visited_tile(raw_x, raw_y, current_map)\n",
    "        self.accumulate_temp_debt(current_map)\n",
    "        self.last_direction = int(context_state[5])\n",
    "        \n",
    "        if self.timestep % 300 == 0:\n",
    "            self.decay_all_debts()\n",
    "\n",
    "    def on_map_change(self, new_map):\n",
    "        self.save_exploration_memory()\n",
    "        self.control_mode = \"move\"\n",
    "        self.frames_in_current_mode = 0\n",
    "        memory = self.get_current_map_memory(new_map)\n",
    "        print(f\"  ðŸ—ºï¸ MAP CHANGE â†’ {new_map}: {len(memory['visited_tiles'])} visited\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # UTILITY & LEARNING HELPERS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def enforce_utility_floors(self):\n",
    "        for a in self.actions():\n",
    "            floor = self.MOVE_UTILITY_FLOOR if a.group == \"move\" else self.INTERACT_UTILITY_FLOOR\n",
    "            a.utility = max(a.utility, floor)\n",
    "\n",
    "    def stagnation_level(self, window=10):\n",
    "        if len(self.prev_learning_states) < window:\n",
    "            return 0.0\n",
    "        recent = list(self.prev_learning_states)[-window:]\n",
    "        return 1.0 - np.tanh(np.mean([np.linalg.norm(recent[i] - recent[i-1]) for i in range(1, len(recent))]) * 2.0)\n",
    "\n",
    "    def track_consecutive_action(self, action_name):\n",
    "        if action_name == self.current_repeated_action:\n",
    "            self.consecutive_action_count += 1\n",
    "        else:\n",
    "            self.current_repeated_action = action_name\n",
    "            self.consecutive_action_count = 1\n",
    "\n",
    "    def get_learning_multiplier(self, action_name):\n",
    "        if action_name != self.current_repeated_action or self.consecutive_action_count < self.LEARNING_SLOWDOWN_START:\n",
    "            return 1.0\n",
    "        progress = min(1.0, (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) / \n",
    "                       (self.LEARNING_SLOWDOWN_MAX - self.LEARNING_SLOWDOWN_START))\n",
    "        return max(0.05, 1.0 - 0.95 * progress)\n",
    "\n",
    "    def apply_repetition_penalty(self):\n",
    "        if self.current_repeated_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.current_repeated_action:\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                if self.consecutive_action_count >= self.HARD_RESET_THRESHOLD:\n",
    "                    a.utility = max(floor, a.utility * 0.5)\n",
    "                    self.consecutive_action_count = 0\n",
    "                elif self.consecutive_action_count >= self.PENALTY_THRESHOLD:\n",
    "                    a.utility = max(a.utility * 0.7, floor)\n",
    "                break\n",
    "\n",
    "    def apply_pattern_penalty(self):\n",
    "        pass  # Simplified for now\n",
    "\n",
    "    def spawn_innate_entities(self, learning_state):\n",
    "        if self.innate_entities_spawned:\n",
    "            return\n",
    "        for etype, indices in [(\"sense_menu\", [5, 6]), (\"sense_battle\", [3, 4]), \n",
    "                                (\"sense_movement\", [0, 1]), (\"sense_map_transition\", [2])]:\n",
    "            entity = Perceptron(\"entity\", entity_type=etype)\n",
    "            entity.ensure_weights(len(learning_state))\n",
    "            entity.weights = np.zeros(len(learning_state))\n",
    "            for idx in indices:\n",
    "                if idx < len(entity.weights):\n",
    "                    entity.weights[idx] = 0.5 if len(indices) > 1 else 1.0\n",
    "            self.add(entity)\n",
    "        self.innate_entities_spawned = True\n",
    "\n",
    "    def compute_multi_modal_error(self, state, next_state):\n",
    "        min_len = min(len(state), len(next_state))\n",
    "        diffs = [abs(next_state[i] - state[i]) for i in range(min(8, min_len))]\n",
    "        weights = [0.5, 0.5, 10.0, 5.0, 3.0, 2.0, 1.5, 0.3]\n",
    "        weighted = sum(d * w for d, w in zip(diffs, weights[:len(diffs)]))\n",
    "        if min_len > 8:\n",
    "            weighted += np.linalg.norm(next_state[8:min_len] - state[8:min_len]) * 2.0\n",
    "        numeric = sum(diffs)\n",
    "        visual = np.linalg.norm(next_state[8:min_len] - state[8:min_len]) if min_len > 8 else 0.0\n",
    "        return weighted, numeric, visual\n",
    "\n",
    "    # =========================================================================\n",
    "    # MAIN LEARN METHOD\n",
    "    # =========================================================================\n",
    "    \n",
    "    def learn(self, learning_state, next_learning_state, context_state, next_context_state, dead=False,\n",
    "            raw_position=None, next_raw_position=None):\n",
    "        \n",
    "        # Ensure same shape\n",
    "        if len(learning_state) != len(next_learning_state):\n",
    "            max_dim = max(len(learning_state), len(next_learning_state))\n",
    "            learning_state = np.pad(learning_state, (0, max(0, max_dim - len(learning_state))))\n",
    "            next_learning_state = np.pad(next_learning_state, (0, max(0, max_dim - len(next_learning_state))))\n",
    "        \n",
    "        if not self.innate_entities_spawned:\n",
    "            self.spawn_innate_entities(learning_state)\n",
    "        \n",
    "        prev_context = self.prev_context_states[-1] if self.prev_context_states else None\n",
    "        prev_raw = getattr(self, '_last_raw_position', None)\n",
    "        self.update_exploration_tracking(context_state, prev_context, raw_position, prev_raw)\n",
    "        self._last_raw_position = raw_position\n",
    "        \n",
    "        weighted_error, numeric_error, visual_error = self.compute_multi_modal_error(learning_state, next_learning_state)\n",
    "        self.error_history.append(weighted_error)\n",
    "        self.numeric_error_history.append(numeric_error)\n",
    "        self.visual_error_history.append(visual_error)\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        \n",
    "        self.visited_maps[current_map] = self.visited_maps.get(current_map, 0) + 1\n",
    "        \n",
    "        if len(self.location_memory) < self.MAX_LOCATIONS:\n",
    "            self.location_memory[loc] = self.location_memory.get(loc, 0) + 1\n",
    "        \n",
    "        if self.visited_maps[current_map] > 10:\n",
    "            self.map_novelty_debt[current_map] = min(self.MAX_MAP_DEBT, \n",
    "                self.map_novelty_debt.get(current_map, 0.0) + 0.05 * (self.visited_maps[current_map] - 10))\n",
    "        \n",
    "        if self.visited_maps[current_map] > 30:\n",
    "            weighted_error *= 0.5\n",
    "        if self.location_memory.get(loc, 0) > 25:\n",
    "            weighted_error *= 0.7\n",
    "        \n",
    "        stagnation = self.stagnation_level()\n",
    "        learning_mult = self.get_learning_multiplier(self.last_action) if self.last_action else 1.0\n",
    "        \n",
    "        for p in self.perceptrons:\n",
    "            mult = learning_mult if (p.kind == \"action\" and p.action == self.last_action) else 1.0\n",
    "            p.update(learning_state, weighted_error * mult, stagnation=stagnation)\n",
    "        \n",
    "        self.apply_repetition_penalty()\n",
    "        self.apply_pattern_penalty()\n",
    "        self.enforce_utility_floors()\n",
    "        \n",
    "        if self.timestep % 1000 == 0:\n",
    "            self.cleanup_memory()\n",
    "        \n",
    "        if self.timestep % self.SAVE_INTERVAL == 0:\n",
    "            self.save_exploration_memory()\n",
    "        \n",
    "        self.action_history.append(self.last_action)\n",
    "\n",
    "    # =========================================================================\n",
    "    # TEACHING MODE\n",
    "    # =========================================================================\n",
    "        \n",
    "    def learn_from_human_action(self, learning_state, human_action, context_state):\n",
    "        if human_action is None or human_action == \"NONE\":\n",
    "            return\n",
    "        \n",
    "        self.demonstration_count += 1\n",
    "        context = self._detect_context(context_state)\n",
    "        \n",
    "        context_key = f\"{context}_{int(context_state[2])}\"\n",
    "        if len(self.context_action_stats) < self.MAX_CONTEXT_STATS:\n",
    "            if context_key not in self.context_action_stats:\n",
    "                self.context_action_stats[context_key] = {}\n",
    "            if human_action not in self.context_action_stats[context_key]:\n",
    "                self.context_action_stats[context_key][human_action] = 0\n",
    "            self.context_action_stats[context_key][human_action] += 1\n",
    "        \n",
    "        for a in self.actions():\n",
    "            if a.action == human_action:\n",
    "                a.utility = min(a.utility * 1.05, 2.0)\n",
    "                break\n",
    "        \n",
    "        for a in self.actions():\n",
    "            a.ensure_weights(len(learning_state))\n",
    "            if a.action == human_action:\n",
    "                a.update(learning_state, 0.1, stagnation=0.0)\n",
    "            else:\n",
    "                a.update(learning_state, -0.02, stagnation=0.0)\n",
    "\n",
    "    def _detect_context(self, context_state):\n",
    "        if context_state[3] > 0.5:\n",
    "            return \"battle\"\n",
    "        elif context_state[4] > 0.5:\n",
    "            return \"menu\"\n",
    "        else:\n",
    "            return \"overworld\"\n",
    "\n",
    "    def print_teaching_stats(self):\n",
    "        if not self.context_action_stats:\n",
    "            return\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ðŸ“š TEACHING STATS (Demos: {self.demonstration_count})\")\n",
    "        sorted_contexts = sorted(self.context_action_stats.items(),\n",
    "                                 key=lambda x: sum(x[1].values()), reverse=True)[:5]\n",
    "        for context_key, actions in sorted_contexts:\n",
    "            total = sum(actions.values())\n",
    "            print(f\"\\n  {context_key}:\")\n",
    "            for action, count in sorted(actions.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "                print(f\"    {action}: {count} ({count/total*100:.0f}%)\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODEL SAVE/LOAD\n",
    "    # =========================================================================\n",
    "\n",
    "    def save_model(self, filepath=None):\n",
    "        if filepath is None:\n",
    "            filepath = MODEL_FILE\n",
    "        \n",
    "        actions_data = []\n",
    "        for a in self.actions():\n",
    "            if a.weights is not None:\n",
    "                nonzero_indices = np.where(np.abs(a.weights) > 1e-6)[0]\n",
    "                nonzero_weights = [(int(idx), float(a.weights[idx])) for idx in nonzero_indices]\n",
    "                actions_data.append({\n",
    "                    \"action\": a.action, \"group\": a.group, \"utility\": float(a.utility),\n",
    "                    \"weights_shape\": int(len(a.weights)), \"weights_nonzero\": nonzero_weights,\n",
    "                    \"learning_rate\": float(a.learning_rate), \"familiarity\": float(a.familiarity)\n",
    "                })\n",
    "        \n",
    "        entities_data = []\n",
    "        for e in self.entities():\n",
    "            if e.weights is not None:\n",
    "                nonzero_indices = np.where(np.abs(e.weights) > 1e-6)[0]\n",
    "                nonzero_weights = [(int(idx), float(e.weights[idx])) for idx in nonzero_indices]\n",
    "                entities_data.append({\n",
    "                    \"entity_type\": e.entity_type, \"utility\": float(e.utility),\n",
    "                    \"weights_shape\": int(len(e.weights)), \"weights_nonzero\": nonzero_weights,\n",
    "                    \"familiarity\": float(e.familiarity)\n",
    "                })\n",
    "        \n",
    "        model_data = {\n",
    "            \"timestep\": int(self.timestep),\n",
    "            \"perceptrons\": {\"actions\": actions_data, \"entities\": entities_data},\n",
    "            \"debt_tracking\": {\n",
    "                \"map_novelty_debt\": {int(k): float(v) for k, v in self.map_novelty_debt.items()},\n",
    "                \"visited_maps\": {int(k): int(v) for k, v in self.visited_maps.items()}\n",
    "            },\n",
    "            \"teaching_stats\": {\n",
    "                \"demonstration_count\": int(self.demonstration_count),\n",
    "                \"context_action_stats\": self.context_action_stats\n",
    "            },\n",
    "            \"control_mode\": self.control_mode\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(model_data, f)\n",
    "            print(f\"ðŸ’¾ Model saved: {self.timestep} steps, {self.demonstration_count} demos\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Save error: {e}\")\n",
    "\n",
    "    def load_model(self, filepath=None):\n",
    "        if filepath is None:\n",
    "            filepath = MODEL_FILE\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"â„¹ï¸ No saved model at {filepath}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                model_data = json.load(f)\n",
    "            \n",
    "            self.timestep = model_data.get(\"timestep\", 0)\n",
    "            self.control_mode = model_data.get(\"control_mode\", \"move\")\n",
    "            \n",
    "            for a_data in model_data.get(\"perceptrons\", {}).get(\"actions\", []):\n",
    "                for a in self.actions():\n",
    "                    if a.action == a_data[\"action\"]:\n",
    "                        a.utility = a_data[\"utility\"]\n",
    "                        a.learning_rate = a_data.get(\"learning_rate\", 0.01)\n",
    "                        a.familiarity = a_data.get(\"familiarity\", 0.0)\n",
    "                        weights_shape = a_data[\"weights_shape\"]\n",
    "                        a.weights = np.zeros(weights_shape)\n",
    "                        for idx, val in a_data[\"weights_nonzero\"]:\n",
    "                            if idx < weights_shape:\n",
    "                                a.weights[idx] = val\n",
    "                        break\n",
    "            \n",
    "            debt_data = model_data.get(\"debt_tracking\", {})\n",
    "            self.map_novelty_debt = {int(k): float(v) for k, v in debt_data.get(\"map_novelty_debt\", {}).items()}\n",
    "            self.visited_maps = {int(k): int(v) for k, v in debt_data.get(\"visited_maps\", {}).items()}\n",
    "            \n",
    "            teaching_data = model_data.get(\"teaching_stats\", {})\n",
    "            self.demonstration_count = teaching_data.get(\"demonstration_count\", 0)\n",
    "            self.context_action_stats = teaching_data.get(\"context_action_stats\", {})\n",
    "            \n",
    "            print(f\"âœ… Model loaded: {self.timestep} steps, {self.demonstration_count} demos\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Load error: {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c1df11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 4: Action Selection - Updated with All Fixes\n",
    "# # ============================================================================\n",
    "# # CHANGES:\n",
    "# # 1. Added FORCED_EXPLORE_PROB (18%) for random exploration\n",
    "# # 2. Added \"both\" mode handling - allows all actions when stuck\n",
    "# # 3. Added turn-for-probing override - allows turns even in interact mode\n",
    "# # ============================================================================\n",
    "\n",
    "# import random  # Add to imports if not present\n",
    "\n",
    "# GBA_ACTIONS = [\"Up\", \"Down\", \"Left\", \"Right\", \"A\", \"B\", \"Start\", \"Select\"]\n",
    "# ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "# DIRECTION_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "# ACTION_TO_DIRECTION = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "\n",
    "# def manhattan_distance(pos1, pos2):\n",
    "#     return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "\n",
    "# def anticipatory_action(brain, learning_state, context_state, \n",
    "#                        exploration_weight=1.3, min_interact_prob=0.15,\n",
    "#                        raw_position=None,\n",
    "#                        forced_explore_prob=0.18):  # NEW: 18% forced random\n",
    "#     \"\"\"\n",
    "#     Action selection with all fixes:\n",
    "#     1. Forced random exploration (18%)\n",
    "#     2. \"Both\" mode when extremely stuck\n",
    "#     3. Turn-for-probing override\n",
    "#     4. Tile-based interaction probing\n",
    "#     5. Novelty-driven movement\n",
    "#     \"\"\"\n",
    "#     actions_list = brain.actions()\n",
    "#     if not actions_list:\n",
    "#         return Perceptron(\"action\", action=\"UP\", group=\"move\")\n",
    "\n",
    "#     mode = brain.determine_control_mode(context_state, raw_position=raw_position)\n",
    "#     current_map = int(context_state[2])\n",
    "#     current_dir = int(context_state[5])\n",
    "    \n",
    "#     raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "#     raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "#     current_pos = (raw_x, raw_y)\n",
    "    \n",
    "#     # Get exploration memory\n",
    "#     memory = brain.get_current_map_memory(current_map)\n",
    "#     visited_tiles = memory['visited_tiles']\n",
    "#     obstructions = memory['obstructions']\n",
    "    \n",
    "#     # Get tile interaction state\n",
    "#     tile_needs_probing = brain.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "    \n",
    "#     # NEW: Get best probe action (handles turn-then-interact)\n",
    "#     probe_action, probe_dir = brain.get_best_probe_action(raw_x, raw_y, current_map, current_dir)\n",
    "    \n",
    "#     # Get transition info\n",
    "#     transition_attraction, best_transition = brain.get_transition_attraction(current_map)\n",
    "#     coverage = brain.get_exploration_coverage(current_map)\n",
    "    \n",
    "#     # === BUILD ALLOWED ACTIONS LIST ===\n",
    "#     if mode == \"battle\":\n",
    "#         # In battle, use group weights to decide\n",
    "#         move_weight = brain.get_group_weight(\"move\")\n",
    "#         interact_weight = brain.get_group_weight(\"interact\")\n",
    "#         total = move_weight + interact_weight + 1e-9\n",
    "#         if random.random() < move_weight / total:\n",
    "#             allowed = [a for a in actions_list if a.group == \"move\"]\n",
    "#         else:\n",
    "#             allowed = [a for a in actions_list if a.group == \"interact\"]\n",
    "#         all_actions = actions_list  # Fallback\n",
    "        \n",
    "#     elif mode == \"both\":\n",
    "#         # NEW: \"Both\" mode - allow everything\n",
    "#         allowed = actions_list\n",
    "#         all_actions = actions_list\n",
    "        \n",
    "#     elif mode == \"interact\":\n",
    "#         allowed = [a for a in actions_list if a.group == \"interact\"]\n",
    "#         all_actions = None\n",
    "        \n",
    "#         # NEW: Turn-for-probing override\n",
    "#         # If we need to turn to probe, allow that movement action\n",
    "#         if probe_action and probe_action in ['UP', 'DOWN', 'LEFT', 'RIGHT']:\n",
    "#             turn_actions = [a for a in actions_list if a.action == probe_action]\n",
    "#             if turn_actions:\n",
    "#                 # Add the turn action to allowed list\n",
    "#                 allowed = allowed + turn_actions\n",
    "        \n",
    "#     else:  # move\n",
    "#         allowed = [a for a in actions_list if a.group == \"move\"]\n",
    "#         all_actions = None\n",
    "\n",
    "#     if not allowed:\n",
    "#         allowed = actions_list\n",
    "\n",
    "#     # === NEW: FORCED RANDOM EXPLORATION (18%) ===\n",
    "#     if random.random() < forced_explore_prob:\n",
    "#         chosen = random.choice(allowed)\n",
    "#         brain.record_action_execution(chosen.action)\n",
    "#         brain.track_consecutive_action(chosen.action)\n",
    "        \n",
    "#         # Still start interaction verification if it's an A press on a probeable tile\n",
    "#         if chosen.action == 'A' and tile_needs_probing:\n",
    "#             brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "        \n",
    "#         return chosen\n",
    "\n",
    "#     # === SCORE ACTIONS ===\n",
    "#     action_scores = []\n",
    "    \n",
    "#     for a in allowed:\n",
    "#         predicted = brain.predict_future_error(learning_state, a, context_state, raw_position=raw_position)\n",
    "        \n",
    "#         # --- MOVE ACTIONS ---\n",
    "#         if a.group == \"move\":\n",
    "#             if mode in [\"move\", \"both\"]:\n",
    "#                 predicted *= exploration_weight\n",
    "            \n",
    "#             dx, dy = ACTION_DELTAS.get(a.action, (0, 0))\n",
    "#             target_tile = (raw_x + dx, raw_y + dy)\n",
    "#             action_direction = ACTION_TO_DIRECTION.get(a.action, -1)\n",
    "            \n",
    "#             # BONUS: Unvisited tile\n",
    "#             if target_tile not in visited_tiles:\n",
    "#                 predicted *= brain.UNVISITED_TILE_BONUS\n",
    "            \n",
    "#             # PENALTY: Known obstruction\n",
    "#             if target_tile in obstructions:\n",
    "#                 predicted *= brain.OBSTRUCTION_PENALTY\n",
    "            \n",
    "#             # PENALTY: Transition ban\n",
    "#             if brain.is_position_banned(current_map, raw_x, raw_y, action_direction):\n",
    "#                 predicted *= 0.05\n",
    "            \n",
    "#             # BONUS: Toward transition when well-explored\n",
    "#             if transition_attraction > 0.3 and best_transition and coverage > 0.5:\n",
    "#                 trans_pos = tuple(best_transition['position']) if isinstance(best_transition['position'], list) else best_transition['position']\n",
    "#                 if manhattan_distance(target_tile, trans_pos) < manhattan_distance(current_pos, trans_pos):\n",
    "#                     predicted *= (1.0 + transition_attraction)\n",
    "            \n",
    "#             # NEW: If this is a turn needed for probing, boost it\n",
    "#             if probe_action == a.action and probe_dir is not None:\n",
    "#                 predicted *= 2.0  # Strong boost for needed turn\n",
    "            \n",
    "#             # Random factor for variety\n",
    "#             predicted *= (0.9 + random.random() * 0.2)\n",
    "        \n",
    "#         # --- INTERACT ACTIONS ---\n",
    "#         elif a.group == \"interact\":\n",
    "#             predicted = max(predicted, min_interact_prob)\n",
    "            \n",
    "#             # Menu trap B-boost\n",
    "#             if a.action == 'B':\n",
    "#                 predicted *= brain.menu_trap_b_boost\n",
    "            \n",
    "#             # A-press logic\n",
    "#             if a.action == 'A':\n",
    "#                 if tile_needs_probing and probe_action == 'A':\n",
    "#                     # We're facing an untried direction - strong boost!\n",
    "#                     predicted *= 3.0\n",
    "#                 elif tile_needs_probing:\n",
    "#                     # Tile needs probing but we need to turn first\n",
    "#                     predicted *= 0.5  # Mild penalty - turn should happen instead\n",
    "#                 else:\n",
    "#                     # Tile exhausted\n",
    "#                     predicted *= 0.1\n",
    "            \n",
    "#             # Start/Select - always penalize, no boost\n",
    "#             if a.action in ['Start', 'Select']:\n",
    "#                 predicted *= 0.3\n",
    "        \n",
    "#         action_scores.append((a, predicted))\n",
    "\n",
    "#     # === SELECT BEST ===\n",
    "#     if action_scores:\n",
    "#         best_action = max(action_scores, key=lambda x: x[1])[0]\n",
    "#         best_score = max(s for _, s in action_scores)\n",
    "        \n",
    "#         if best_score > 0.01:\n",
    "#             brain.record_action_execution(best_action.action)\n",
    "#             brain.track_consecutive_action(best_action.action)\n",
    "            \n",
    "#             # Start interaction verification for A-press on probeable tile\n",
    "#             if best_action.action == 'A' and tile_needs_probing:\n",
    "#                 brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "            \n",
    "#             return best_action\n",
    "    \n",
    "#     # === FALLBACKS ===\n",
    "    \n",
    "#     # Battle fallback\n",
    "#     if mode == \"battle\" and all_actions:\n",
    "#         all_scores = [(a, brain.predict_future_error(learning_state, a, context_state, raw_position=raw_position)) \n",
    "#                       for a in all_actions]\n",
    "#         if all_scores:\n",
    "#             best_action = max(all_scores, key=lambda x: x[1])[0]\n",
    "#             brain.record_action_execution(best_action.action)\n",
    "#             brain.track_consecutive_action(best_action.action)\n",
    "#             return best_action\n",
    "    \n",
    "#     # Move fallback: prefer unvisited\n",
    "#     if mode in [\"move\", \"both\"]:\n",
    "#         for a in allowed:\n",
    "#             if a.group == \"move\":\n",
    "#                 dx, dy = ACTION_DELTAS.get(a.action, (0, 0))\n",
    "#                 target = (raw_x + dx, raw_y + dy)\n",
    "#                 if target not in visited_tiles and target not in obstructions:\n",
    "#                     brain.record_action_execution(a.action)\n",
    "#                     brain.track_consecutive_action(a.action)\n",
    "#                     return a\n",
    "    \n",
    "#     # Generic fallback\n",
    "#     if allowed:\n",
    "#         best = max(allowed, key=lambda a: a.utility)\n",
    "#         brain.record_action_execution(best.action)\n",
    "#         brain.track_consecutive_action(best.action)\n",
    "#         return best\n",
    "    \n",
    "#     best = max(actions_list, key=lambda a: a.utility)\n",
    "#     brain.record_action_execution(best.action)\n",
    "#     brain.track_consecutive_action(best.action)\n",
    "#     return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b935e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Taught Transitions Manager (Markov Imitation Learning)\n",
    "# ============================================================================\n",
    "\n",
    "TRANSITIONS_FILE = BASE_PATH / \"taught_transitions.json\"\n",
    "\n",
    "class TaughtTransitionsManager:\n",
    "    \"\"\"\n",
    "    Manages taught transition data for Markov imitation learning.\n",
    "    \n",
    "    Similarity Scoring:\n",
    "    - Immediate (50%): Same map, position within 2 tiles, same direction, same battle/menu state\n",
    "    - Sequential (30%): Last 8 actions match (with subset checks at 5, 3)\n",
    "    - Partial (20%): in_battle, in_menu, movement_blocked, near_transition, tile_probed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batches = []\n",
    "        self.metadata = {}\n",
    "        self.frame_index = []  # Flat list of all frames for fast lookup\n",
    "        self.last_load_time = 0\n",
    "        \n",
    "        # Similarity weights\n",
    "        self.IMMEDIATE_WEIGHT = 0.50\n",
    "        self.SEQUENTIAL_WEIGHT = 0.30\n",
    "        self.PARTIAL_WEIGHT = 0.20\n",
    "        \n",
    "        # Thresholds\n",
    "        self.SIMILARITY_THRESHOLD = 0.6\n",
    "        self.POSITION_TOLERANCE = 2  # Tiles\n",
    "        \n",
    "    def load_transitions(self, force=False):\n",
    "        \"\"\"Load taught transitions from file.\"\"\"\n",
    "        if not TRANSITIONS_FILE.exists():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            mod_time = TRANSITIONS_FILE.stat().st_mtime\n",
    "            if not force and mod_time == self.last_load_time:\n",
    "                return True  # Already loaded\n",
    "            \n",
    "            with open(TRANSITIONS_FILE, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            self.batches = data.get('batches', [])\n",
    "            self.metadata = data.get('metadata', {})\n",
    "            self.last_load_time = mod_time\n",
    "            \n",
    "            # Build flat frame index for fast lookup\n",
    "            self._build_frame_index()\n",
    "            \n",
    "            print(f\"âœ… Loaded {len(self.batches)} batches, {len(self.frame_index)} frames\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading transitions: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _build_frame_index(self):\n",
    "        \"\"\"Build flat index of all frames for fast similarity search.\"\"\"\n",
    "        self.frame_index = []\n",
    "        \n",
    "        for batch in self.batches:\n",
    "            batch_type = batch.get('batch_type', 'steady')\n",
    "            trigger = batch.get('trigger_action', None)\n",
    "            \n",
    "            for frame in batch.get('frames', []):\n",
    "                self.frame_index.append({\n",
    "                    'batch_type': batch_type,\n",
    "                    'trigger_action': trigger,\n",
    "                    'state': frame.get('state', {}),\n",
    "                    'action': frame.get('action'),\n",
    "                    'recent_actions': frame.get('recent_actions', [])\n",
    "                })\n",
    "    \n",
    "    def compute_immediate_similarity(self, current_state, taught_state):\n",
    "        \"\"\"\n",
    "        Immediate context similarity (50% weight).\n",
    "        Same map, position within 2 tiles, same direction, same battle/menu state.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Map match (required)\n",
    "        if current_state.get('map_id') != taught_state.get('map_id'):\n",
    "            return 0.0\n",
    "        score += 0.25\n",
    "        \n",
    "        # Position within tolerance\n",
    "        dx = abs(current_state.get('x', 0) - taught_state.get('x', 0))\n",
    "        dy = abs(current_state.get('y', 0) - taught_state.get('y', 0))\n",
    "        if dx <= self.POSITION_TOLERANCE and dy <= self.POSITION_TOLERANCE:\n",
    "            # Closer = higher score\n",
    "            dist = dx + dy\n",
    "            score += 0.35 * (1.0 - dist / (2 * self.POSITION_TOLERANCE + 1))\n",
    "        \n",
    "        # Direction match\n",
    "        if current_state.get('direction') == taught_state.get('direction'):\n",
    "            score += 0.20\n",
    "        \n",
    "        # Battle/menu state match\n",
    "        if current_state.get('in_battle') == taught_state.get('in_battle'):\n",
    "            score += 0.10\n",
    "        if current_state.get('in_menu') == taught_state.get('in_menu'):\n",
    "            score += 0.10\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def compute_sequential_similarity(self, current_recent, taught_recent):\n",
    "        \"\"\"\n",
    "        Sequential similarity (30% weight).\n",
    "        Last 8 actions match, with subset checks at 5, 3.\n",
    "        \"\"\"\n",
    "        if not current_recent or not taught_recent:\n",
    "            return 0.0\n",
    "        \n",
    "        score = 0.0\n",
    "        \n",
    "        # Full 8-action match\n",
    "        match_8 = self._count_trailing_matches(current_recent, taught_recent, 8)\n",
    "        if match_8 >= 8:\n",
    "            score = 1.0\n",
    "        elif match_8 >= 5:\n",
    "            score = 0.7\n",
    "        elif match_8 >= 3:\n",
    "            score = 0.4\n",
    "        else:\n",
    "            # Partial credit for any matches\n",
    "            score = match_8 / 8.0 * 0.3\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _count_trailing_matches(self, list1, list2, max_check):\n",
    "        \"\"\"Count how many trailing elements match.\"\"\"\n",
    "        matches = 0\n",
    "        len1 = len(list1)\n",
    "        len2 = len(list2)\n",
    "        \n",
    "        for i in range(1, min(max_check + 1, len1 + 1, len2 + 1)):\n",
    "            if list1[-i] == list2[-i]:\n",
    "                matches += 1\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def compute_partial_similarity(self, current_state, taught_state, brain=None):\n",
    "        \"\"\"\n",
    "        Partial context similarity (20% weight).\n",
    "        in_battle, in_menu, movement_blocked, near_transition, tile_probed.\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        # Battle state\n",
    "        if current_state.get('in_battle') == taught_state.get('in_battle'):\n",
    "            score += 0.30\n",
    "        \n",
    "        # Menu state\n",
    "        if current_state.get('in_menu') == taught_state.get('in_menu'):\n",
    "            score += 0.30\n",
    "        \n",
    "        # Additional context if brain available\n",
    "        if brain is not None:\n",
    "            map_id = current_state.get('map_id', 0)\n",
    "            x = current_state.get('x', 0)\n",
    "            y = current_state.get('y', 0)\n",
    "            \n",
    "            # Near transition\n",
    "            memory = brain.get_current_map_memory(map_id)\n",
    "            transitions = memory.get('transitions', [])\n",
    "            for t in transitions:\n",
    "                pos = t.get('position', [0, 0])\n",
    "                if abs(x - pos[0]) <= 3 and abs(y - pos[1]) <= 3:\n",
    "                    score += 0.20\n",
    "                    break\n",
    "            \n",
    "            # Tile probed\n",
    "            if brain.should_interact_at_tile(x, y, map_id):\n",
    "                score += 0.20\n",
    "        else:\n",
    "            # Without brain, give partial credit\n",
    "            score += 0.20\n",
    "        \n",
    "        return min(score, 1.0)\n",
    "    \n",
    "    def compute_similarity(self, current_state, current_recent, taught_frame, brain=None):\n",
    "        \"\"\"Compute total similarity score.\"\"\"\n",
    "        taught_state = taught_frame.get('state', {})\n",
    "        taught_recent = taught_frame.get('recent_actions', [])\n",
    "        \n",
    "        immediate = self.compute_immediate_similarity(current_state, taught_state)\n",
    "        sequential = self.compute_sequential_similarity(current_recent, taught_recent)\n",
    "        partial = self.compute_partial_similarity(current_state, taught_state, brain)\n",
    "        \n",
    "        total = (immediate * self.IMMEDIATE_WEIGHT + \n",
    "                 sequential * self.SEQUENTIAL_WEIGHT + \n",
    "                 partial * self.PARTIAL_WEIGHT)\n",
    "        \n",
    "        return total, {\n",
    "            'immediate': immediate,\n",
    "            'sequential': sequential,\n",
    "            'partial': partial\n",
    "        }\n",
    "    \n",
    "    def find_best_action(self, current_state, current_recent, brain=None):\n",
    "        \"\"\"\n",
    "        Find the best action from taught data based on similarity.\n",
    "        \n",
    "        Returns: (action, similarity_score, details) or (None, 0, None) if no good match\n",
    "        \"\"\"\n",
    "        if not self.frame_index:\n",
    "            self.load_transitions()\n",
    "        \n",
    "        if not self.frame_index:\n",
    "            return None, 0.0, None\n",
    "        \n",
    "        best_action = None\n",
    "        best_score = 0.0\n",
    "        best_details = None\n",
    "        best_frame = None\n",
    "        \n",
    "        for frame in self.frame_index:\n",
    "            score, details = self.compute_similarity(\n",
    "                current_state, current_recent, frame, brain\n",
    "            )\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = frame.get('action')\n",
    "                best_details = details\n",
    "                best_frame = frame\n",
    "        \n",
    "        # Only return if above threshold\n",
    "        if best_score >= self.SIMILARITY_THRESHOLD:\n",
    "            return best_action, best_score, {\n",
    "                'details': best_details,\n",
    "                'matched_state': best_frame.get('state') if best_frame else None,\n",
    "                'batch_type': best_frame.get('batch_type') if best_frame else None\n",
    "            }\n",
    "        \n",
    "        return None, best_score, None\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get statistics about loaded transitions.\"\"\"\n",
    "        if not self.batches:\n",
    "            return None\n",
    "        \n",
    "        action_counts = {}\n",
    "        for frame in self.frame_index:\n",
    "            action = frame.get('action', 'NONE')\n",
    "            action_counts[action] = action_counts.get(action, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_batches': len(self.batches),\n",
    "            'total_frames': len(self.frame_index),\n",
    "            'action_changes': self.metadata.get('action_changes', 0),\n",
    "            'maps_visited': self.metadata.get('maps_visited', []),\n",
    "            'action_distribution': action_counts\n",
    "        }\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print transition statistics.\"\"\"\n",
    "        stats = self.get_stats()\n",
    "        if not stats:\n",
    "            print(\"No transitions loaded\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ðŸ“Š TAUGHT TRANSITIONS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"  Batches: {stats['total_batches']}\")\n",
    "        print(f\"  Frames: {stats['total_frames']}\")\n",
    "        print(f\"  Action changes: {stats['action_changes']}\")\n",
    "        print(f\"  Maps: {stats['maps_visited']}\")\n",
    "        print(f\"\\n  Action distribution:\")\n",
    "        for action, count in sorted(stats['action_distribution'].items(), \n",
    "                                     key=lambda x: x[1], reverse=True):\n",
    "            pct = count / stats['total_frames'] * 100\n",
    "            print(f\"    {action}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "\n",
    "# Global instance\n",
    "transitions_manager = TaughtTransitionsManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "963a80e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded exploration memory: 0 maps\n",
      "âœ… Model loaded: 0 steps, 0 demos\n",
      "âœ… Loaded 0 batches, 0 frames\n",
      "======================================================================\n",
      "ðŸŽ“ TEACHING MODE + MARKOV TRANSITIONS\n",
      "======================================================================\n",
      "Input file: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\PokeAI\\input_cache.txt\n",
      "Transitions file: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\PokeAI\\taught_transitions.json\n",
      "Memory file: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\PokeAI\\exploration_memory.json\n",
      "Model file: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\PokeAI\\model_checkpoint.json\n",
      "======================================================================\n",
      "No transitions loaded\n",
      "\n",
      "============================================================\n",
      "ðŸ“¦ BATCH #1: 109 inputs\n",
      "  Actions: {'Start': 12, 'DOWN': 42, 'A': 55}\n",
      "  Map 17 | Pos (13, 13)\n",
      "  Visited: 1 | Demos: 109\n",
      "  Taught matches: 0/109 (0%)\n",
      "  Utils: DOWN:1.99 A:1.99 Start:0.99 UP:0.59 LEFT:0.59 RIGHT:0.59 B:0.59 Select:0.59\n",
      "\n",
      "  ðŸ’¾ SAVING...\n",
      "ðŸ’¾ Model saved: 109 steps, 109 demos\n",
      "     âœ“ Saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m file_size = INPUT_FILE.stat().st_size\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_mod_time == last_input_mod_time \u001b[38;5;129;01mor\u001b[39;00m file_size == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPOLL_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     77\u001b[39m last_input_mod_time = current_mod_time\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Main Loop - WITH TRANSITION LEARNING\n",
    "# ============================================================================\n",
    "\n",
    "import gc\n",
    "\n",
    "brain = Brain()\n",
    "\n",
    "# Action perceptrons\n",
    "for b in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"move\"))\n",
    "for b in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"interact\"))\n",
    "\n",
    "# Load existing model\n",
    "brain.load_model()\n",
    "\n",
    "# Initialize transitions manager\n",
    "transitions_manager = TaughtTransitionsManager()\n",
    "transitions_manager.load_transitions()\n",
    "\n",
    "# Stats\n",
    "total_inputs_processed = 0\n",
    "batches_processed = 0\n",
    "\n",
    "# Visual cache\n",
    "cached_palette = np.zeros(PALETTE_DIM)\n",
    "cached_tiles = np.zeros(TILE_DIM)\n",
    "last_visual_update = 0\n",
    "VISUAL_UPDATE_INTERVAL = 50\n",
    "\n",
    "# Recent actions tracking (mirrors Lua's tracking)\n",
    "recent_actions_buffer = []\n",
    "RECENT_ACTIONS_SIZE = 8\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ“ TEACHING MODE + MARKOV TRANSITIONS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Input file: {INPUT_FILE}\")\n",
    "print(f\"Transitions file: {TRANSITIONS_FILE}\")\n",
    "print(f\"Memory file: {brain.EXPLORATION_MEMORY_FILE}\")\n",
    "print(f\"Model file: {MODEL_FILE}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Print initial transition stats\n",
    "transitions_manager.print_stats()\n",
    "\n",
    "POLL_INTERVAL = 2.0\n",
    "prev_context_state = None\n",
    "prev_raw_position = None\n",
    "last_input_mod_time = 0\n",
    "\n",
    "def add_to_recent_actions(action):\n",
    "    \"\"\"Track recent actions for similarity matching.\"\"\"\n",
    "    global recent_actions_buffer\n",
    "    if action:\n",
    "        recent_actions_buffer.append(action)\n",
    "        if len(recent_actions_buffer) > RECENT_ACTIONS_SIZE:\n",
    "            recent_actions_buffer = recent_actions_buffer[-RECENT_ACTIONS_SIZE:]\n",
    "\n",
    "while True:\n",
    "    # Check if input file exists\n",
    "    if not INPUT_FILE.exists():\n",
    "        print(f\"[Waiting for {INPUT_FILE.name}...]\")\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "        continue\n",
    "    \n",
    "    # Check file modification\n",
    "    try:\n",
    "        current_mod_time = INPUT_FILE.stat().st_mtime\n",
    "        file_size = INPUT_FILE.stat().st_size\n",
    "        \n",
    "        if current_mod_time == last_input_mod_time or file_size == 0:\n",
    "            time.sleep(POLL_INTERVAL)\n",
    "            continue\n",
    "        \n",
    "        last_input_mod_time = current_mod_time\n",
    "    except Exception as e:\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "        continue\n",
    "    \n",
    "    # Read inputs\n",
    "    inputs = []\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 7:\n",
    "                    action_code = parts[0]\n",
    "                    inputs.append({\n",
    "                        'action': ACTION_MAP.get(action_code, action_code),\n",
    "                        'x': int(parts[1]),\n",
    "                        'y': int(parts[2]),\n",
    "                        'map': int(parts[3]),\n",
    "                        'in_battle': int(parts[4]),\n",
    "                        'menu_flag': int(parts[5]),\n",
    "                        'direction': int(parts[6])\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"[Read error: {e}]\")\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "        continue\n",
    "    \n",
    "    if not inputs:\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "        continue\n",
    "    \n",
    "    # Process batch\n",
    "    batches_processed += 1\n",
    "    batch_size = len(inputs)\n",
    "    total_inputs_processed += batch_size\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ“¦ BATCH #{batches_processed}: {batch_size} inputs\")\n",
    "    \n",
    "    # Reload transitions periodically\n",
    "    if batches_processed % 5 == 0:\n",
    "        transitions_manager.load_transitions()\n",
    "    \n",
    "    # Update visual cache\n",
    "    if batches_processed - last_visual_update >= VISUAL_UPDATE_INTERVAL:\n",
    "        context, palette, tiles, raw_pos = read_game_state_full()\n",
    "        if np.any(palette != 0):\n",
    "            cached_palette = palette\n",
    "            cached_tiles = tiles\n",
    "            last_visual_update = batches_processed\n",
    "    \n",
    "    # Process inputs\n",
    "    action_counts = {}\n",
    "    taught_matches = 0\n",
    "    \n",
    "    for inp in inputs:\n",
    "        inp_context, inp_raw_pos, human_action = process_cached_input(inp)\n",
    "        \n",
    "        brain.update_position(inp_raw_pos[0], inp_raw_pos[1])\n",
    "        \n",
    "        derived = compute_derived_features(inp_context, prev_context_state)\n",
    "        learning_state = build_learning_state(derived, cached_palette, cached_tiles, inp_context[3])\n",
    "        \n",
    "        brain.log_state(learning_state, inp_context)\n",
    "        \n",
    "        if human_action:\n",
    "            # Track recent actions\n",
    "            add_to_recent_actions(human_action)\n",
    "            \n",
    "            # Learn from human action\n",
    "            brain.learn_from_human_action(learning_state, human_action, inp_context)\n",
    "            brain.last_action = human_action\n",
    "            action_counts[human_action] = action_counts.get(human_action, 0) + 1\n",
    "            \n",
    "            # Check similarity with taught transitions\n",
    "            current_state = {\n",
    "                'map_id': inp.get('map'),\n",
    "                'x': inp.get('x'),\n",
    "                'y': inp.get('y'),\n",
    "                'direction': inp.get('direction'),\n",
    "                'in_battle': inp.get('in_battle'),\n",
    "                'in_menu': inp.get('menu_flag')\n",
    "            }\n",
    "            \n",
    "            taught_action, sim_score, match_info = transitions_manager.find_best_action(\n",
    "                current_state, recent_actions_buffer, brain\n",
    "            )\n",
    "            \n",
    "            if taught_action and sim_score >= 0.6:\n",
    "                taught_matches += 1\n",
    "                # Could log or use this for analysis\n",
    "        \n",
    "        if prev_context_state is not None:\n",
    "            prev_derived = compute_derived_features(prev_context_state, None)\n",
    "            prev_learning = build_learning_state(prev_derived, cached_palette, cached_tiles, \n",
    "                                                  prev_context_state[3])\n",
    "            brain.learn(prev_learning, learning_state, prev_context_state, inp_context,\n",
    "                       dead=False, raw_position=prev_raw_position, next_raw_position=inp_raw_pos)\n",
    "        \n",
    "        prev_context_state = inp_context.copy()\n",
    "        prev_raw_position = inp_raw_pos\n",
    "        brain.timestep += 1\n",
    "    \n",
    "    # Summary\n",
    "    current_map = int(inp_context[2])\n",
    "    memory = brain.get_current_map_memory(current_map)\n",
    "    \n",
    "    print(f\"  Actions: {action_counts}\")\n",
    "    print(f\"  Map {current_map} | Pos ({inp_raw_pos[0]}, {inp_raw_pos[1]})\")\n",
    "    print(f\"  Visited: {len(memory['visited_tiles'])} | Demos: {brain.demonstration_count}\")\n",
    "    print(f\"  Taught matches: {taught_matches}/{batch_size} ({taught_matches/max(1,batch_size)*100:.0f}%)\")\n",
    "    \n",
    "    # Utilities\n",
    "    utils = sorted([(a.action, a.utility) for a in brain.actions()], \n",
    "                   key=lambda x: x[1], reverse=True)\n",
    "    print(f\"  Utils: {' '.join([f'{k}:{v:.2f}' for k,v in utils])}\")\n",
    "    \n",
    "    # Save every batch\n",
    "    print(f\"\\n  ðŸ’¾ SAVING...\")\n",
    "    try:\n",
    "        brain.save_exploration_memory()\n",
    "        brain.save_model()\n",
    "        print(f\"     âœ“ Saved\")\n",
    "    except Exception as e:\n",
    "        print(f\"     âœ— Error: {e}\")\n",
    "    \n",
    "    # Memory cleanup every 10 batches\n",
    "    if batches_processed % 10 == 0:\n",
    "        stats = brain.get_memory_stats()\n",
    "        print(f\"\\n  ðŸ“Š MEMORY: Maps={stats['exploration_maps']}, Tiles={stats['total_tiles']}\")\n",
    "        \n",
    "        # Print transition stats\n",
    "        transitions_manager.print_stats()\n",
    "        \n",
    "        brain.cleanup_memory()\n",
    "        gc.collect()\n",
    "    \n",
    "    inputs = None\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
