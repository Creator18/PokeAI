{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "677ae718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: State Management & Utilities (OPTIMIZED)\n",
    "# ============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "BASE_PATH = Path(\"C:/Users/natmaw/Documents/Boston Stuff/CS 5100 Foundations of AI/cogai\")\n",
    "ACTION_FILE = BASE_PATH / \"action.json\"\n",
    "STATE_FILE = BASE_PATH / \"game_state.json\"\n",
    "INPUT_FILE = BASE_PATH / \"input_cache.txt\"  # Separate input file\n",
    "MODEL_FILE = BASE_PATH / \"model_checkpoint.json\"\n",
    "\n",
    "EXPECTED_STATE_DIM = 6\n",
    "PALETTE_DIM = 768\n",
    "TILE_DIM = 600\n",
    "\n",
    "# Action code mapping (shortened in Lua for performance)\n",
    "ACTION_MAP = {\n",
    "    'U': 'UP', 'D': 'DOWN', 'L': 'LEFT', 'R': 'RIGHT',\n",
    "    'A': 'A', 'B': 'B', 'S': 'Start', 'E': 'Select'\n",
    "}\n",
    "\n",
    "# Track file modification times\n",
    "last_state_mod_time = 0\n",
    "last_input_mod_time = 0\n",
    "\n",
    "def normalize_game_state(raw_state):\n",
    "    \"\"\"Normalize context state for learning.\"\"\"\n",
    "    if len(raw_state) < 6:\n",
    "        raw_state = list(raw_state) + [0] * (6 - len(raw_state))\n",
    "    \n",
    "    normalized = np.array(raw_state, dtype=float)\n",
    "    normalized[0] = raw_state[0] / 255.0\n",
    "    normalized[1] = raw_state[1] / 255.0\n",
    "    normalized[2] = np.clip(raw_state[2], 0, 255)\n",
    "    normalized[3] = 1.0 if raw_state[3] > 0 else 0.0\n",
    "    normalized[4] = 1.0 if raw_state[4] > 0 else 0.0\n",
    "    normalized[5] = int(raw_state[5]) % 4\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def compute_derived_features(current, prev):\n",
    "    \"\"\"Extract temporal features (8D)\"\"\"\n",
    "    if prev is None:\n",
    "        return np.zeros(8)\n",
    "    \n",
    "    vel_x = current[0] - prev[0]\n",
    "    vel_y = current[1] - prev[1]\n",
    "    map_changed = 1.0 if abs(current[2] - prev[2]) > 0.5 else 0.0\n",
    "    battle_started = 1.0 if current[3] > prev[3] else 0.0\n",
    "    battle_ended = 1.0 if current[3] < prev[3] else 0.0\n",
    "    menu_opened = 1.0 if current[4] > prev[4] else 0.0\n",
    "    menu_closed = 1.0 if current[4] < prev[4] else 0.0\n",
    "    direction_changed = 1.0 if current[5] != prev[5] else 0.0\n",
    "    \n",
    "    return np.array([vel_x, vel_y, map_changed, battle_started, battle_ended,\n",
    "                     menu_opened, menu_closed, direction_changed])\n",
    "\n",
    "def build_learning_state(derived, palette, tiles, in_battle):\n",
    "    \"\"\"Build learning state vector.\"\"\"\n",
    "    if in_battle > 0.5:\n",
    "        state = np.concatenate([derived, palette])\n",
    "    else:\n",
    "        state = np.concatenate([derived, tiles, palette])\n",
    "    \n",
    "    noise = np.random.randn(len(state)) * 0.0001\n",
    "    return state + noise\n",
    "\n",
    "def read_input_cache():\n",
    "    \"\"\"\n",
    "    Read and parse the input cache file.\n",
    "    Returns list of input dicts, or empty list if no new data.\n",
    "    \"\"\"\n",
    "    global last_input_mod_time\n",
    "    \n",
    "    if not INPUT_FILE.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        current_mod_time = INPUT_FILE.stat().st_mtime\n",
    "        if current_mod_time == last_input_mod_time:\n",
    "            return []  # No new data\n",
    "        last_input_mod_time = current_mod_time\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    inputs = []\n",
    "    try:\n",
    "        with open(INPUT_FILE, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                \n",
    "                parts = line.split(',')\n",
    "                if len(parts) >= 7:\n",
    "                    action_code = parts[0]\n",
    "                    inputs.append({\n",
    "                        'action': ACTION_MAP.get(action_code, action_code),\n",
    "                        'x': int(parts[1]),\n",
    "                        'y': int(parts[2]),\n",
    "                        'map': int(parts[3]),\n",
    "                        'in_battle': int(parts[4]),\n",
    "                        'menu_flag': int(parts[5]),\n",
    "                        'direction': int(parts[6])\n",
    "                    })\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error reading input cache: {e}\")\n",
    "        return []\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "def read_game_state_minimal():\n",
    "    \"\"\"\n",
    "    Read current game state (minimal version).\n",
    "    Returns: context_state, raw_position, input_count\n",
    "    \"\"\"\n",
    "    global last_state_mod_time\n",
    "    \n",
    "    if not STATE_FILE.exists():\n",
    "        return np.zeros(EXPECTED_STATE_DIM), (0, 0), 0\n",
    "    \n",
    "    try:\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        \n",
    "        raw = data.get('s', [0, 0, 0, 0, 0, 0])\n",
    "        input_count = data.get('ic', 0)\n",
    "        \n",
    "        context_state = normalize_game_state(raw)\n",
    "        raw_position = (int(raw[0]), int(raw[1]))\n",
    "        \n",
    "        return context_state, raw_position, input_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return np.zeros(EXPECTED_STATE_DIM), (0, 0), 0\n",
    "\n",
    "def read_game_state_full():\n",
    "    \"\"\"\n",
    "    Read full game state including visuals.\n",
    "    Returns: context_state, palette_state, tile_state, raw_position\n",
    "    \"\"\"\n",
    "    if not STATE_FILE.exists():\n",
    "        return (np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), \n",
    "                np.zeros(TILE_DIM), (0, 0))\n",
    "    \n",
    "    try:\n",
    "        with open(STATE_FILE, 'r') as f:\n",
    "            data = json.loads(f.read())\n",
    "        \n",
    "        raw = data.get('s', [0, 0, 0, 0, 0, 0])\n",
    "        palette_raw = data.get('p', [])\n",
    "        tiles_raw = data.get('t', [])\n",
    "        \n",
    "        context_state = normalize_game_state(raw)\n",
    "        raw_position = (int(raw[0]), int(raw[1]))\n",
    "        \n",
    "        # Process palette\n",
    "        if palette_raw:\n",
    "            palette_state = np.array(palette_raw, dtype=float)\n",
    "        else:\n",
    "            palette_state = np.zeros(PALETTE_DIM)\n",
    "        \n",
    "        # Process tiles\n",
    "        if tiles_raw:\n",
    "            tile_state = np.array(tiles_raw, dtype=float)\n",
    "        else:\n",
    "            tile_state = np.zeros(TILE_DIM)\n",
    "        \n",
    "        # Ensure correct dimensions\n",
    "        if len(palette_state) < PALETTE_DIM:\n",
    "            palette_state = np.pad(palette_state, (0, PALETTE_DIM - len(palette_state)))\n",
    "        elif len(palette_state) > PALETTE_DIM:\n",
    "            palette_state = palette_state[:PALETTE_DIM]\n",
    "            \n",
    "        if len(tile_state) < TILE_DIM:\n",
    "            tile_state = np.pad(tile_state, (0, TILE_DIM - len(tile_state)))\n",
    "        elif len(tile_state) > TILE_DIM:\n",
    "            tile_state = tile_state[:TILE_DIM]\n",
    "        \n",
    "        return context_state, palette_state, tile_state, raw_position\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Error reading full state: {e}\")\n",
    "        return (np.zeros(EXPECTED_STATE_DIM), np.zeros(PALETTE_DIM), \n",
    "                np.zeros(TILE_DIM), (0, 0))\n",
    "\n",
    "def process_cached_input(inp):\n",
    "    \"\"\"Convert a cached input dict to normalized state + action.\"\"\"\n",
    "    raw_state = [\n",
    "        inp.get('x', 0),\n",
    "        inp.get('y', 0),\n",
    "        inp.get('map', 0),\n",
    "        inp.get('in_battle', 0),\n",
    "        inp.get('menu_flag', 0),\n",
    "        inp.get('direction', 0)\n",
    "    ]\n",
    "    context_state = normalize_game_state(raw_state)\n",
    "    raw_position = (inp.get('x', 0), inp.get('y', 0))\n",
    "    action = inp.get('action', None)\n",
    "    \n",
    "    return context_state, raw_position, action\n",
    "\n",
    "# Legacy compatibility\n",
    "def read_game_state(max_retries=3):\n",
    "    \"\"\"Legacy function for compatibility.\"\"\"\n",
    "    context, palette, tiles, raw_pos = read_game_state_full()\n",
    "    return context, palette, tiles, False, raw_pos, None\n",
    "\n",
    "def write_action(action_name):\n",
    "    if action_name:\n",
    "        action_name = action_name.upper()\n",
    "    try:\n",
    "        with open(ACTION_FILE, \"w\") as f:\n",
    "            json.dump({\"action\": action_name}, f)\n",
    "            f.flush()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to write action: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9dd8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Perceptron Classes\n",
    "# ============================================================================\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, kind, action=None, group=None, entity_type=None):\n",
    "        self.kind = kind\n",
    "        self.action = action\n",
    "        self.group = group\n",
    "        self.entity_type = entity_type\n",
    "        \n",
    "        self.utility = 1.0\n",
    "        self.weights = None\n",
    "        \n",
    "        self.eligibility_fast = 0.0\n",
    "        self.eligibility_slow = 0.0\n",
    "        \n",
    "        self.familiarity = 0.0\n",
    "        self.activation_history = deque(maxlen=10)\n",
    "        \n",
    "        self.learning_rate = 0.01\n",
    "        self.prediction_errors = deque(maxlen=50)\n",
    "\n",
    "    def ensure_weights(self, dim):\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(dim) * 0.001\n",
    "\n",
    "    def predict(self, state):\n",
    "        self.ensure_weights(len(state))\n",
    "        raw_activation = np.dot(self.weights, state)\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            novelty_factor = 1.0 / (1.0 + np.sqrt(self.familiarity * 0.5))\n",
    "            decayed_activation = raw_activation * novelty_factor\n",
    "            self.activation_history.append(abs(raw_activation))\n",
    "            return decayed_activation\n",
    "        else:\n",
    "            return raw_activation\n",
    "\n",
    "    def adapt_learning_rate(self):\n",
    "        if len(self.prediction_errors) >= 50:\n",
    "            avg_error = np.mean(self.prediction_errors)\n",
    "            \n",
    "            if avg_error < 0.1:\n",
    "                self.learning_rate = max(0.001, self.learning_rate * 0.99)\n",
    "            elif avg_error > 0.5:\n",
    "                self.learning_rate = min(0.05, self.learning_rate * 1.01)\n",
    "\n",
    "    def update(self, state, error, gamma_fast=0.5, gamma_slow=0.95, stagnation=0.0):\n",
    "        self.ensure_weights(len(state))\n",
    "        \n",
    "        self.eligibility_fast = gamma_fast * self.eligibility_fast + 1.0\n",
    "        self.eligibility_slow = gamma_slow * self.eligibility_slow + 1.0\n",
    "        \n",
    "        self.adapt_learning_rate()\n",
    "        \n",
    "        fast_update = 0.7 * self.learning_rate * error * state * self.eligibility_fast\n",
    "        slow_update = 0.3 * self.learning_rate * error * state * self.eligibility_slow\n",
    "        self.weights += fast_update + slow_update\n",
    "\n",
    "        if self.kind == \"action\":\n",
    "            if error > 0.01:\n",
    "                if stagnation > 0.5:\n",
    "                    self.utility *= 0.97\n",
    "                elif error > 0.2:\n",
    "                    self.utility = min(self.utility * 1.02, 2.0)\n",
    "                else:\n",
    "                    self.utility *= 0.995\n",
    "            \n",
    "            if self.group == \"move\":\n",
    "                self.utility = np.clip(self.utility, 0.1, 2.0)\n",
    "            else:\n",
    "                self.utility = np.clip(self.utility, 0.01, 2.0)\n",
    "        \n",
    "        if self.kind == \"entity\" and len(self.activation_history) > 0:\n",
    "            recent_avg = np.mean(self.activation_history)\n",
    "            if recent_avg > 0.1:\n",
    "                self.familiarity += 0.03\n",
    "        \n",
    "        if self.kind == \"entity\":\n",
    "            prediction = self.predict(state)\n",
    "            self.prediction_errors.append(abs(prediction - error))\n",
    "\n",
    "\n",
    "class ControlSwapPerceptron(Perceptron):\n",
    "    def __init__(self):\n",
    "        super().__init__(kind=\"control_swap\")\n",
    "        self.swap_history = deque(maxlen=100)\n",
    "        self.confidence = 0.0\n",
    "        \n",
    "    def should_swap(self, state, movement_stagnation):\n",
    "        if self.weights is None:\n",
    "            return False, 0.0\n",
    "        \n",
    "        self.ensure_weights(len(state))\n",
    "        swap_score = np.dot(self.weights, state)\n",
    "        stagnation_factor = np.tanh(movement_stagnation / 5.0)\n",
    "        combined_score = swap_score * 0.7 + stagnation_factor * 0.3\n",
    "        \n",
    "        return combined_score > 0.5, abs(combined_score)\n",
    "    \n",
    "    def record_swap_outcome(self, state, swapped, novelty_gained):\n",
    "        self.swap_history.append((swapped, novelty_gained))\n",
    "        \n",
    "        if len(self.swap_history) >= 20:\n",
    "            recent = list(self.swap_history)[-20:]\n",
    "            successful = sum(1 for swap, nov in recent if swap and nov > 0.2)\n",
    "            self.confidence = successful / 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "980759f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Brain Class - All Updates Integrated\n",
    "# ============================================================================\n",
    "# CHANGES FROM PREVIOUS VERSION:\n",
    "# 1. Added get_best_probe_action() for turn-then-interact sequencing\n",
    "# 2. Added \"both\" mode support (should_use_both_mode, BOTH_MODE thresholds)\n",
    "# 3. Added debt caps (MAX_MAP_DEBT, MAX_LOCATION_DEBT) and decay_all_debts()\n",
    "# 4. Added direction change as partial progress tracking\n",
    "# 5. Increased INTERACTION_VERIFY_FRAMES from 5 to 8\n",
    "# ============================================================================\n",
    "\n",
    "class Brain:\n",
    "    def __init__(self):\n",
    "        self.perceptrons = []\n",
    "        \n",
    "        self.prev_learning_states = deque(maxlen=50)\n",
    "        self.prev_context_states = deque(maxlen=10)\n",
    "        self.last_positions = deque(maxlen=30)\n",
    "        self.action_history = deque(maxlen=100)\n",
    "        \n",
    "        self.control_mode = \"move\"\n",
    "        self.timestep = 0\n",
    "        self.last_action = None\n",
    "        self.last_direction = 0\n",
    "        \n",
    "        self.MOVE_UTILITY_FLOOR = 0.05\n",
    "        self.INTERACT_UTILITY_FLOOR = 0.15\n",
    "        \n",
    "        # === PERSISTENT EXPLORATION MEMORY ===\n",
    "        self.EXPLORATION_MEMORY_FILE = BASE_PATH / \"exploration_memory.json\"\n",
    "        self.exploration_memory = {}\n",
    "        self.current_map_id = None\n",
    "        self.SAVE_INTERVAL = 100\n",
    "        \n",
    "        # Direction mapping\n",
    "        self.DIRECTION_NAMES = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        self.DIRECTION_TO_INT = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "        self.INT_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "        \n",
    "        self.DIRECTION_DELTAS_INT = {0: (0, 1), 1: (0, -1), 2: (-1, 0), 3: (1, 0)}\n",
    "        self.ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "        self.DELTA_TO_DIRECTION = {(0, 1): 0, (0, -1): 1, (-1, 0): 2, (1, 0): 3}\n",
    "        \n",
    "        self.load_exploration_memory()\n",
    "        \n",
    "        # === ACTION EXECUTION CONFIRMATION ===\n",
    "        self.pending_action = None\n",
    "        self.pending_action_frames = 0\n",
    "        self.ACTION_CONFIRM_FRAMES = 3\n",
    "        self.last_confirmed_action = None\n",
    "        \n",
    "        # === TILE INTERACTION PROBING ===\n",
    "        self.INTERACTION_VERIFY_FRAMES = 8\n",
    "        self.MIN_SUCCESS_RATE_THRESHOLD = 0.1\n",
    "        self.pending_interaction_verify = None\n",
    "        self.interaction_verify_countdown = 0\n",
    "        \n",
    "        # === MENU ESCAPE B-BOOST ===\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.B_BOOST_INCREMENT = 0.15\n",
    "        self.B_BOOST_MAX = 3.0\n",
    "        self.MENU_TRAP_THRESHOLD = 5\n",
    "        self.original_b_utility = None\n",
    "        \n",
    "        # === ADAPTIVE MODE SWAPPING ===\n",
    "        self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD = 15\n",
    "        self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD = 25\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.THRESHOLD_INCREMENT = 15\n",
    "        self.MAX_THRESHOLD = 150\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.swap_chain_count = 0\n",
    "        self.position_at_mode_swap = None\n",
    "        self.last_map_id = None\n",
    "        self.last_battle_state = None\n",
    "        \n",
    "        # === UNPRODUCTIVE MODE SWAP TRACKING ===\n",
    "        self.UNPRODUCTIVE_SWAP_THRESHOLD = 3\n",
    "        self.unproductive_swap_count = 0\n",
    "        self.utilities_before_swapping = {}\n",
    "        self.swap_chain_active = False\n",
    "        \n",
    "        # === STATE STAGNATION DETECTION ===\n",
    "        self.STATE_STAGNATION_THRESHOLD = 20\n",
    "        self.state_stagnation_count = 0\n",
    "        self.last_context_state_hash = None\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.STAGNATION_INITIATOR_PENALTY = 0.7\n",
    "        \n",
    "        # === NEW: \"BOTH\" MODE THRESHOLDS ===\n",
    "        self.BOTH_MODE_STAGNATION_THRESHOLD = 35\n",
    "        self.BOTH_MODE_SWAP_THRESHOLD = 5\n",
    "        \n",
    "        # === NEW: TURN AS PROGRESS TRACKING ===\n",
    "        self.last_direction_for_progress = None\n",
    "        self.direction_change_counts_as_progress = True\n",
    "        \n",
    "        # === NOVELTY WEIGHTS ===\n",
    "        self.UNVISITED_TILE_BONUS = 1.5\n",
    "        self.OBSTRUCTION_PENALTY = 0.25\n",
    "        \n",
    "        # === TRANSITION SYSTEM ===\n",
    "        self.TRANSITION_ATTRACTION_WEIGHT = 0.6\n",
    "        self.TEMP_DEBT_ACCUMULATION = 0.5\n",
    "        self.TEMP_DEBT_DECAY = 0.02\n",
    "        self.TEMP_DEBT_MAX = 15.0\n",
    "        \n",
    "        # === NEW: DEBT CAPS AND DECAY ===\n",
    "        self.MAX_MAP_DEBT = 10.0\n",
    "        self.MAX_LOCATION_DEBT = 5.0\n",
    "        self.DEBT_DECAY_RATE = 0.005\n",
    "        \n",
    "        # === TRANSITION BAN SYSTEM ===\n",
    "        self.transition_bans = {}\n",
    "        self.BAN_VICINITY_RADIUS = 3\n",
    "        self.BAN_COVERAGE_LIFT_THRESHOLD = 0.6\n",
    "        self.BAN_TIMEOUT_STEPS = 300\n",
    "        \n",
    "        # Multi-scale memory\n",
    "        self.visited_maps = {}\n",
    "        self.map_novelty_debt = {}\n",
    "        self.location_memory = {}\n",
    "        self.location_novelty = {}\n",
    "        self.action_execution_count = {}\n",
    "        \n",
    "        self.swap_perceptron = ControlSwapPerceptron()\n",
    "        self.error_history = deque(maxlen=1000)\n",
    "        self.numeric_error_history = deque(maxlen=1000)\n",
    "        self.visual_error_history = deque(maxlen=1000)\n",
    "        self._entity_norms_cache = {}\n",
    "        self._cache_valid = False\n",
    "        self.innate_entities_spawned = False\n",
    "        \n",
    "        # === REPETITION CORRECTION ===\n",
    "        self.consecutive_action_count = 0\n",
    "        self.current_repeated_action = None\n",
    "        self.LEARNING_SLOWDOWN_START = 3\n",
    "        self.LEARNING_SLOWDOWN_MAX = 10\n",
    "        self.PENALTY_THRESHOLD = 12\n",
    "        self.HARD_RESET_THRESHOLD = 18\n",
    "        \n",
    "        # === PATTERN DETECTION ===\n",
    "        self.PATTERN_CHECK_WINDOW = 50\n",
    "        self.PATTERN_MIN_REPEATS = 3\n",
    "        self.PATTERN_MAX_LENGTH = 10\n",
    "        self.detected_pattern = None\n",
    "        self.pattern_repeat_count = 0\n",
    "\n",
    "        # === PROBE ACTION CACHE ===\n",
    "        self._cached_probe_action = None\n",
    "        self._cached_probe_dir = None\n",
    "        self._probe_cache_position = None\n",
    "        \n",
    "        # === TEACHING MODE (NEW) ===\n",
    "        self.teaching_mode = True\n",
    "        self.demonstration_count = 0\n",
    "        self.context_action_stats = {}\n",
    "\n",
    "    # =========================================================================\n",
    "    # ACTION EXECUTION CONFIRMATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def set_pending_action(self, action_name):\n",
    "        self.pending_action = action_name\n",
    "        self.pending_action_frames = 0\n",
    "    \n",
    "    def confirm_action_executed(self, context_state, prev_context_state):\n",
    "        if self.pending_action is None:\n",
    "            return True\n",
    "        self.pending_action_frames += 1\n",
    "        action_executed = False\n",
    "        if prev_context_state is not None:\n",
    "            if self.pending_action in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "                pos_changed = (context_state[0] != prev_context_state[0] or \n",
    "                              context_state[1] != prev_context_state[1])\n",
    "                dir_changed = context_state[5] != prev_context_state[5]\n",
    "                action_executed = pos_changed or dir_changed\n",
    "            elif self.pending_action in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "                menu_changed = abs(context_state[4] - prev_context_state[4]) > 0.1\n",
    "                battle_changed = context_state[3] != prev_context_state[3]\n",
    "                map_changed = context_state[2] != prev_context_state[2]\n",
    "                action_executed = menu_changed or battle_changed or map_changed\n",
    "        if action_executed or self.pending_action_frames >= self.ACTION_CONFIRM_FRAMES:\n",
    "            self.last_confirmed_action = self.pending_action\n",
    "            self.pending_action = None\n",
    "            self.pending_action_frames = 0\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def should_send_new_action(self):\n",
    "        return self.pending_action is None or self.pending_action_frames >= self.ACTION_CONFIRM_FRAMES\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION MEMORY PERSISTENCE\n",
    "    # =========================================================================\n",
    "    \n",
    "    def load_exploration_memory(self):\n",
    "        try:\n",
    "            if self.EXPLORATION_MEMORY_FILE.exists():\n",
    "                with open(self.EXPLORATION_MEMORY_FILE, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    self.exploration_memory = {}\n",
    "                    for map_key, map_data in data.items():\n",
    "                        map_id = int(map_key.replace('map_', ''))\n",
    "                        self.exploration_memory[map_id] = self._deserialize_map_memory(map_data)\n",
    "                print(f\"  Loaded exploration memory: {len(self.exploration_memory)} maps\")\n",
    "            else:\n",
    "                self.exploration_memory = {}\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading exploration memory: {e}\")\n",
    "            self.exploration_memory = {}\n",
    "\n",
    "    def _deserialize_map_memory(self, map_data):\n",
    "        memory = {\n",
    "            'visited_tiles': set(tuple(t) for t in map_data.get('visited_tiles', [])),\n",
    "            'obstructions': set(tuple(t) for t in map_data.get('obstructions', [])),\n",
    "            'interactable_objects': map_data.get('interactable_objects', []),\n",
    "            'last_visited_timestep': map_data.get('last_visited_timestep', 0),\n",
    "            'transitions': map_data.get('transitions', []),\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': {}\n",
    "        }\n",
    "        for tile_key, tile_data in map_data.get('tile_interactions', {}).items():\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(tile_data.get('directions_tried', [])),\n",
    "                'direction_attempts': {int(k): v for k, v in tile_data.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {int(k): v for k, v in tile_data.get('direction_successes', {}).items()},\n",
    "                'exhausted': tile_data.get('exhausted', False)\n",
    "            }\n",
    "        return memory\n",
    "\n",
    "    def save_exploration_memory(self):\n",
    "        try:\n",
    "            data = {f'map_{mid}': self._serialize_map_memory(md) for mid, md in self.exploration_memory.items()}\n",
    "            with open(self.EXPLORATION_MEMORY_FILE, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving exploration memory: {e}\")\n",
    "\n",
    "    def _serialize_map_memory(self, map_data):\n",
    "        serialized_ti = {}\n",
    "        for tile_key, td in map_data.get('tile_interactions', {}).items():\n",
    "            serialized_ti[tile_key] = {\n",
    "                'directions_tried': list(td.get('directions_tried', set())),\n",
    "                'direction_attempts': {str(k): v for k, v in td.get('direction_attempts', {}).items()},\n",
    "                'direction_successes': {str(k): v for k, v in td.get('direction_successes', {}).items()},\n",
    "                'exhausted': td.get('exhausted', False)\n",
    "            }\n",
    "        return {\n",
    "            'visited_tiles': list(map_data['visited_tiles']),\n",
    "            'obstructions': list(map_data['obstructions']),\n",
    "            'interactable_objects': map_data['interactable_objects'],\n",
    "            'last_visited_timestep': map_data['last_visited_timestep'],\n",
    "            'transitions': map_data.get('transitions', []),\n",
    "            'temp_debt': map_data.get('temp_debt', 0.0),\n",
    "            'tile_interactions': serialized_ti\n",
    "        }\n",
    "\n",
    "    def get_current_map_memory(self, map_id):\n",
    "        if map_id not in self.exploration_memory:\n",
    "            self.exploration_memory[map_id] = {\n",
    "                'visited_tiles': set(), 'obstructions': set(), 'interactable_objects': [],\n",
    "                'last_visited_timestep': self.timestep, 'transitions': [], 'temp_debt': 0.0,\n",
    "                'tile_interactions': {}\n",
    "            }\n",
    "        return self.exploration_memory[map_id]\n",
    "\n",
    "    def record_visited_tile(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['visited_tiles'].add((int(x), int(y)))\n",
    "        memory['last_visited_timestep'] = self.timestep\n",
    "\n",
    "    def record_obstruction(self, x, y, map_id, direction):\n",
    "        dx, dy = self.DIRECTION_DELTAS_INT.get(direction, (0, 0))\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['obstructions'].add((int(x + dx), int(y + dy)))\n",
    "\n",
    "    # =========================================================================\n",
    "    # TILE-BASED INTERACTION PROBING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_tile_interaction_key(self, x, y):\n",
    "        return f\"{int(x)}_{int(y)}\"\n",
    "    \n",
    "    def get_tile_interaction_state(self, x, y, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_key = self.get_tile_interaction_key(x, y)\n",
    "        if tile_key not in memory['tile_interactions']:\n",
    "            memory['tile_interactions'][tile_key] = {\n",
    "                'directions_tried': set(),\n",
    "                'direction_attempts': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'direction_successes': {0: 0, 1: 0, 2: 0, 3: 0},\n",
    "                'exhausted': False\n",
    "            }\n",
    "        return memory['tile_interactions'][tile_key]\n",
    "    \n",
    "    def should_interact_at_tile(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if tile_state['exhausted']:\n",
    "            return False\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return True\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            successes = tile_state['direction_successes'].get(d, 0)\n",
    "            if attempts > 0 and successes / attempts >= self.MIN_SUCCESS_RATE_THRESHOLD:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def get_untried_directions(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        return [d for d in range(4) if d not in tile_state['directions_tried']]\n",
    "    \n",
    "    def get_best_interaction_direction(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        untried = self.get_untried_directions(x, y, map_id)\n",
    "        if untried:\n",
    "            return untried[0]\n",
    "        best_dir, best_rate = None, 0.0\n",
    "        for d in range(4):\n",
    "            attempts = tile_state['direction_attempts'].get(d, 0)\n",
    "            if attempts > 0:\n",
    "                rate = tile_state['direction_successes'].get(d, 0) / attempts\n",
    "                if rate > best_rate:\n",
    "                    best_rate, best_dir = rate, d\n",
    "        return best_dir\n",
    "    \n",
    "    def get_best_probe_action(self, raw_x, raw_y, current_map, current_dir):\n",
    "        \"\"\"Cached version - returns (action, target_direction) for tile probing.\"\"\"\n",
    "        cache_key = (raw_x, raw_y, current_map, current_dir)\n",
    "        \n",
    "        if self._probe_cache_position == cache_key:\n",
    "            return self._cached_probe_action, self._cached_probe_dir\n",
    "        \n",
    "        if not self.should_interact_at_tile(raw_x, raw_y, current_map):\n",
    "            result = (None, None)\n",
    "        else:\n",
    "            untried = self.get_untried_directions(raw_x, raw_y, current_map)\n",
    "            if not untried:\n",
    "                best_dir = self.get_best_interaction_direction(raw_x, raw_y, current_map)\n",
    "                if best_dir is not None:\n",
    "                    result = ('A', current_dir) if current_dir == best_dir else (self.INT_TO_ACTION[best_dir], best_dir)\n",
    "                else:\n",
    "                    result = (None, None)\n",
    "            elif current_dir in untried:\n",
    "                result = ('A', current_dir)\n",
    "            else:\n",
    "                target_dir = untried[0]\n",
    "                result = (self.INT_TO_ACTION[target_dir], target_dir)\n",
    "        \n",
    "        self._probe_cache_position = cache_key\n",
    "        self._cached_probe_action, self._cached_probe_dir = result\n",
    "        return result\n",
    "    \n",
    "    def record_tile_interaction_attempt(self, x, y, map_id, direction, success):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        tile_state['directions_tried'].add(direction)\n",
    "        tile_state['direction_attempts'][direction] = tile_state['direction_attempts'].get(direction, 0) + 1\n",
    "        if success:\n",
    "            tile_state['direction_successes'][direction] = tile_state['direction_successes'].get(direction, 0) + 1\n",
    "            memory = self.get_current_map_memory(map_id)\n",
    "            dir_name = self.DIRECTION_NAMES.get(direction, str(direction))\n",
    "            interactable = [int(x), int(y), dir_name]\n",
    "            if interactable not in memory['interactable_objects']:\n",
    "                memory['interactable_objects'].append(interactable)\n",
    "                print(f\"  ðŸŽ¯ INTERACTABLE FOUND: ({x}, {y}) facing {dir_name}\")\n",
    "        self._check_tile_exhaustion(x, y, map_id)\n",
    "    \n",
    "    def _check_tile_exhaustion(self, x, y, map_id):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        if len(tile_state['directions_tried']) < 4:\n",
    "            return\n",
    "        if not any(tile_state['direction_successes'].get(d, 0) > 0 for d in range(4)):\n",
    "            tile_state['exhausted'] = True\n",
    "            print(f\"  âœ“ Tile ({x}, {y}) exhausted - no interactions found\")\n",
    "    \n",
    "    def get_direction_success_rate(self, x, y, map_id, direction):\n",
    "        tile_state = self.get_tile_interaction_state(x, y, map_id)\n",
    "        attempts = tile_state['direction_attempts'].get(direction, 0)\n",
    "        if attempts == 0:\n",
    "            return None\n",
    "        return tile_state['direction_successes'].get(direction, 0) / attempts\n",
    "    \n",
    "    def start_interaction_verification(self, x, y, map_id, direction):\n",
    "        self.pending_interaction_verify = {'x': x, 'y': y, 'map_id': map_id, 'direction': direction}\n",
    "        self.interaction_verify_countdown = self.INTERACTION_VERIFY_FRAMES\n",
    "    \n",
    "    def check_interaction_verification(self, context_state, prev_context_state):\n",
    "        if self.pending_interaction_verify is None:\n",
    "            return\n",
    "        self.interaction_verify_countdown -= 1\n",
    "        success = False\n",
    "        if prev_context_state is not None:\n",
    "            menu_changed = abs(context_state[4] - prev_context_state[4]) > 0.1\n",
    "            battle_started = context_state[3] > 0.5 and prev_context_state[3] <= 0.5\n",
    "            map_changed = int(context_state[2]) != int(prev_context_state[2])\n",
    "            success = menu_changed or battle_started or map_changed\n",
    "        if success or self.interaction_verify_countdown <= 0:\n",
    "            info = self.pending_interaction_verify\n",
    "            self.record_tile_interaction_attempt(info['x'], info['y'], info['map_id'], info['direction'], success)\n",
    "            self.pending_interaction_verify = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITION SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def record_transition(self, from_pos, from_map, to_map, direction, action_type):\n",
    "        memory = self.get_current_map_memory(from_map)\n",
    "        for t in memory['transitions']:\n",
    "            if t['position'] == from_pos and t['direction'] == direction:\n",
    "                t['use_count'] += 1\n",
    "                t['last_used'] = self.timestep\n",
    "                return\n",
    "        memory['transitions'].append({\n",
    "            'position': from_pos, 'direction': direction, 'action': action_type,\n",
    "            'destination_map': to_map, 'use_count': 1, 'last_used': self.timestep\n",
    "        })\n",
    "        print(f\"  ðŸšª TRANSITION FOUND: Map {from_map} ({from_pos}) â†’ Map {to_map}\")\n",
    "\n",
    "    def get_transition_attraction(self, current_map):\n",
    "        memory = self.get_current_map_memory(current_map)\n",
    "        transitions = memory.get('transitions', [])\n",
    "        if not transitions:\n",
    "            return 0.0, None\n",
    "        current_debt = self.map_novelty_debt.get(current_map, 0.0)\n",
    "        current_temp_debt = self.get_temp_debt(current_map)\n",
    "        current_coverage = self.get_exploration_coverage(current_map)\n",
    "        best_attraction, best_transition = 0.0, None\n",
    "        for t in transitions:\n",
    "            if self.is_transition_banned(current_map, t['position'], t['direction']):\n",
    "                continue\n",
    "            dest_map = t['destination_map']\n",
    "            dest_debt = self.map_novelty_debt.get(dest_map, 0.0)\n",
    "            dest_temp_debt = self.get_temp_debt(dest_map)\n",
    "            dest_coverage = self.get_exploration_coverage(dest_map)\n",
    "            debt_diff = (current_debt + current_temp_debt * 2.0) - (dest_debt + dest_temp_debt * 2.0)\n",
    "            coverage_diff = current_coverage - dest_coverage\n",
    "            attraction = debt_diff * 0.5 + coverage_diff * 0.5\n",
    "            if t['use_count'] < 3:\n",
    "                attraction *= 1.5\n",
    "            if attraction > best_attraction:\n",
    "                best_attraction, best_transition = attraction, t\n",
    "        return best_attraction * self.TRANSITION_ATTRACTION_WEIGHT, best_transition\n",
    "\n",
    "    # =========================================================================\n",
    "    # TRANSITION BAN SYSTEM\n",
    "    # =========================================================================\n",
    "    \n",
    "    def create_transition_ban(self, map_id, tile_pos, direction_back):\n",
    "        self.transition_bans[map_id] = {\n",
    "            'banned_tile': tile_pos, 'banned_direction': direction_back,\n",
    "            'vicinity_radius': self.BAN_VICINITY_RADIUS, 'vicinity_active': False,\n",
    "            'created_at': self.timestep\n",
    "        }\n",
    "        print(f\"  ðŸš« TRANSITION BAN: Map {map_id} at {tile_pos} facing {self.DIRECTION_NAMES.get(direction_back, '?')}\")\n",
    "    \n",
    "    def is_transition_banned(self, map_id, position, direction):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return False\n",
    "        ban = self.transition_bans[map_id]\n",
    "        banned_tile = tuple(ban['banned_tile']) if isinstance(ban['banned_tile'], list) else ban['banned_tile']\n",
    "        position = tuple(position) if isinstance(position, list) else position\n",
    "        if position == banned_tile and direction == ban['banned_direction']:\n",
    "            return True\n",
    "        if ban['vicinity_active']:\n",
    "            dist = abs(position[0] - banned_tile[0]) + abs(position[1] - banned_tile[1])\n",
    "            if dist <= ban['vicinity_radius'] and direction == ban['banned_direction']:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def is_position_banned(self, map_id, x, y, direction):\n",
    "        return self.is_transition_banned(map_id, (x, y), direction)\n",
    "    \n",
    "    def update_transition_ban(self, map_id, current_pos):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return\n",
    "        ban = self.transition_bans[map_id]\n",
    "        banned_tile = tuple(ban['banned_tile']) if isinstance(ban['banned_tile'], list) else ban['banned_tile']\n",
    "        if not ban['vicinity_active'] and abs(current_pos[0] - banned_tile[0]) + abs(current_pos[1] - banned_tile[1]) >= 3:\n",
    "            ban['vicinity_active'] = True\n",
    "            print(f\"  ðŸš« VICINITY BAN ACTIVE: Map {map_id}\")\n",
    "    \n",
    "    def check_ban_lift_conditions(self, map_id):\n",
    "        if map_id not in self.transition_bans:\n",
    "            return\n",
    "        ban = self.transition_bans[map_id]\n",
    "        should_lift, reason = False, \"\"\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        non_banned = [t for t in memory.get('transitions', []) if not self.is_transition_banned(map_id, t['position'], t['direction'])]\n",
    "        if non_banned:\n",
    "            should_lift, reason = True, \"alternative transition found\"\n",
    "        elif self.get_exploration_coverage(map_id) >= self.BAN_COVERAGE_LIFT_THRESHOLD:\n",
    "            should_lift, reason = True, f\"coverage reached\"\n",
    "        elif self.timestep - ban['created_at'] >= self.BAN_TIMEOUT_STEPS:\n",
    "            should_lift, reason = True, \"timeout\"\n",
    "        if should_lift:\n",
    "            del self.transition_bans[map_id]\n",
    "            print(f\"  âœ… BAN LIFTED: Map {map_id} - {reason}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # DEBT SYSTEMS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        raw_debt = memory.get('temp_debt', 0.0)\n",
    "        if map_id != self.current_map_id:\n",
    "            steps_away = self.timestep - memory.get('last_visited_timestep', 0)\n",
    "            return max(0.0, raw_debt - steps_away * self.TEMP_DEBT_DECAY)\n",
    "        return raw_debt\n",
    "\n",
    "    def accumulate_temp_debt(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        memory['temp_debt'] = min(self.TEMP_DEBT_MAX, memory.get('temp_debt', 0.0) + self.TEMP_DEBT_ACCUMULATION)\n",
    "\n",
    "    def decay_all_debts(self):\n",
    "        \"\"\"Decay debts for non-current locations to prevent runaway accumulation.\"\"\"\n",
    "        for map_id in list(self.map_novelty_debt.keys()):\n",
    "            if map_id != self.current_map_id:\n",
    "                self.map_novelty_debt[map_id] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.map_novelty_debt[map_id] < 0.1:\n",
    "                    del self.map_novelty_debt[map_id]\n",
    "        \n",
    "        current_loc = None\n",
    "        if self.current_map_id is not None and len(self.last_positions) > 0:\n",
    "            pos = self.last_positions[-1]\n",
    "            current_loc = self.get_location_key(pos[0], pos[1], self.current_map_id)\n",
    "        \n",
    "        for loc in list(self.location_novelty.keys()):\n",
    "            if loc != current_loc:\n",
    "                self.location_novelty[loc] *= (1.0 - self.DEBT_DECAY_RATE)\n",
    "                if self.location_novelty[loc] < 0.1:\n",
    "                    del self.location_novelty[loc]\n",
    "\n",
    "    def get_exploration_coverage(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        visited = len(memory['visited_tiles'])\n",
    "        obstructions = len(memory['obstructions'])\n",
    "        if visited == 0 or visited + obstructions < 10:\n",
    "            return 0.0\n",
    "        return visited / (visited + obstructions)\n",
    "\n",
    "    def detect_obstruction(self, prev_context, context_state, raw_position, prev_raw_position):\n",
    "        if prev_context is None or prev_raw_position is None:\n",
    "            return False\n",
    "        if self.last_action not in ['UP', 'DOWN', 'LEFT', 'RIGHT']:\n",
    "            return False\n",
    "        if raw_position == prev_raw_position:\n",
    "            self.record_obstruction(raw_position[0], raw_position[1], int(context_state[2]), int(context_state[5]))\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # =========================================================================\n",
    "    # MENU TRAP B-BOOST\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_menu_trap_tracking(self, context_state, action_taken, raw_position=None):\n",
    "        current_pos = raw_position if raw_position else (round(context_state[0] * 255), round(context_state[1] * 255))\n",
    "        if self.menu_trap_position is not None and current_pos != self.menu_trap_position:\n",
    "            self.reset_menu_trap_boost()\n",
    "            return\n",
    "        if self.get_context_state_hash(context_state) == self.last_context_state_hash:\n",
    "            if action_taken in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "                self.menu_trap_frames += 1\n",
    "                self.menu_trap_position = current_pos\n",
    "                if self.menu_trap_frames > self.MENU_TRAP_THRESHOLD:\n",
    "                    if self.original_b_utility is None:\n",
    "                        for a in self.actions():\n",
    "                            if a.action == 'B':\n",
    "                                self.original_b_utility = a.utility\n",
    "                                break\n",
    "                    self.menu_trap_b_boost = min(self.B_BOOST_MAX, self.menu_trap_b_boost + self.B_BOOST_INCREMENT)\n",
    "        elif current_pos != self.menu_trap_position:\n",
    "            self.reset_menu_trap_boost()\n",
    "\n",
    "    def reset_menu_trap_boost(self):\n",
    "        if self.menu_trap_b_boost > 1.0 and self.original_b_utility is not None:\n",
    "            for a in self.actions():\n",
    "                if a.action == 'B':\n",
    "                    a.utility = self.original_b_utility\n",
    "                    break\n",
    "        self.menu_trap_frames = 0\n",
    "        self.menu_trap_b_boost = 1.0\n",
    "        self.menu_trap_position = None\n",
    "        self.original_b_utility = None\n",
    "\n",
    "    # =========================================================================\n",
    "    # STANDARD METHODS\n",
    "    # =========================================================================\n",
    "    \n",
    "    def add(self, p):\n",
    "        self.perceptrons.append(p)\n",
    "        self._cache_valid = False\n",
    "\n",
    "    def actions(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"action\"]\n",
    "\n",
    "    def entities(self):\n",
    "        return [p for p in self.perceptrons if p.kind == \"entity\"]\n",
    "\n",
    "    def get_location_key(self, x, y, map_id, bin_size=5):\n",
    "        return (int(map_id), int(x // bin_size) * bin_size, int(y // bin_size) * bin_size)\n",
    "\n",
    "    def is_near_map_edge(self, x, y):\n",
    "        return x < 10 or x > 245 or y < 10 or y > 245\n",
    "\n",
    "    def record_action_execution(self, action_name):\n",
    "        if action_name:\n",
    "            self.action_execution_count[action_name] = self.action_execution_count.get(action_name, 0) + 1\n",
    "\n",
    "    def get_position_stagnation(self):\n",
    "        if len(self.last_positions) < 2:\n",
    "            return 0\n",
    "        current_pos = self.last_positions[-1]\n",
    "        return sum(1 for pos in reversed(list(self.last_positions)[:-1]) if pos == current_pos)\n",
    "\n",
    "    def get_group_weight(self, group):\n",
    "        return sum(a.utility for a in self.actions() if a.group == group)\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODE SWAP & STAGNATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    def get_context_state_hash(self, context_state):\n",
    "        return (round(context_state[0], 2), round(context_state[1], 2), int(context_state[2]),\n",
    "                int(context_state[3]), round(context_state[4], 2), int(context_state[5]))\n",
    "\n",
    "    def check_state_stagnation(self, context_state):\n",
    "        current_hash = self.get_context_state_hash(context_state)\n",
    "        if current_hash == self.last_context_state_hash:\n",
    "            self.state_stagnation_count += 1\n",
    "            if self.state_stagnation_count == 1 and self.last_action:\n",
    "                self.stagnation_initiator_action = self.last_action\n",
    "        else:\n",
    "            self.state_stagnation_count = 0\n",
    "            self.stagnation_initiator_action = None\n",
    "        self.last_context_state_hash = current_hash\n",
    "        return self.state_stagnation_count >= self.STATE_STAGNATION_THRESHOLD\n",
    "\n",
    "    def check_direction_change_progress(self, context_state):\n",
    "        \"\"\"Check if direction changed - counts as partial progress.\"\"\"\n",
    "        current_dir = int(context_state[5])\n",
    "        if self.last_direction_for_progress is None:\n",
    "            self.last_direction_for_progress = current_dir\n",
    "            return False\n",
    "        changed = current_dir != self.last_direction_for_progress\n",
    "        self.last_direction_for_progress = current_dir\n",
    "        return changed\n",
    "\n",
    "    def apply_stagnation_initiator_penalty(self):\n",
    "        if self.stagnation_initiator_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.stagnation_initiator_action:\n",
    "                old_util = a.utility\n",
    "                a.utility *= self.STAGNATION_INITIATOR_PENALTY\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                a.utility = max(a.utility, floor)\n",
    "                print(f\"  ðŸ“ STAGNATION PENALTY: {self.stagnation_initiator_action} {old_util:.3f} â†’ {a.utility:.3f}\")\n",
    "                break\n",
    "        self.stagnation_initiator_action = None\n",
    "\n",
    "    def check_productive_change(self, context_state):\n",
    "        current_map = int(context_state[2])\n",
    "        current_battle = context_state[3] > 0.5\n",
    "        current_pos = (context_state[0], context_state[1])\n",
    "        productive, reason = False, \"\"\n",
    "        \n",
    "        if self.last_map_id is not None and current_map != self.last_map_id:\n",
    "            productive, reason = True, \"map change\"\n",
    "        if self.last_battle_state is not None and current_battle != self.last_battle_state:\n",
    "            productive, reason = True, \"battle change\"\n",
    "        if self.position_at_mode_swap is not None:\n",
    "            dist = np.sqrt((current_pos[0] - self.position_at_mode_swap[0])**2 + \n",
    "                          (current_pos[1] - self.position_at_mode_swap[1])**2)\n",
    "            if dist > 0.03:\n",
    "                productive, reason = True, f\"moved {dist*255:.1f} tiles\"\n",
    "        \n",
    "        if self.direction_change_counts_as_progress and self.check_direction_change_progress(context_state):\n",
    "            self.state_stagnation_count = max(0, self.state_stagnation_count - 5)\n",
    "        \n",
    "        self.last_map_id = current_map\n",
    "        self.last_battle_state = current_battle\n",
    "        return productive, reason\n",
    "\n",
    "    def on_productive_change(self, reason):\n",
    "        self.move_to_interact_threshold = self.DEFAULT_MOVE_TO_INTERACT_THRESHOLD\n",
    "        self.interact_to_move_threshold = self.DEFAULT_INTERACT_TO_MOVE_THRESHOLD\n",
    "        self.swap_chain_count = 0\n",
    "        self.state_stagnation_count = 0\n",
    "        self.stagnation_initiator_action = None\n",
    "        self.unproductive_swap_count = 0\n",
    "\n",
    "    def on_mode_swap(self, from_mode, to_mode):\n",
    "        self.swap_chain_count += 1\n",
    "        self.frames_in_current_mode = 0\n",
    "        self.unproductive_swap_count += 1\n",
    "        if self.unproductive_swap_count >= self.UNPRODUCTIVE_SWAP_THRESHOLD:\n",
    "            self._reset_highest_to_third(to_mode)\n",
    "            self.unproductive_swap_count = 0\n",
    "        if to_mode == \"interact\":\n",
    "            self.interact_to_move_threshold = min(self.MAX_THRESHOLD, self.interact_to_move_threshold + self.THRESHOLD_INCREMENT)\n",
    "        else:\n",
    "            self.move_to_interact_threshold = min(self.MAX_THRESHOLD, self.move_to_interact_threshold + self.THRESHOLD_INCREMENT)\n",
    "\n",
    "    def _reset_highest_to_third(self, mode):\n",
    "        if mode in [\"battle\", \"both\"]:\n",
    "            return\n",
    "        group = \"move\" if mode == \"move\" else \"interact\"\n",
    "        group_actions = [a for a in self.actions() if a.group == group]\n",
    "        if len(group_actions) < 3:\n",
    "            return\n",
    "        sorted_actions = sorted(group_actions, key=lambda a: a.utility, reverse=True)\n",
    "        floor = self.INTERACT_UTILITY_FLOOR if group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "        sorted_actions[0].utility = max(sorted_actions[2].utility * 0.9, floor)\n",
    "\n",
    "    def should_use_both_mode(self):\n",
    "        \"\"\"Check if we should allow all actions (both mode).\"\"\"\n",
    "        return (self.state_stagnation_count > self.BOTH_MODE_STAGNATION_THRESHOLD or \n",
    "                self.unproductive_swap_count > self.BOTH_MODE_SWAP_THRESHOLD)\n",
    "\n",
    "    def determine_control_mode(self, context_state, raw_position=None):\n",
    "        if context_state[3] > 0.5:\n",
    "            return \"battle\"\n",
    "        \n",
    "        self.frames_in_current_mode += 1\n",
    "        position_stagnation = self.get_position_stagnation()\n",
    "        \n",
    "        productive, reason = self.check_productive_change(context_state)\n",
    "        if productive:\n",
    "            self.on_productive_change(reason)\n",
    "        \n",
    "        if self.should_use_both_mode():\n",
    "            return \"both\"\n",
    "        \n",
    "        if self.check_state_stagnation(context_state):\n",
    "            self.apply_stagnation_initiator_penalty()\n",
    "            new_mode = \"interact\" if self.control_mode == \"move\" else \"move\"\n",
    "            self.control_mode = new_mode\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.on_mode_swap(self.control_mode, new_mode)\n",
    "            self.state_stagnation_count = 0\n",
    "            return self.control_mode\n",
    "        \n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_map = int(context_state[2])\n",
    "        \n",
    "        tile_needs_probing = self.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "        untried_directions = self.get_untried_directions(raw_x, raw_y, current_map)\n",
    "        \n",
    "        if tile_needs_probing and untried_directions and self.control_mode == \"move\" and self.frames_in_current_mode >= 3:\n",
    "            self.control_mode = \"interact\"\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.frames_in_current_mode = 0\n",
    "            return self.control_mode\n",
    "        \n",
    "        if self.control_mode == \"move\" and position_stagnation >= self.move_to_interact_threshold:\n",
    "            self.control_mode = \"interact\"\n",
    "            self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "            self.on_mode_swap(\"move\", \"interact\")\n",
    "        elif self.control_mode == \"interact\":\n",
    "            if (not tile_needs_probing or not untried_directions) and self.frames_in_current_mode >= 5:\n",
    "                self.control_mode = \"move\"\n",
    "                self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "                self.frames_in_current_mode = 0\n",
    "            elif self.frames_in_current_mode >= self.interact_to_move_threshold:\n",
    "                self.control_mode = \"move\"\n",
    "                self.position_at_mode_swap = (context_state[0], context_state[1])\n",
    "                self.on_mode_swap(\"interact\", \"move\")\n",
    "        \n",
    "        return self.control_mode\n",
    "\n",
    "    # =========================================================================\n",
    "    # EXPLORATION TRACKING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def update_exploration_tracking(self, context_state, prev_context_state, raw_position=None, prev_raw_position=None):\n",
    "        current_map = int(context_state[2])\n",
    "        raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "        raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "        current_pos = (raw_x, raw_y)\n",
    "        \n",
    "        if self.current_map_id is not None and current_map != self.current_map_id:\n",
    "            prev_map = self.current_map_id\n",
    "            if prev_context_state is not None and prev_raw_position is not None:\n",
    "                self.record_transition(prev_raw_position, prev_map, current_map,\n",
    "                    int(prev_context_state[5]), 'interact' if self.last_action == 'A' else 'walk')\n",
    "            if prev_raw_position is not None:\n",
    "                entry_dir = int(context_state[5]) if prev_context_state is not None else 0\n",
    "                self.create_transition_ban(current_map, current_pos, (entry_dir + 2) % 4)\n",
    "            self.on_map_change(current_map)\n",
    "        \n",
    "        self.current_map_id = current_map\n",
    "        self.record_visited_tile(raw_x, raw_y, current_map)\n",
    "        self.accumulate_temp_debt(current_map)\n",
    "        self.update_transition_ban(current_map, current_pos)\n",
    "        self.check_ban_lift_conditions(current_map)\n",
    "        \n",
    "        if prev_context_state is not None and prev_raw_position is not None:\n",
    "            self.detect_obstruction(prev_context_state, context_state, raw_position, prev_raw_position)\n",
    "        \n",
    "        self.check_interaction_verification(context_state, prev_context_state)\n",
    "        self.last_direction = int(context_state[5])\n",
    "        \n",
    "        if self.timestep % 300 == 0:\n",
    "            self.decay_all_debts()\n",
    "\n",
    "    def on_map_change(self, new_map):\n",
    "        self.save_exploration_memory()\n",
    "        self.control_mode = \"move\"\n",
    "        self.frames_in_current_mode = 0\n",
    "        memory = self.get_current_map_memory(new_map)\n",
    "        tile_interactions = memory.get('tile_interactions', {})\n",
    "        print(f\"  ðŸ—ºï¸ MAP CHANGE â†’ {new_map}: {len(memory['visited_tiles'])} visited, {len(memory['obstructions'])} obs\")\n",
    "        print(f\"     Tiles probed: {len(tile_interactions)}, exhausted: {sum(1 for t in tile_interactions.values() if t.get('exhausted', False))}\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # REPETITION & PATTERN HANDLING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def track_consecutive_action(self, action_name):\n",
    "        if action_name == self.current_repeated_action:\n",
    "            self.consecutive_action_count += 1\n",
    "        else:\n",
    "            self.current_repeated_action = action_name\n",
    "            self.consecutive_action_count = 1\n",
    "\n",
    "    def get_learning_multiplier(self, action_name):\n",
    "        if action_name != self.current_repeated_action or self.consecutive_action_count < self.LEARNING_SLOWDOWN_START:\n",
    "            return 1.0\n",
    "        progress = min(1.0, (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) / \n",
    "                       (self.LEARNING_SLOWDOWN_MAX - self.LEARNING_SLOWDOWN_START))\n",
    "        return max(0.05, 1.0 - 0.95 * progress)\n",
    "\n",
    "    def get_nth_highest_utility(self, group, n=3):\n",
    "        utilities = sorted([a.utility for a in self.actions() if a.group == group], reverse=True)\n",
    "        if len(utilities) < n:\n",
    "            return self.INTERACT_UTILITY_FLOOR if group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "        return utilities[n-1]\n",
    "\n",
    "    def detect_pattern(self):\n",
    "        if len(self.action_history) < 6:\n",
    "            return None, 0\n",
    "        recent = list(self.action_history)[-self.PATTERN_CHECK_WINDOW:]\n",
    "        for pattern_len in range(1, self.PATTERN_MAX_LENGTH + 1):\n",
    "            if len(recent) < pattern_len * self.PATTERN_MIN_REPEATS:\n",
    "                continue\n",
    "            candidate = tuple(recent[-pattern_len:])\n",
    "            repeat_count, idx = 0, len(recent) - pattern_len\n",
    "            while idx >= 0 and tuple(recent[idx:idx + pattern_len]) == candidate:\n",
    "                repeat_count += 1\n",
    "                idx -= pattern_len\n",
    "            if repeat_count >= self.PATTERN_MIN_REPEATS:\n",
    "                return candidate, repeat_count\n",
    "        return None, 0\n",
    "\n",
    "    def apply_pattern_penalty(self):\n",
    "        pattern, repeat_count = self.detect_pattern()\n",
    "        if pattern is None:\n",
    "            self.detected_pattern, self.pattern_repeat_count = None, 0\n",
    "            return\n",
    "        self.detected_pattern, self.pattern_repeat_count = pattern, repeat_count\n",
    "        for action_name in set(pattern):\n",
    "            group = \"interact\" if action_name in [\"A\", \"B\", \"Start\", \"Select\"] else \"move\"\n",
    "            third_util = self.get_nth_highest_utility(group, n=3)\n",
    "            for a in self.actions():\n",
    "                if a.action == action_name:\n",
    "                    floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                    a.utility = max(third_util * 0.9, floor)\n",
    "                    break\n",
    "\n",
    "    def apply_repetition_penalty(self):\n",
    "        if self.current_repeated_action is None:\n",
    "            return\n",
    "        for a in self.actions():\n",
    "            if a.action == self.current_repeated_action:\n",
    "                floor = self.INTERACT_UTILITY_FLOOR if a.group == \"interact\" else self.MOVE_UTILITY_FLOOR\n",
    "                if self.consecutive_action_count >= self.HARD_RESET_THRESHOLD:\n",
    "                    a.utility = max(self.get_nth_highest_utility(a.group, n=3) * 0.9, floor)\n",
    "                    self.consecutive_action_count = 0\n",
    "                elif self.consecutive_action_count >= self.PENALTY_THRESHOLD:\n",
    "                    a.utility = max(a.utility * 0.7, floor)\n",
    "                break\n",
    "\n",
    "    # =========================================================================\n",
    "    # ENTITY & LEARNING\n",
    "    # =========================================================================\n",
    "    \n",
    "    def spawn_innate_entities(self, learning_state):\n",
    "        if self.innate_entities_spawned:\n",
    "            return\n",
    "        for etype, indices in [(\"sense_menu\", [5, 6]), (\"sense_battle\", [3, 4]), \n",
    "                                (\"sense_movement\", [0, 1]), (\"sense_map_transition\", [2])]:\n",
    "            entity = Perceptron(\"entity\", entity_type=etype)\n",
    "            entity.ensure_weights(len(learning_state))\n",
    "            entity.weights = np.zeros(len(learning_state))\n",
    "            for idx in indices:\n",
    "                entity.weights[idx] = 0.5 if len(indices) > 1 else 1.0\n",
    "            self.add(entity)\n",
    "        self.innate_entities_spawned = True\n",
    "\n",
    "    def enforce_utility_floors(self):\n",
    "        for a in self.actions():\n",
    "            floor = self.MOVE_UTILITY_FLOOR if a.group == \"move\" else self.INTERACT_UTILITY_FLOOR\n",
    "            a.utility = max(a.utility, floor)\n",
    "\n",
    "    def get_spawn_threshold_adaptive(self, error_type='combined', percentile=50):\n",
    "        history = {'numeric': self.numeric_error_history, 'visual': self.visual_error_history}.get(error_type, self.error_history)\n",
    "        return max(0.001, np.percentile(history, percentile)) if len(history) >= 100 else 0.0005\n",
    "\n",
    "    def stagnation_level(self, window=10):\n",
    "        if len(self.prev_learning_states) < window:\n",
    "            return 0.0\n",
    "        recent = list(self.prev_learning_states)[-window:]\n",
    "        return 1.0 - np.tanh(np.mean([np.linalg.norm(recent[i] - recent[i-1]) for i in range(1, len(recent))]) * 2.0)\n",
    "\n",
    "    def predict_future_error(self, state, action, context_state, raw_position=None):\n",
    "        entity_novelty = np.mean([e.predict(state) * e.utility for e in self.entities()]) if self.entities() else 0.5\n",
    "        combined = entity_novelty * 0.7 + action.utility * 0.3\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        map_debt = min(self.map_novelty_debt.get(current_map, 0.0), self.MAX_MAP_DEBT)\n",
    "        loc_debt = min(self.location_novelty.get(loc, 0.0), self.MAX_LOCATION_DEBT)\n",
    "        total_debt = map_debt + self.get_temp_debt(current_map) + loc_debt * 0.5\n",
    "        combined *= 1.0 / (1.0 + total_debt * 5.0)\n",
    "        \n",
    "        if action.action == self.current_repeated_action and self.consecutive_action_count > self.LEARNING_SLOWDOWN_START:\n",
    "            combined *= 1.0 / (1.0 + (self.consecutive_action_count - self.LEARNING_SLOWDOWN_START) * 0.15)\n",
    "        if self.detected_pattern and action.action in self.detected_pattern:\n",
    "            combined *= 1.0 / (1.0 + self.pattern_repeat_count * 0.2)\n",
    "        \n",
    "        return combined + np.random.randn() * 0.05\n",
    "\n",
    "    def compute_multi_modal_error(self, state, next_state):\n",
    "        diffs = [abs(next_state[i] - state[i]) for i in range(min(8, len(state), len(next_state)))]\n",
    "        weights = [0.5, 0.5, 10.0, 5.0, 3.0, 2.0, 1.5, 0.3]\n",
    "        weighted = sum(d * w for d, w in zip(diffs, weights)) + np.linalg.norm(next_state[8:] - state[8:]) * 2.0\n",
    "        numeric = sum(diffs)\n",
    "        visual = np.linalg.norm(next_state[8:] - state[8:])\n",
    "        return weighted, numeric, visual\n",
    "\n",
    "    def learn(self, learning_state, next_learning_state, context_state, next_context_state, dead=False,\n",
    "            raw_position=None, next_raw_position=None):\n",
    "        if learning_state.shape != next_learning_state.shape:\n",
    "            max_dim = max(len(learning_state), len(next_learning_state))\n",
    "            learning_state = np.pad(learning_state, (0, max(0, max_dim - len(learning_state))))\n",
    "            next_learning_state = np.pad(next_learning_state, (0, max(0, max_dim - len(next_learning_state))))\n",
    "        \n",
    "        if not self.innate_entities_spawned:\n",
    "            self.spawn_innate_entities(learning_state)\n",
    "        \n",
    "        prev_context = self.prev_context_states[-1] if self.prev_context_states else None\n",
    "        prev_raw = getattr(self, '_last_raw_position', None)\n",
    "        self.update_exploration_tracking(context_state, prev_context, raw_position, prev_raw)\n",
    "        self._last_raw_position = raw_position\n",
    "        \n",
    "        weighted_error, numeric_error, visual_error = self.compute_multi_modal_error(learning_state, next_learning_state)\n",
    "        self.error_history.append(weighted_error)\n",
    "        self.numeric_error_history.append(numeric_error)\n",
    "        self.visual_error_history.append(visual_error)\n",
    "        \n",
    "        current_map = int(context_state[2])\n",
    "        loc = self.get_location_key(*(raw_position if raw_position else (context_state[0]*255, context_state[1]*255)), current_map)\n",
    "        \n",
    "        self.visited_maps[current_map] = self.visited_maps.get(current_map, 0) + 1\n",
    "        self.location_memory[loc] = self.location_memory.get(loc, 0) + 1\n",
    "        \n",
    "        if self.visited_maps[current_map] > 10:\n",
    "            self.map_novelty_debt[current_map] = min(self.MAX_MAP_DEBT, \n",
    "                self.map_novelty_debt.get(current_map, 0.0) + 0.05 * (self.visited_maps[current_map] - 10))\n",
    "        if self.location_memory[loc] > 15:\n",
    "            self.location_novelty[loc] = min(self.MAX_LOCATION_DEBT,\n",
    "                self.location_novelty.get(loc, 0.0) + 0.1 * (self.location_memory[loc] - 15))\n",
    "        \n",
    "        if self.visited_maps[current_map] > 30:\n",
    "            weighted_error *= 0.5\n",
    "        if self.location_memory[loc] > 25:\n",
    "            weighted_error *= 0.7\n",
    "        \n",
    "        stagnation = self.stagnation_level()\n",
    "        learning_mult = self.get_learning_multiplier(self.last_action) if self.last_action else 1.0\n",
    "        if self.detected_pattern and self.last_action in self.detected_pattern:\n",
    "            learning_mult *= 0.5\n",
    "        \n",
    "        for p in self.perceptrons:\n",
    "            mult = learning_mult if (p.kind == \"action\" and p.action == self.last_action) else 1.0\n",
    "            if p.kind == \"action\" and self.detected_pattern and p.action in self.detected_pattern:\n",
    "                mult *= 0.5\n",
    "            p.update(learning_state, weighted_error * mult, stagnation=stagnation)\n",
    "        \n",
    "        self.apply_repetition_penalty()\n",
    "        self.apply_pattern_penalty()\n",
    "        self.enforce_utility_floors()\n",
    "        \n",
    "        if prev_context is not None and np.linalg.norm(context_state[:2] - prev_context[:2]) > 0.001:\n",
    "            if self.last_action:\n",
    "                for a in self.actions():\n",
    "                    if a.action == self.last_action:\n",
    "                        boost = 1.15 if raw_position and self.is_near_map_edge(*raw_position) else 1.08\n",
    "                        a.utility = min(a.utility * boost, 2.0)\n",
    "                        break\n",
    "        \n",
    "        if self.timestep % self.SAVE_INTERVAL == 0:\n",
    "            self.save_exploration_memory()\n",
    "        \n",
    "        self.action_history.append(self.last_action)\n",
    "\n",
    "    def log_state(self, learning_state, context_state):\n",
    "        self.prev_learning_states.append(learning_state)\n",
    "        self.prev_context_states.append(context_state)\n",
    "\n",
    "    def update_position(self, x, y):\n",
    "        self.last_positions.append((int(x), int(y)))\n",
    "\n",
    "    def get_tile_interaction_stats(self, map_id):\n",
    "        memory = self.get_current_map_memory(map_id)\n",
    "        tile_interactions = memory.get('tile_interactions', {})\n",
    "        return {\n",
    "            'probed': len(tile_interactions),\n",
    "            'exhausted': sum(1 for t in tile_interactions.values() if t.get('exhausted', False)),\n",
    "            'with_success': sum(1 for t in tile_interactions.values() if any(t.get('direction_successes', {}).get(d, 0) > 0 for d in range(4)))\n",
    "        }\n",
    "\n",
    "    # =========================================================================\n",
    "    # TEACHING MODE - LEARNING FROM HUMAN (NEW)\n",
    "    # =========================================================================\n",
    "        \n",
    "    def learn_from_human_action(self, learning_state, human_action, context_state):\n",
    "        \"\"\"Learn from human's action choice.\"\"\"\n",
    "        if human_action is None or human_action == \"NONE\":\n",
    "            return\n",
    "        \n",
    "        self.demonstration_count += 1\n",
    "        \n",
    "        # Detect context\n",
    "        context = self._detect_context(context_state)\n",
    "        \n",
    "        # Track context-action statistics\n",
    "        context_key = f\"{context}_{int(context_state[2])}\"\n",
    "        if context_key not in self.context_action_stats:\n",
    "            self.context_action_stats[context_key] = {}\n",
    "        \n",
    "        if human_action not in self.context_action_stats[context_key]:\n",
    "            self.context_action_stats[context_key][human_action] = 0\n",
    "        self.context_action_stats[context_key][human_action] += 1\n",
    "        \n",
    "        # Boost utility of human's chosen action\n",
    "        for a in self.actions():\n",
    "            if a.action == human_action:\n",
    "                old_utility = a.utility\n",
    "                a.utility = min(a.utility * 1.05, 2.0)\n",
    "                \n",
    "                if self.demonstration_count % 50 == 0:\n",
    "                    print(f\"  ðŸ“š Learning: {human_action} utility {old_utility:.3f} â†’ {a.utility:.3f}\")\n",
    "                break\n",
    "        \n",
    "        # Update weights toward predicting this action\n",
    "        for a in self.actions():\n",
    "            if a.action == human_action:\n",
    "                a.ensure_weights(len(learning_state))\n",
    "                teaching_error = 0.1\n",
    "                a.update(learning_state, teaching_error, stagnation=0.0)\n",
    "            else:\n",
    "                a.ensure_weights(len(learning_state))\n",
    "                teaching_error = -0.02\n",
    "                a.update(learning_state, teaching_error, stagnation=0.0)\n",
    "\n",
    "    def _detect_context(self, context_state):\n",
    "        \"\"\"Determine game context from state.\"\"\"\n",
    "        if context_state[3] > 0.5:\n",
    "            return \"battle\"\n",
    "        elif context_state[4] > 0.5:\n",
    "            return \"menu\"\n",
    "        else:\n",
    "            return \"overworld\"\n",
    "\n",
    "    def print_teaching_stats(self):\n",
    "        \"\"\"Print what AI has learned from human.\"\"\"\n",
    "        if not self.context_action_stats:\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ðŸ“š TEACHING STATISTICS (Total demonstrations: {self.demonstration_count})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        for context_key, actions in self.context_action_stats.items():\n",
    "            total = sum(actions.values())\n",
    "            print(f\"\\n  Context: {context_key}\")\n",
    "            sorted_actions = sorted(actions.items(), key=lambda x: x[1], reverse=True)\n",
    "            for action, count in sorted_actions[:5]:\n",
    "                pct = (count / total) * 100\n",
    "                print(f\"    {action}: {count} times ({pct:.1f}%)\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # MODEL SAVING/LOADING\n",
    "    # =========================================================================\n",
    "\n",
    "    def save_model(self, filepath=None):\n",
    "        \"\"\"Save model with compressed weights.\"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = MODEL_FILE\n",
    "        \n",
    "        actions_data = []\n",
    "        for a in self.actions():\n",
    "            if a.weights is not None:\n",
    "                nonzero_indices = np.where(np.abs(a.weights) > 1e-6)[0]\n",
    "                nonzero_weights = [(int(idx), float(a.weights[idx])) for idx in nonzero_indices]\n",
    "                \n",
    "                actions_data.append({\n",
    "                    \"action\": a.action,\n",
    "                    \"group\": a.group,\n",
    "                    \"utility\": float(a.utility),\n",
    "                    \"weights_shape\": int(len(a.weights)),\n",
    "                    \"weights_nonzero\": nonzero_weights,\n",
    "                    \"learning_rate\": float(a.learning_rate),\n",
    "                    \"familiarity\": float(a.familiarity)\n",
    "                })\n",
    "        \n",
    "        entities_data = []\n",
    "        for e in self.entities():\n",
    "            if e.weights is not None:\n",
    "                nonzero_indices = np.where(np.abs(e.weights) > 1e-6)[0]\n",
    "                nonzero_weights = [(int(idx), float(e.weights[idx])) for idx in nonzero_indices]\n",
    "                \n",
    "                entities_data.append({\n",
    "                    \"entity_type\": e.entity_type,\n",
    "                    \"utility\": float(e.utility),\n",
    "                    \"weights_shape\": int(len(e.weights)),\n",
    "                    \"weights_nonzero\": nonzero_weights,\n",
    "                    \"familiarity\": float(e.familiarity)\n",
    "                })\n",
    "        \n",
    "        debt_data = {\n",
    "            \"map_novelty_debt\": {int(k): float(v) for k, v in self.map_novelty_debt.items()},\n",
    "            \"location_novelty\": {str(k): float(v) for k, v in self.location_novelty.items()},\n",
    "            \"visited_maps\": {int(k): int(v) for k, v in self.visited_maps.items()}\n",
    "        }\n",
    "        \n",
    "        teaching_data = {\n",
    "            \"demonstration_count\": int(self.demonstration_count),\n",
    "            \"context_action_stats\": {k: {ak: int(av) for ak, av in v.items()} \n",
    "                                        for k, v in self.context_action_stats.items()}\n",
    "        }\n",
    "        \n",
    "        model_data = {\n",
    "            \"timestep\": int(self.timestep),\n",
    "            \"perceptrons\": {\n",
    "                \"actions\": actions_data,\n",
    "                \"entities\": entities_data\n",
    "            },\n",
    "            \"debt_tracking\": debt_data,\n",
    "            \"teaching_stats\": teaching_data,\n",
    "            \"control_mode\": self.control_mode\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(model_data, f, indent=2)\n",
    "            print(f\"\\nðŸ’¾ MODEL SAVED: {filepath}\")\n",
    "            print(f\"   Timestep: {self.timestep}\")\n",
    "            print(f\"   Actions: {len(actions_data)} perceptrons\")\n",
    "            print(f\"   Entities: {len(entities_data)} perceptrons\")\n",
    "            print(f\"   Demonstrations: {self.demonstration_count}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving model: {e}\")\n",
    "\n",
    "    def load_model(self, filepath=None):\n",
    "        \"\"\"Load model with compressed weights.\"\"\"\n",
    "        if filepath is None:\n",
    "            filepath = MODEL_FILE\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"â„¹ï¸  No saved model found at {filepath}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                model_data = json.load(f)\n",
    "            \n",
    "            self.timestep = model_data.get(\"timestep\", 0)\n",
    "            self.control_mode = model_data.get(\"control_mode\", \"move\")\n",
    "            \n",
    "            actions_data = model_data.get(\"perceptrons\", {}).get(\"actions\", [])\n",
    "            for a_data in actions_data:\n",
    "                matching = [a for a in self.actions() if a.action == a_data[\"action\"]]\n",
    "                if matching:\n",
    "                    a = matching[0]\n",
    "                    a.utility = a_data[\"utility\"]\n",
    "                    a.learning_rate = a_data.get(\"learning_rate\", 0.01)\n",
    "                    a.familiarity = a_data.get(\"familiarity\", 0.0)\n",
    "                    \n",
    "                    weights_shape = a_data[\"weights_shape\"]\n",
    "                    a.weights = np.zeros(weights_shape)\n",
    "                    for idx, val in a_data[\"weights_nonzero\"]:\n",
    "                        if idx < weights_shape:\n",
    "                            a.weights[idx] = val\n",
    "            \n",
    "            entities_data = model_data.get(\"perceptrons\", {}).get(\"entities\", [])\n",
    "            for e_data in entities_data:\n",
    "                matching = [e for e in self.entities() if e.entity_type == e_data[\"entity_type\"]]\n",
    "                if matching:\n",
    "                    e = matching[0]\n",
    "                    e.utility = e_data[\"utility\"]\n",
    "                    e.familiarity = e_data.get(\"familiarity\", 0.0)\n",
    "                    \n",
    "                    weights_shape = e_data[\"weights_shape\"]\n",
    "                    e.weights = np.zeros(weights_shape)\n",
    "                    for idx, val in e_data[\"weights_nonzero\"]:\n",
    "                        if idx < weights_shape:\n",
    "                            e.weights[idx] = val\n",
    "            \n",
    "            debt_data = model_data.get(\"debt_tracking\", {})\n",
    "            self.map_novelty_debt = {int(k): float(v) for k, v in debt_data.get(\"map_novelty_debt\", {}).items()}\n",
    "            self.location_novelty = {eval(k) if k.startswith('(') else k: float(v) \n",
    "                                        for k, v in debt_data.get(\"location_novelty\", {}).items()}\n",
    "            self.visited_maps = {int(k): int(v) for k, v in debt_data.get(\"visited_maps\", {}).items()}\n",
    "            \n",
    "            teaching_data = model_data.get(\"teaching_stats\", {})\n",
    "            self.demonstration_count = teaching_data.get(\"demonstration_count\", 0)\n",
    "            self.context_action_stats = teaching_data.get(\"context_action_stats\", {})\n",
    "            \n",
    "            print(f\"\\nâœ… MODEL LOADED: {filepath}\")\n",
    "            print(f\"   Timestep: {self.timestep}\")\n",
    "            print(f\"   Actions: {len(actions_data)} perceptrons\")\n",
    "            print(f\"   Entities: {len(entities_data)} perceptrons\")\n",
    "            print(f\"   Demonstrations: {self.demonstration_count}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c1df11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # CELL 4: Action Selection - Updated with All Fixes\n",
    "# # ============================================================================\n",
    "# # CHANGES:\n",
    "# # 1. Added FORCED_EXPLORE_PROB (18%) for random exploration\n",
    "# # 2. Added \"both\" mode handling - allows all actions when stuck\n",
    "# # 3. Added turn-for-probing override - allows turns even in interact mode\n",
    "# # ============================================================================\n",
    "\n",
    "# import random  # Add to imports if not present\n",
    "\n",
    "# GBA_ACTIONS = [\"Up\", \"Down\", \"Left\", \"Right\", \"A\", \"B\", \"Start\", \"Select\"]\n",
    "# ACTION_DELTAS = {\"UP\": (0, -1), \"DOWN\": (0, 1), \"LEFT\": (-1, 0), \"RIGHT\": (1, 0)}\n",
    "# DIRECTION_TO_ACTION = {0: \"DOWN\", 1: \"UP\", 2: \"LEFT\", 3: \"RIGHT\"}\n",
    "# ACTION_TO_DIRECTION = {\"DOWN\": 0, \"UP\": 1, \"LEFT\": 2, \"RIGHT\": 3}\n",
    "\n",
    "# def manhattan_distance(pos1, pos2):\n",
    "#     return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])\n",
    "\n",
    "\n",
    "# def anticipatory_action(brain, learning_state, context_state, \n",
    "#                        exploration_weight=1.3, min_interact_prob=0.15,\n",
    "#                        raw_position=None,\n",
    "#                        forced_explore_prob=0.18):  # NEW: 18% forced random\n",
    "#     \"\"\"\n",
    "#     Action selection with all fixes:\n",
    "#     1. Forced random exploration (18%)\n",
    "#     2. \"Both\" mode when extremely stuck\n",
    "#     3. Turn-for-probing override\n",
    "#     4. Tile-based interaction probing\n",
    "#     5. Novelty-driven movement\n",
    "#     \"\"\"\n",
    "#     actions_list = brain.actions()\n",
    "#     if not actions_list:\n",
    "#         return Perceptron(\"action\", action=\"UP\", group=\"move\")\n",
    "\n",
    "#     mode = brain.determine_control_mode(context_state, raw_position=raw_position)\n",
    "#     current_map = int(context_state[2])\n",
    "#     current_dir = int(context_state[5])\n",
    "    \n",
    "#     raw_x = raw_position[0] if raw_position else int(context_state[0] * 255)\n",
    "#     raw_y = raw_position[1] if raw_position else int(context_state[1] * 255)\n",
    "#     current_pos = (raw_x, raw_y)\n",
    "    \n",
    "#     # Get exploration memory\n",
    "#     memory = brain.get_current_map_memory(current_map)\n",
    "#     visited_tiles = memory['visited_tiles']\n",
    "#     obstructions = memory['obstructions']\n",
    "    \n",
    "#     # Get tile interaction state\n",
    "#     tile_needs_probing = brain.should_interact_at_tile(raw_x, raw_y, current_map)\n",
    "    \n",
    "#     # NEW: Get best probe action (handles turn-then-interact)\n",
    "#     probe_action, probe_dir = brain.get_best_probe_action(raw_x, raw_y, current_map, current_dir)\n",
    "    \n",
    "#     # Get transition info\n",
    "#     transition_attraction, best_transition = brain.get_transition_attraction(current_map)\n",
    "#     coverage = brain.get_exploration_coverage(current_map)\n",
    "    \n",
    "#     # === BUILD ALLOWED ACTIONS LIST ===\n",
    "#     if mode == \"battle\":\n",
    "#         # In battle, use group weights to decide\n",
    "#         move_weight = brain.get_group_weight(\"move\")\n",
    "#         interact_weight = brain.get_group_weight(\"interact\")\n",
    "#         total = move_weight + interact_weight + 1e-9\n",
    "#         if random.random() < move_weight / total:\n",
    "#             allowed = [a for a in actions_list if a.group == \"move\"]\n",
    "#         else:\n",
    "#             allowed = [a for a in actions_list if a.group == \"interact\"]\n",
    "#         all_actions = actions_list  # Fallback\n",
    "        \n",
    "#     elif mode == \"both\":\n",
    "#         # NEW: \"Both\" mode - allow everything\n",
    "#         allowed = actions_list\n",
    "#         all_actions = actions_list\n",
    "        \n",
    "#     elif mode == \"interact\":\n",
    "#         allowed = [a for a in actions_list if a.group == \"interact\"]\n",
    "#         all_actions = None\n",
    "        \n",
    "#         # NEW: Turn-for-probing override\n",
    "#         # If we need to turn to probe, allow that movement action\n",
    "#         if probe_action and probe_action in ['UP', 'DOWN', 'LEFT', 'RIGHT']:\n",
    "#             turn_actions = [a for a in actions_list if a.action == probe_action]\n",
    "#             if turn_actions:\n",
    "#                 # Add the turn action to allowed list\n",
    "#                 allowed = allowed + turn_actions\n",
    "        \n",
    "#     else:  # move\n",
    "#         allowed = [a for a in actions_list if a.group == \"move\"]\n",
    "#         all_actions = None\n",
    "\n",
    "#     if not allowed:\n",
    "#         allowed = actions_list\n",
    "\n",
    "#     # === NEW: FORCED RANDOM EXPLORATION (18%) ===\n",
    "#     if random.random() < forced_explore_prob:\n",
    "#         chosen = random.choice(allowed)\n",
    "#         brain.record_action_execution(chosen.action)\n",
    "#         brain.track_consecutive_action(chosen.action)\n",
    "        \n",
    "#         # Still start interaction verification if it's an A press on a probeable tile\n",
    "#         if chosen.action == 'A' and tile_needs_probing:\n",
    "#             brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "        \n",
    "#         return chosen\n",
    "\n",
    "#     # === SCORE ACTIONS ===\n",
    "#     action_scores = []\n",
    "    \n",
    "#     for a in allowed:\n",
    "#         predicted = brain.predict_future_error(learning_state, a, context_state, raw_position=raw_position)\n",
    "        \n",
    "#         # --- MOVE ACTIONS ---\n",
    "#         if a.group == \"move\":\n",
    "#             if mode in [\"move\", \"both\"]:\n",
    "#                 predicted *= exploration_weight\n",
    "            \n",
    "#             dx, dy = ACTION_DELTAS.get(a.action, (0, 0))\n",
    "#             target_tile = (raw_x + dx, raw_y + dy)\n",
    "#             action_direction = ACTION_TO_DIRECTION.get(a.action, -1)\n",
    "            \n",
    "#             # BONUS: Unvisited tile\n",
    "#             if target_tile not in visited_tiles:\n",
    "#                 predicted *= brain.UNVISITED_TILE_BONUS\n",
    "            \n",
    "#             # PENALTY: Known obstruction\n",
    "#             if target_tile in obstructions:\n",
    "#                 predicted *= brain.OBSTRUCTION_PENALTY\n",
    "            \n",
    "#             # PENALTY: Transition ban\n",
    "#             if brain.is_position_banned(current_map, raw_x, raw_y, action_direction):\n",
    "#                 predicted *= 0.05\n",
    "            \n",
    "#             # BONUS: Toward transition when well-explored\n",
    "#             if transition_attraction > 0.3 and best_transition and coverage > 0.5:\n",
    "#                 trans_pos = tuple(best_transition['position']) if isinstance(best_transition['position'], list) else best_transition['position']\n",
    "#                 if manhattan_distance(target_tile, trans_pos) < manhattan_distance(current_pos, trans_pos):\n",
    "#                     predicted *= (1.0 + transition_attraction)\n",
    "            \n",
    "#             # NEW: If this is a turn needed for probing, boost it\n",
    "#             if probe_action == a.action and probe_dir is not None:\n",
    "#                 predicted *= 2.0  # Strong boost for needed turn\n",
    "            \n",
    "#             # Random factor for variety\n",
    "#             predicted *= (0.9 + random.random() * 0.2)\n",
    "        \n",
    "#         # --- INTERACT ACTIONS ---\n",
    "#         elif a.group == \"interact\":\n",
    "#             predicted = max(predicted, min_interact_prob)\n",
    "            \n",
    "#             # Menu trap B-boost\n",
    "#             if a.action == 'B':\n",
    "#                 predicted *= brain.menu_trap_b_boost\n",
    "            \n",
    "#             # A-press logic\n",
    "#             if a.action == 'A':\n",
    "#                 if tile_needs_probing and probe_action == 'A':\n",
    "#                     # We're facing an untried direction - strong boost!\n",
    "#                     predicted *= 3.0\n",
    "#                 elif tile_needs_probing:\n",
    "#                     # Tile needs probing but we need to turn first\n",
    "#                     predicted *= 0.5  # Mild penalty - turn should happen instead\n",
    "#                 else:\n",
    "#                     # Tile exhausted\n",
    "#                     predicted *= 0.1\n",
    "            \n",
    "#             # Start/Select - always penalize, no boost\n",
    "#             if a.action in ['Start', 'Select']:\n",
    "#                 predicted *= 0.3\n",
    "        \n",
    "#         action_scores.append((a, predicted))\n",
    "\n",
    "#     # === SELECT BEST ===\n",
    "#     if action_scores:\n",
    "#         best_action = max(action_scores, key=lambda x: x[1])[0]\n",
    "#         best_score = max(s for _, s in action_scores)\n",
    "        \n",
    "#         if best_score > 0.01:\n",
    "#             brain.record_action_execution(best_action.action)\n",
    "#             brain.track_consecutive_action(best_action.action)\n",
    "            \n",
    "#             # Start interaction verification for A-press on probeable tile\n",
    "#             if best_action.action == 'A' and tile_needs_probing:\n",
    "#                 brain.start_interaction_verification(raw_x, raw_y, current_map, current_dir)\n",
    "            \n",
    "#             return best_action\n",
    "    \n",
    "#     # === FALLBACKS ===\n",
    "    \n",
    "#     # Battle fallback\n",
    "#     if mode == \"battle\" and all_actions:\n",
    "#         all_scores = [(a, brain.predict_future_error(learning_state, a, context_state, raw_position=raw_position)) \n",
    "#                       for a in all_actions]\n",
    "#         if all_scores:\n",
    "#             best_action = max(all_scores, key=lambda x: x[1])[0]\n",
    "#             brain.record_action_execution(best_action.action)\n",
    "#             brain.track_consecutive_action(best_action.action)\n",
    "#             return best_action\n",
    "    \n",
    "#     # Move fallback: prefer unvisited\n",
    "#     if mode in [\"move\", \"both\"]:\n",
    "#         for a in allowed:\n",
    "#             if a.group == \"move\":\n",
    "#                 dx, dy = ACTION_DELTAS.get(a.action, (0, 0))\n",
    "#                 target = (raw_x + dx, raw_y + dy)\n",
    "#                 if target not in visited_tiles and target not in obstructions:\n",
    "#                     brain.record_action_execution(a.action)\n",
    "#                     brain.track_consecutive_action(a.action)\n",
    "#                     return a\n",
    "    \n",
    "#     # Generic fallback\n",
    "#     if allowed:\n",
    "#         best = max(allowed, key=lambda a: a.utility)\n",
    "#         brain.record_action_execution(best.action)\n",
    "#         brain.track_consecutive_action(best.action)\n",
    "#         return best\n",
    "    \n",
    "#     best = max(actions_list, key=lambda a: a.utility)\n",
    "#     brain.record_action_execution(best.action)\n",
    "#     brain.track_consecutive_action(best.action)\n",
    "#     return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a80e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded exploration memory: 7 maps\n",
      "\n",
      "âœ… MODEL LOADED: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\cogai\\model_checkpoint.json\n",
      "   Timestep: 204411\n",
      "   Actions: 8 perceptrons\n",
      "   Entities: 4 perceptrons\n",
      "   Demonstrations: 13411\n",
      "======================================================================\n",
      "ðŸŽ“ TEACHING MODE - OPTIMIZED BATCH PROCESSING\n",
      "======================================================================\n",
      "Lua writes inputs to: input_cache.txt\n",
      "Python polls every 2 seconds for new batches\n",
      "======================================================================\n",
      "PERSISTENT MEMORY: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\cogai\\exploration_memory.json\n",
      "MODEL CHECKPOINT: C:\\Users\\natmaw\\Documents\\Boston Stuff\\CS 5100 Foundations of AI\\cogai\\model_checkpoint.json\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "ðŸ“¦ BATCH #1: 331 inputs\n",
      "  ðŸ“š Learning: UP utility 0.090 â†’ 0.095\n",
      "  ðŸ“š Learning: DOWN utility 0.090 â†’ 0.095\n",
      "  ðŸ“š Learning: DOWN utility 0.090 â†’ 0.095\n",
      "  ðŸšª TRANSITION FOUND: Map 14 ((14, 15)) â†’ Map 24\n",
      "  ðŸš« TRANSITION BAN: Map 24 at (24, 33) facing LEFT\n",
      "  ðŸ—ºï¸ MAP CHANGE â†’ 24: 126 visited, 135 obs\n",
      "     Tiles probed: 0, exhausted: 0\n",
      "  âœ… BAN LIFTED: Map 24 - alternative transition found\n",
      "  ðŸ“š Learning: DOWN utility 0.090 â†’ 0.095\n",
      "  ðŸ“š Learning: DOWN utility 0.090 â†’ 0.095\n",
      "  ðŸ“š Learning: RIGHT utility 0.090 â†’ 0.095\n",
      "  Actions: {'UP': 64, 'DOWN': 188, 'RIGHT': 56, 'LEFT': 23}\n",
      "  Map 24 | Pos (26, 38)\n",
      "  Visited: 126 | Demos: 13742\n",
      "  Utils: A:0.15 B:0.15 Start:0.15 Select:0.15 UP:0.10 DOWN:0.10 LEFT:0.10 RIGHT:0.09\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Main Loop - OPTIMIZED CACHED TEACHING MODE\n",
    "# ============================================================================\n",
    "\n",
    "brain = Brain()\n",
    "\n",
    "# Action perceptrons\n",
    "for b in [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"move\"))\n",
    "for b in [\"A\", \"B\", \"Start\", \"Select\"]:\n",
    "    brain.add(Perceptron(\"action\", action=b, group=\"interact\"))\n",
    "\n",
    "# Try to load existing model\n",
    "brain.load_model()\n",
    "\n",
    "# Batch processing stats\n",
    "total_inputs_processed = 0\n",
    "batches_processed = 0\n",
    "\n",
    "# Cache for visual data (updated less frequently)\n",
    "cached_palette = np.zeros(PALETTE_DIM)\n",
    "cached_tiles = np.zeros(TILE_DIM)\n",
    "last_visual_update = 0\n",
    "VISUAL_UPDATE_INTERVAL = 30  # Update visuals every 30 batches\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸŽ“ TEACHING MODE - OPTIMIZED BATCH PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Lua writes inputs to: input_cache.txt\")\n",
    "print(\"Python polls every 2 seconds for new batches\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PERSISTENT MEMORY: {brain.EXPLORATION_MEMORY_FILE}\")\n",
    "print(f\"MODEL CHECKPOINT: {MODEL_FILE}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Polling interval\n",
    "POLL_INTERVAL = 2.0  # Check every 2 seconds\n",
    "\n",
    "prev_context_state = None\n",
    "prev_raw_position = None\n",
    "\n",
    "while True:\n",
    "    # Check for new input batch\n",
    "    cached_inputs = read_input_cache()\n",
    "    \n",
    "    if not cached_inputs:\n",
    "        time.sleep(POLL_INTERVAL)\n",
    "        continue\n",
    "    \n",
    "    # We have new inputs to process!\n",
    "    batches_processed += 1\n",
    "    batch_size = len(cached_inputs)\n",
    "    total_inputs_processed += batch_size\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ðŸ“¦ BATCH #{batches_processed}: {batch_size} inputs\")\n",
    "    \n",
    "    # Update visual cache occasionally\n",
    "    if batches_processed - last_visual_update >= VISUAL_UPDATE_INTERVAL:\n",
    "        context, palette, tiles, raw_pos = read_game_state_full()\n",
    "        if np.any(palette != 0):\n",
    "            cached_palette = palette\n",
    "            cached_tiles = tiles\n",
    "            last_visual_update = batches_processed\n",
    "            print(\"  [Visual cache updated]\")\n",
    "    \n",
    "    # Process each input in the batch\n",
    "    action_counts = {}\n",
    "    \n",
    "    for inp in cached_inputs:\n",
    "        inp_context, inp_raw_pos, human_action = process_cached_input(inp)\n",
    "        \n",
    "        raw_x, raw_y = inp_raw_pos\n",
    "        in_battle = inp_context[3]\n",
    "        current_map = int(inp_context[2])\n",
    "        \n",
    "        brain.update_position(raw_x, raw_y)\n",
    "        \n",
    "        # Build learning state\n",
    "        derived = compute_derived_features(inp_context, prev_context_state)\n",
    "        learning_state = build_learning_state(derived, cached_palette, cached_tiles, in_battle)\n",
    "        \n",
    "        brain.log_state(learning_state, inp_context)\n",
    "        \n",
    "        # Learn from human action\n",
    "        if human_action:\n",
    "            brain.learn_from_human_action(learning_state, human_action, inp_context)\n",
    "            brain.last_action = human_action\n",
    "            brain.record_action_execution(human_action)\n",
    "            action_counts[human_action] = action_counts.get(human_action, 0) + 1\n",
    "        \n",
    "        # Learn from state transition\n",
    "        if prev_context_state is not None:\n",
    "            prev_derived = compute_derived_features(prev_context_state, None)\n",
    "            prev_learning = build_learning_state(prev_derived, cached_palette, cached_tiles, \n",
    "                                                  prev_context_state[3])\n",
    "            brain.learn(prev_learning, learning_state, prev_context_state, inp_context,\n",
    "                       dead=False, raw_position=prev_raw_position, next_raw_position=inp_raw_pos)\n",
    "        \n",
    "        prev_context_state = inp_context.copy()\n",
    "        prev_raw_position = inp_raw_pos\n",
    "        brain.timestep += 1\n",
    "    \n",
    "    # Batch summary\n",
    "    current_map = int(inp_context[2])\n",
    "    memory = brain.get_current_map_memory(current_map)\n",
    "    \n",
    "    print(f\"  Actions: {action_counts}\")\n",
    "    print(f\"  Map {current_map} | Pos ({inp_raw_pos[0]}, {inp_raw_pos[1]})\")\n",
    "    print(f\"  Visited: {len(memory['visited_tiles'])} | Demos: {brain.demonstration_count}\")\n",
    "    \n",
    "    # Show utilities\n",
    "    utils = sorted([(a.action, a.utility) for a in brain.actions()], \n",
    "                   key=lambda x: x[1], reverse=True)\n",
    "    print(f\"  Utils: {' '.join([f'{k}:{v:.2f}' for k,v in utils])}\")\n",
    "    \n",
    "    # Periodic saves (every 5 batches = ~2.5 min)\n",
    "    if batches_processed % 5 == 0:\n",
    "        print(f\"\\n{'#'*50}\")\n",
    "        print(f\"# SAVING - Batch #{batches_processed}\")\n",
    "        print(f\"# Total inputs: {total_inputs_processed}\")\n",
    "        print(f\"# Demos: {brain.demonstration_count}\")\n",
    "        print(f\"{'#'*50}\")\n",
    "        \n",
    "        brain.print_teaching_stats()\n",
    "        brain.save_model()\n",
    "        brain.save_exploration_memory()\n",
    "    \n",
    "    # Don't sleep here - immediately check for next batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
